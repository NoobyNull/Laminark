{"version":3,"file":"index.js","names":["prependNotifications","textResponse","errorResponse","prependNotifications","textResponse","graphTablesDDL","prependNotifications","textResponse"],"sources":["../src/storage/search.ts","../src/storage/embeddings.ts","../src/storage/stash-manager.ts","../src/storage/threshold-store.ts","../src/mcp/server.ts","../src/search/hybrid.ts","../src/mcp/token-budget.ts","../src/mcp/tools/recall.ts","../src/mcp/tools/save-memory.ts","../src/commands/resume.ts","../src/mcp/tools/topic-context.ts","../src/graph/types.ts","../src/graph/migrations/001-graph-tables.ts","../src/graph/schema.ts","../src/mcp/tools/query-graph.ts","../src/graph/staleness.ts","../src/mcp/tools/graph-stats.ts","../src/analysis/worker-bridge.ts","../src/hooks/topic-shift-handler.ts","../src/intelligence/topic-detector.ts","../src/intelligence/adaptive-threshold.ts","../src/intelligence/decision-logger.ts","../src/config/topic-detection-config.ts","../src/storage/notifications.ts","../src/graph/extraction-rules.ts","../src/graph/entity-extractor.ts","../src/graph/constraints.ts","../src/graph/relationship-detector.ts","../src/graph/observation-merger.ts","../src/graph/curation-agent.ts","../src/web/routes/sse.ts","../src/web/routes/api.ts","../src/web/server.ts","../src/index.ts"],"sourcesContent":["import type BetterSqlite3 from 'better-sqlite3';\n\nimport { debug, debugTimed } from '../shared/debug.js';\nimport {\n  rowToObservation,\n  type ObservationRow,\n  type SearchResult,\n} from '../shared/types.js';\n\n/**\n * FTS5 search engine with BM25 ranking, snippet extraction, and strict project scoping.\n *\n * All queries are scoped to the projectHash provided at construction time.\n * Queries are sanitized to prevent FTS5 syntax errors and injection.\n */\nexport class SearchEngine {\n  private readonly db: BetterSqlite3.Database;\n  private readonly projectHash: string;\n\n  constructor(db: BetterSqlite3.Database, projectHash: string) {\n    this.db = db;\n    this.projectHash = projectHash;\n  }\n\n  /**\n   * Full-text search with BM25 ranking and snippet extraction.\n   *\n   * bm25() returns NEGATIVE values where more negative = more relevant.\n   * ORDER BY rank (ascending) puts best matches first.\n   *\n   * @param query - User's search query (sanitized for FTS5 safety)\n   * @param options - Optional limit and sessionId filter\n   * @returns SearchResult[] ordered by relevance (best match first)\n   */\n  searchKeyword(\n    query: string,\n    options?: { limit?: number; sessionId?: string },\n  ): SearchResult[] {\n    const sanitized = this.sanitizeQuery(query);\n    if (!sanitized) {\n      return [];\n    }\n\n    const limit = options?.limit ?? 20;\n\n    let sql = `\n      SELECT\n        o.*,\n        bm25(observations_fts, 2.0, 1.0) AS rank,\n        snippet(observations_fts, 1, '<mark>', '</mark>', '...', 32) AS snippet\n      FROM observations_fts\n      JOIN observations o ON o.rowid = observations_fts.rowid\n      WHERE observations_fts MATCH ?\n        AND o.project_hash = ?\n        AND o.deleted_at IS NULL\n    `;\n    const params: unknown[] = [sanitized, this.projectHash];\n\n    if (options?.sessionId) {\n      sql += ' AND o.session_id = ?';\n      params.push(options.sessionId);\n    }\n\n    sql += ' ORDER BY rank LIMIT ?';\n    params.push(limit);\n\n    const results = debugTimed('search', 'FTS5 keyword search', () => {\n      const rows = this.db.prepare(sql).all(...params) as (ObservationRow & {\n        rank: number;\n        snippet: string;\n      })[];\n\n      return rows.map((row) => ({\n        observation: rowToObservation(row),\n        score: Math.abs(row.rank),\n        matchType: 'fts' as const,\n        snippet: row.snippet,\n      }));\n    });\n\n    debug('search', 'Keyword search completed', { query: sanitized, resultCount: results.length });\n\n    return results;\n  }\n\n  /**\n   * Prefix search for autocomplete-style matching.\n   * Appends `*` to each word for prefix matching.\n   */\n  searchByPrefix(prefix: string, limit?: number): SearchResult[] {\n    const words = prefix.trim().split(/\\s+/).filter(Boolean);\n    if (words.length === 0) {\n      return [];\n    }\n\n    // Sanitize each word and append * for prefix matching\n    const sanitizedWords = words\n      .map((w) => this.sanitizeWord(w))\n      .filter(Boolean);\n    if (sanitizedWords.length === 0) {\n      return [];\n    }\n\n    const ftsQuery = sanitizedWords.map((w) => `${w}*`).join(' ');\n    const effectiveLimit = limit ?? 20;\n\n    const sql = `\n      SELECT\n        o.*,\n        bm25(observations_fts, 2.0, 1.0) AS rank,\n        snippet(observations_fts, 1, '<mark>', '</mark>', '...', 32) AS snippet\n      FROM observations_fts\n      JOIN observations o ON o.rowid = observations_fts.rowid\n      WHERE observations_fts MATCH ?\n        AND o.project_hash = ?\n        AND o.deleted_at IS NULL\n      ORDER BY rank\n      LIMIT ?\n    `;\n\n    const results = debugTimed('search', 'FTS5 prefix search', () => {\n      const rows = this.db\n        .prepare(sql)\n        .all(ftsQuery, this.projectHash, effectiveLimit) as (ObservationRow & {\n        rank: number;\n        snippet: string;\n      })[];\n\n      return rows.map((row) => ({\n        observation: rowToObservation(row),\n        score: Math.abs(row.rank),\n        matchType: 'fts' as const,\n        snippet: row.snippet,\n      }));\n    });\n\n    debug('search', 'Prefix search completed', { prefix, resultCount: results.length });\n\n    return results;\n  }\n\n  /**\n   * Rebuild the FTS5 index if it gets out of sync.\n   */\n  rebuildIndex(): void {\n    debug('search', 'Rebuilding FTS5 index');\n    this.db.exec(\n      \"INSERT INTO observations_fts(observations_fts) VALUES('rebuild')\",\n    );\n  }\n\n  /**\n   * Sanitizes a user query for safe FTS5 MATCH usage.\n   * Removes FTS5 operators and special characters.\n   * Returns null if the query is empty after sanitization.\n   */\n  private sanitizeQuery(query: string): string | null {\n    const words = query.trim().split(/\\s+/).filter(Boolean);\n    if (words.length === 0) {\n      return null;\n    }\n\n    const sanitizedWords = words\n      .map((w) => this.sanitizeWord(w))\n      .filter(Boolean);\n\n    if (sanitizedWords.length === 0) {\n      return null;\n    }\n\n    // FTS5 defaults to implicit AND for space-separated words\n    return sanitizedWords.join(' ');\n  }\n\n  /**\n   * Sanitizes a single word for FTS5 safety.\n   * Removes quotes, parentheses, asterisks, and FTS5 operator keywords.\n   */\n  private sanitizeWord(word: string): string {\n    // Remove FTS5 special characters\n    let cleaned = word.replace(/[\"*()^{}[\\]]/g, '');\n\n    // Remove FTS5 operator keywords (case-insensitive, only when the whole word is an operator)\n    if (/^(NEAR|OR|AND|NOT)$/i.test(cleaned)) {\n      return '';\n    }\n\n    // Remove any remaining non-alphanumeric characters except hyphens and underscores\n    cleaned = cleaned.replace(/[^\\w\\-]/g, '');\n\n    return cleaned;\n  }\n}\n","/**\n * EmbeddingStore for sqlite-vec vec0 table operations.\n *\n * Provides store/search/delete/has/findUnembedded methods against the\n * cosine-distance vec0 table (observation_embeddings). All operations\n * are project-scoped via subquery join on the observations table.\n *\n * Float32Array passes directly to better-sqlite3 for vec0 operations\n * (per research Finding 7 -- no Buffer conversion needed).\n */\n\nimport type BetterSqlite3 from 'better-sqlite3';\n\nimport { debug } from '../shared/debug.js';\n\n/** A single search result with observation ID and cosine distance. */\nexport interface EmbeddingSearchResult {\n  observationId: string;\n  distance: number;\n}\n\n/**\n * Data layer for vector insert/query against the cosine-distance vec0 table.\n *\n * All methods catch errors internally and return empty/default values for\n * graceful degradation (DQ-03). Uses debug('embed', ...) logging.\n */\nexport class EmbeddingStore {\n  private stmtInsert: BetterSqlite3.Statement;\n  private stmtSearch: BetterSqlite3.Statement;\n  private stmtDelete: BetterSqlite3.Statement;\n  private stmtExists: BetterSqlite3.Statement;\n  private stmtFindUnembedded: BetterSqlite3.Statement;\n\n  constructor(\n    private db: BetterSqlite3.Database,\n    private projectHash: string,\n  ) {\n    this.stmtInsert = db.prepare(\n      'INSERT OR REPLACE INTO observation_embeddings(observation_id, embedding) VALUES (?, ?)',\n    );\n\n    this.stmtSearch = db.prepare(`\n      SELECT observation_id, distance\n      FROM observation_embeddings\n      WHERE embedding MATCH ?\n        AND observation_id IN (\n          SELECT id FROM observations WHERE project_hash = ? AND deleted_at IS NULL\n        )\n      ORDER BY distance\n      LIMIT ?\n    `);\n\n    this.stmtDelete = db.prepare(\n      'DELETE FROM observation_embeddings WHERE observation_id = ?',\n    );\n\n    this.stmtExists = db.prepare(\n      'SELECT 1 FROM observation_embeddings WHERE observation_id = ?',\n    );\n\n    this.stmtFindUnembedded = db.prepare(`\n      SELECT id FROM observations\n      WHERE project_hash = ?\n        AND deleted_at IS NULL\n        AND id NOT IN (SELECT observation_id FROM observation_embeddings)\n      LIMIT ?\n    `);\n  }\n\n  /**\n   * Stores an embedding for an observation.\n   *\n   * Uses INSERT OR REPLACE so re-embedding an observation overwrites\n   * the old vector.\n   */\n  store(observationId: string, embedding: Float32Array): void {\n    try {\n      this.stmtInsert.run(observationId, embedding);\n      debug('embed', 'Stored embedding', { observationId, dimensions: embedding.length });\n    } catch (err) {\n      debug('embed', 'Failed to store embedding', {\n        observationId,\n        error: String(err),\n      });\n    }\n  }\n\n  /**\n   * Project-scoped KNN search using cosine distance.\n   *\n   * Returns the nearest observations ordered by distance (ascending).\n   * Only returns observations belonging to this store's project that\n   * have not been soft-deleted.\n   */\n  search(queryEmbedding: Float32Array, limit = 20): EmbeddingSearchResult[] {\n    try {\n      const rows = this.stmtSearch.all(\n        queryEmbedding,\n        this.projectHash,\n        limit,\n      ) as Array<{ observation_id: string; distance: number }>;\n\n      debug('embed', 'Search completed', {\n        results: rows.length,\n        limit,\n      });\n\n      return rows.map((row) => ({\n        observationId: row.observation_id,\n        distance: row.distance,\n      }));\n    } catch (err) {\n      debug('embed', 'Search failed', { error: String(err) });\n      return [];\n    }\n  }\n\n  /**\n   * Removes the embedding for a deleted observation.\n   */\n  delete(observationId: string): void {\n    try {\n      this.stmtDelete.run(observationId);\n      debug('embed', 'Deleted embedding', { observationId });\n    } catch (err) {\n      debug('embed', 'Failed to delete embedding', {\n        observationId,\n        error: String(err),\n      });\n    }\n  }\n\n  /**\n   * Checks if an observation has an embedding stored.\n   */\n  has(observationId: string): boolean {\n    try {\n      const row = this.stmtExists.get(observationId);\n      return row !== undefined;\n    } catch (err) {\n      debug('embed', 'Failed to check embedding existence', {\n        observationId,\n        error: String(err),\n      });\n      return false;\n    }\n  }\n\n  /**\n   * Finds observation IDs that need embeddings generated.\n   *\n   * Returns IDs of observations belonging to this project that are\n   * not soft-deleted and have no entry in the embeddings table.\n   */\n  findUnembedded(limit = 50): string[] {\n    try {\n      const rows = this.stmtFindUnembedded.all(\n        this.projectHash,\n        limit,\n      ) as Array<{ id: string }>;\n\n      debug('embed', 'Found unembedded observations', { count: rows.length, limit });\n\n      return rows.map((row) => row.id);\n    } catch (err) {\n      debug('embed', 'Failed to find unembedded observations', {\n        error: String(err),\n      });\n      return [];\n    }\n  }\n}\n","import type BetterSqlite3 from 'better-sqlite3';\nimport { randomBytes } from 'node:crypto';\n\nimport { debug } from '../shared/debug.js';\nimport type {\n  ContextStash,\n  StashObservation,\n  CreateStashInput,\n} from '../types/stash.js';\n\n/**\n * Raw context_stashes row from SQLite (snake_case column names).\n */\ninterface StashRow {\n  id: string;\n  project_id: string;\n  session_id: string;\n  topic_label: string;\n  summary: string;\n  observation_snapshots: string; // JSON string\n  observation_ids: string; // JSON string\n  status: string;\n  created_at: string;\n  resumed_at: string | null;\n}\n\n/**\n * Maps a snake_case StashRow to a camelCase ContextStash interface.\n * JSON-parses observation_snapshots and observation_ids from their\n * serialized TEXT column format back into arrays.\n */\nfunction rowToStash(row: StashRow): ContextStash {\n  return {\n    id: row.id,\n    projectId: row.project_id,\n    sessionId: row.session_id,\n    topicLabel: row.topic_label,\n    summary: row.summary,\n    observationIds: JSON.parse(row.observation_ids) as string[],\n    observationSnapshots: JSON.parse(\n      row.observation_snapshots,\n    ) as StashObservation[],\n    createdAt: row.created_at,\n    resumedAt: row.resumed_at,\n    status: row.status as ContextStash['status'],\n  };\n}\n\n/**\n * Repository for context stash CRUD operations.\n *\n * Manages the lifecycle of stashed context threads: creating snapshots\n * when topic shifts are detected, listing available stashes, retrieving\n * full stash records, resuming stashes, and deleting them.\n *\n * All SQL statements are prepared once in the constructor and reused\n * for every call (better-sqlite3 performance best practice).\n */\nexport class StashManager {\n  private readonly db: BetterSqlite3.Database;\n\n  // Prepared statements\n  private readonly stmtInsert: BetterSqlite3.Statement;\n  private readonly stmtGetById: BetterSqlite3.Statement;\n  private readonly stmtResume: BetterSqlite3.Statement;\n  private readonly stmtDelete: BetterSqlite3.Statement;\n\n  constructor(db: BetterSqlite3.Database) {\n    this.db = db;\n\n    this.stmtInsert = db.prepare(`\n      INSERT INTO context_stashes (id, project_id, session_id, topic_label, summary, observation_snapshots, observation_ids, status)\n      VALUES (?, ?, ?, ?, ?, ?, ?, 'stashed')\n    `);\n\n    this.stmtGetById = db.prepare(`\n      SELECT * FROM context_stashes WHERE id = ?\n    `);\n\n    this.stmtResume = db.prepare(`\n      UPDATE context_stashes\n      SET status = 'resumed', resumed_at = datetime('now')\n      WHERE id = ?\n    `);\n\n    this.stmtDelete = db.prepare(`\n      DELETE FROM context_stashes WHERE id = ?\n    `);\n\n    debug('db', 'StashManager initialized');\n  }\n\n  /**\n   * Creates a new stash record from a context thread snapshot.\n   * JSON-serializes observation snapshots and IDs for TEXT column storage.\n   * Uses randomBytes(16) hex for ID generation (matches ObservationRepository pattern).\n   */\n  createStash(input: CreateStashInput): ContextStash {\n    const id = randomBytes(16).toString('hex');\n    const observationIds = input.observations.map((o) => o.id);\n    const snapshotsJson = JSON.stringify(input.observations);\n    const idsJson = JSON.stringify(observationIds);\n\n    debug('db', 'Creating stash', {\n      topicLabel: input.topicLabel,\n      observationCount: input.observations.length,\n    });\n\n    this.stmtInsert.run(\n      id,\n      input.projectId,\n      input.sessionId,\n      input.topicLabel,\n      input.summary,\n      snapshotsJson,\n      idsJson,\n    );\n\n    const row = this.stmtGetById.get(id) as StashRow | undefined;\n    if (!row) {\n      throw new Error('Failed to retrieve newly created stash');\n    }\n\n    debug('db', 'Stash created', { id });\n\n    return rowToStash(row);\n  }\n\n  /**\n   * Lists stashes for a project, ordered by created_at DESC.\n   * Supports optional filtering by session_id and status.\n   */\n  listStashes(\n    projectId: string,\n    options?: { sessionId?: string; status?: string; limit?: number },\n  ): ContextStash[] {\n    const limit = options?.limit ?? 10;\n\n    let sql =\n      'SELECT * FROM context_stashes WHERE project_id = ?';\n    const params: unknown[] = [projectId];\n\n    if (options?.sessionId) {\n      sql += ' AND session_id = ?';\n      params.push(options.sessionId);\n    }\n\n    if (options?.status) {\n      sql += ' AND status = ?';\n      params.push(options.status);\n    }\n\n    sql += ' ORDER BY created_at DESC LIMIT ?';\n    params.push(limit);\n\n    debug('db', 'Listing stashes', { projectId, ...options });\n\n    const rows = this.db.prepare(sql).all(...params) as StashRow[];\n    return rows.map(rowToStash);\n  }\n\n  /**\n   * Retrieves a single stash by ID with full observation snapshot data.\n   * Returns null for nonexistent IDs.\n   */\n  getStash(id: string): ContextStash | null {\n    const row = this.stmtGetById.get(id) as StashRow | undefined;\n    return row ? rowToStash(row) : null;\n  }\n\n  /**\n   * Marks a stash as resumed and sets resumed_at timestamp.\n   * Returns the updated record.\n   * Throws if the stash does not exist.\n   */\n  resumeStash(id: string): ContextStash {\n    const result = this.stmtResume.run(id);\n\n    if (result.changes === 0) {\n      throw new Error(`Stash not found: ${id}`);\n    }\n\n    debug('db', 'Stash resumed', { id });\n\n    const row = this.stmtGetById.get(id) as StashRow | undefined;\n    if (!row) {\n      throw new Error(`Failed to retrieve resumed stash: ${id}`);\n    }\n\n    return rowToStash(row);\n  }\n\n  /**\n   * Hard-deletes a stash record.\n   */\n  deleteStash(id: string): void {\n    this.stmtDelete.run(id);\n    debug('db', 'Stash deleted', { id });\n  }\n\n  /**\n   * Returns stashes with status='stashed' (excludes resumed) for a project,\n   * ordered by created_at DESC.\n   */\n  getRecentStashes(projectId: string, limit?: number): ContextStash[] {\n    return this.listStashes(projectId, {\n      status: 'stashed',\n      limit: limit ?? 10,\n    });\n  }\n}\n","import type BetterSqlite3 from 'better-sqlite3';\n\nimport { debug } from '../shared/debug.js';\nimport type { ThresholdState } from '../intelligence/adaptive-threshold.js';\n\n/**\n * Result of loading historical seed data for cold start.\n */\nexport interface HistoricalSeed {\n  /** Average EWMA distance across recent sessions */\n  averageDistance: number;\n  /** Average EWMA variance across recent sessions */\n  averageVariance: number;\n}\n\n/**\n * Persists and loads EWMA threshold history for session seeding.\n *\n * At the end of each session, the final EWMA state is saved via\n * saveSessionThreshold(). When a new session starts, loadHistoricalSeed()\n * computes averages from the last 10 sessions to bootstrap the EWMA\n * without cold-start problems.\n *\n * All SQL statements are prepared once in the constructor and reused\n * for every call (better-sqlite3 performance best practice).\n */\nexport class ThresholdStore {\n  private readonly db: BetterSqlite3.Database;\n\n  // Prepared statements\n  private readonly stmtInsert: BetterSqlite3.Statement;\n  private readonly stmtLoadSeed: BetterSqlite3.Statement;\n\n  constructor(db: BetterSqlite3.Database) {\n    this.db = db;\n\n    this.stmtInsert = db.prepare(`\n      INSERT INTO threshold_history (project_id, session_id, final_ewma_distance, final_ewma_variance, observation_count)\n      VALUES (?, ?, ?, ?, ?)\n    `);\n\n    this.stmtLoadSeed = db.prepare(`\n      SELECT\n        AVG(final_ewma_distance) AS avg_distance,\n        AVG(final_ewma_variance) AS avg_variance\n      FROM (\n        SELECT final_ewma_distance, final_ewma_variance\n        FROM threshold_history\n        WHERE project_id = ?\n        ORDER BY created_at DESC\n        LIMIT 10\n      )\n    `);\n\n    debug('db', 'ThresholdStore initialized');\n  }\n\n  /**\n   * Persist the final EWMA state of a session for future seeding.\n   */\n  saveSessionThreshold(\n    projectId: string,\n    sessionId: string,\n    state: ThresholdState,\n  ): void {\n    this.stmtInsert.run(\n      projectId,\n      sessionId,\n      state.ewmaDistance,\n      state.ewmaVariance,\n      state.observationCount,\n    );\n\n    debug('db', 'Threshold saved', {\n      projectId,\n      sessionId,\n      ewmaDistance: state.ewmaDistance,\n      observations: state.observationCount,\n    });\n  }\n\n  /**\n   * Load historical seed by averaging the last 10 sessions for a project.\n   *\n   * Returns null if no history exists for this project.\n   */\n  loadHistoricalSeed(projectId: string): HistoricalSeed | null {\n    const row = this.stmtLoadSeed.get(projectId) as {\n      avg_distance: number | null;\n      avg_variance: number | null;\n    };\n\n    if (row.avg_distance === null || row.avg_variance === null) {\n      debug('db', 'No threshold history found', { projectId });\n      return null;\n    }\n\n    debug('db', 'Threshold seed loaded', {\n      projectId,\n      avgDistance: row.avg_distance,\n      avgVariance: row.avg_variance,\n    });\n\n    return {\n      averageDistance: row.avg_distance,\n      averageVariance: row.avg_variance,\n    };\n  }\n}\n","import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\n\nimport { debug } from '../shared/debug.js';\n\nexport function createServer(): McpServer {\n  return new McpServer(\n    { name: 'laminark', version: '0.1.0' },\n    { capabilities: { tools: {} } },\n  );\n}\n\nexport async function startServer(server: McpServer): Promise<void> {\n  const transport = new StdioServerTransport();\n  await server.connect(transport);\n  debug('mcp', 'MCP server started on stdio transport');\n}\n","/**\n * Hybrid search combining FTS5 keyword results and vec0 vector results\n * using reciprocal rank fusion (RRF).\n *\n * When both keyword and vector results are available, RRF merges the two\n * ranked lists into a single score-sorted list. When only keyword results\n * are available (worker not ready, no embeddings), falls back transparently.\n */\n\nimport { debug, debugTimed } from '../shared/debug.js';\nimport type { SearchResult } from '../shared/types.js';\nimport type { SearchEngine } from '../storage/search.js';\nimport type { EmbeddingStore, EmbeddingSearchResult } from '../storage/embeddings.js';\nimport type { AnalysisWorker } from '../analysis/worker-bridge.js';\nimport { ObservationRepository } from '../storage/observations.js';\n\n// ---------------------------------------------------------------------------\n// Types\n// ---------------------------------------------------------------------------\n\ninterface RankedItem {\n  id: string;\n  [key: string]: unknown;\n}\n\ninterface FusedResult {\n  id: string;\n  fusedScore: number;\n}\n\nexport interface HybridSearchParams {\n  searchEngine: SearchEngine;\n  embeddingStore: EmbeddingStore;\n  worker: AnalysisWorker | null;\n  query: string;\n  db: import('better-sqlite3').Database;\n  projectHash: string;\n  options?: { limit?: number; sessionId?: string };\n}\n\n// ---------------------------------------------------------------------------\n// Reciprocal Rank Fusion\n// ---------------------------------------------------------------------------\n\n/**\n * Merges multiple ranked lists into a single fused ranking using RRF.\n *\n * For each document across all lists, computes:\n *   fusedScore = sum(1 / (k + rank + 1))\n * where rank is the 0-based position in each list.\n *\n * @param rankedLists - Arrays of ranked items, each with an `id` field\n * @param k - Smoothing constant (default 60, standard RRF value)\n * @returns Fused results sorted by fusedScore descending\n */\nexport function reciprocalRankFusion(\n  rankedLists: Array<Array<RankedItem>>,\n  k = 60,\n): FusedResult[] {\n  const scores = new Map<string, number>();\n\n  for (const list of rankedLists) {\n    for (let rank = 0; rank < list.length; rank++) {\n      const item = list[rank];\n      const current = scores.get(item.id) ?? 0;\n      scores.set(item.id, current + 1 / (k + rank + 1));\n    }\n  }\n\n  const results: FusedResult[] = [];\n  for (const [id, fusedScore] of scores) {\n    results.push({ id, fusedScore });\n  }\n\n  results.sort((a, b) => b.fusedScore - a.fusedScore);\n  return results;\n}\n\n// ---------------------------------------------------------------------------\n// Hybrid Search\n// ---------------------------------------------------------------------------\n\n/**\n * Combines FTS5 keyword search and vec0 vector search using RRF.\n *\n * Falls back to keyword-only when:\n * - Worker is null or not ready\n * - Query embedding fails\n * - No vector results returned\n *\n * @returns SearchResult[] with matchType indicating source(s)\n */\nexport async function hybridSearch(\n  params: HybridSearchParams,\n): Promise<SearchResult[]> {\n  const { searchEngine, embeddingStore, worker, query, db, projectHash, options } = params;\n  const limit = options?.limit ?? 20;\n\n  return debugTimed('search', 'Hybrid search', async () => {\n    // Step 1: Always run keyword search\n    const keywordResults = searchEngine.searchKeyword(query, {\n      limit,\n      sessionId: options?.sessionId,\n    });\n\n    debug('search', 'Keyword results', { count: keywordResults.length });\n\n    // Step 2: Attempt vector search if worker is available\n    let vectorResults: EmbeddingSearchResult[] = [];\n\n    if (worker && worker.isReady()) {\n      const queryEmbedding = await worker.embed(query);\n\n      if (queryEmbedding) {\n        // Fetch more vector results than limit to improve fusion quality\n        vectorResults = embeddingStore.search(queryEmbedding, limit * 2);\n        debug('search', 'Vector results', { count: vectorResults.length });\n      } else {\n        debug('search', 'Query embedding failed, keyword-only');\n      }\n    } else {\n      debug('search', 'Worker not ready, keyword-only');\n    }\n\n    // Step 3: Keyword-only fallback\n    if (vectorResults.length === 0) {\n      debug('search', 'Returning keyword-only results', { count: keywordResults.length });\n      return keywordResults;\n    }\n\n    // Step 4: Fuse keyword + vector results with RRF\n    const keywordRanked: RankedItem[] = keywordResults.map((r) => ({\n      id: r.observation.id,\n    }));\n\n    const vectorRanked: RankedItem[] = vectorResults.map((r) => ({\n      id: r.observationId,\n    }));\n\n    const fused = reciprocalRankFusion([keywordRanked, vectorRanked]);\n\n    // Build lookup maps\n    const keywordMap = new Map<string, SearchResult>();\n    for (const r of keywordResults) {\n      keywordMap.set(r.observation.id, r);\n    }\n\n    const vectorIdSet = new Set(vectorResults.map((r) => r.observationId));\n\n    // We need an ObservationRepository to look up vector-only observations\n    const obsRepo = new ObservationRepository(db, projectHash);\n\n    // Merge results\n    const merged: SearchResult[] = [];\n\n    for (const item of fused) {\n      if (merged.length >= limit) break;\n\n      const fromKeyword = keywordMap.get(item.id);\n      const fromVector = vectorIdSet.has(item.id);\n\n      if (fromKeyword && fromVector) {\n        // In both: hybrid match, use keyword snippet\n        merged.push({\n          observation: fromKeyword.observation,\n          score: item.fusedScore,\n          matchType: 'hybrid',\n          snippet: fromKeyword.snippet,\n        });\n      } else if (fromKeyword) {\n        // Keyword only\n        merged.push({\n          observation: fromKeyword.observation,\n          score: item.fusedScore,\n          matchType: 'fts',\n          snippet: fromKeyword.snippet,\n        });\n      } else if (fromVector) {\n        // Vector only -- need to load observation\n        const obs = obsRepo.getById(item.id);\n        if (obs) {\n          const snippet = (obs.content ?? '').replace(/\\n/g, ' ').slice(0, 100);\n          merged.push({\n            observation: obs,\n            score: item.fusedScore,\n            matchType: 'vector',\n            snippet,\n          });\n        }\n      }\n    }\n\n    debug('search', 'Hybrid search complete', {\n      keyword: keywordResults.length,\n      vector: vectorResults.length,\n      fused: merged.length,\n      hybrid: merged.filter((r) => r.matchType === 'hybrid').length,\n    });\n\n    return merged;\n  });\n}\n","export const TOKEN_BUDGET = 2000;\nexport const FULL_VIEW_BUDGET = 4000;\n\nexport function estimateTokens(text: string): number {\n  return Math.ceil(text.length / 4);\n}\n\nexport function enforceTokenBudget<T>(\n  results: T[],\n  formatResult: (item: T) => string,\n  budget: number = TOKEN_BUDGET,\n): { items: T[]; truncated: boolean; tokenEstimate: number } {\n  const METADATA_RESERVE = 100;\n  const effectiveBudget = budget - METADATA_RESERVE;\n  let totalTokens = 0;\n  const items: T[] = [];\n\n  for (const result of results) {\n    const formatted = formatResult(result);\n    const tokens = estimateTokens(formatted);\n    if (totalTokens + tokens > effectiveBudget && items.length > 0) {\n      return { items, truncated: true, tokenEstimate: totalTokens };\n    }\n    items.push(result);\n    totalTokens += tokens;\n  }\n\n  return { items, truncated: false, tokenEstimate: totalTokens };\n}\n","import type { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport type BetterSqlite3 from 'better-sqlite3';\nimport { z } from 'zod';\n\nimport { debug } from '../../shared/debug.js';\nimport type { Observation, SearchResult } from '../../shared/types.js';\nimport { ObservationRepository } from '../../storage/observations.js';\nimport { SearchEngine } from '../../storage/search.js';\nimport type { EmbeddingStore } from '../../storage/embeddings.js';\nimport type { AnalysisWorker } from '../../analysis/worker-bridge.js';\nimport type { NotificationStore } from '../../storage/notifications.js';\nimport { hybridSearch } from '../../search/hybrid.js';\nimport {\n  enforceTokenBudget,\n  estimateTokens,\n  FULL_VIEW_BUDGET,\n  TOKEN_BUDGET,\n} from '../token-budget.js';\n\n// ---------------------------------------------------------------------------\n// Formatting helpers\n// ---------------------------------------------------------------------------\n\nfunction shortId(id: string): string {\n  return id.slice(0, 8);\n}\n\nfunction dateStr(iso: string): string {\n  return iso.slice(0, 10); // YYYY-MM-DD\n}\n\nfunction timeStr(iso: string): string {\n  return iso.slice(11, 16); // HH:MM\n}\n\nfunction snippetText(content: string, maxLen: number): string {\n  return content.replace(/\\n/g, ' ').slice(0, maxLen);\n}\n\n// ---------------------------------------------------------------------------\n// Detail-level formatters\n// ---------------------------------------------------------------------------\n\nfunction formatCompactItem(\n  obs: Observation,\n  index: number,\n  score?: number,\n): string {\n  const idShort = shortId(obs.id);\n  const title = obs.title ?? 'untitled';\n  const scoreStr = score !== undefined ? score.toFixed(2) : '-';\n  const snippet = snippetText(obs.content, 100);\n  const date = dateStr(obs.createdAt);\n  return `[${index}] ${idShort} | ${title} | ${scoreStr} | ${snippet} | ${date}`;\n}\n\nfunction formatTimelineGroup(\n  date: string,\n  items: { obs: Observation; score?: number }[],\n): string {\n  const lines = [`## ${date}`];\n  for (const { obs } of items) {\n    const time = timeStr(obs.createdAt);\n    const title = obs.title ?? 'untitled';\n    const source = obs.source;\n    const snippet = snippetText(obs.content, 150);\n    lines.push(`${time} | ${title} | ${source} | ${snippet}`);\n  }\n  return lines.join('\\n');\n}\n\nfunction formatFullItem(obs: Observation): string {\n  const idShort = shortId(obs.id);\n  const title = obs.title ?? 'untitled';\n  return `--- ${idShort} | ${title} | ${obs.createdAt} ---\\n${obs.content}`;\n}\n\n// ---------------------------------------------------------------------------\n// Response helpers\n// ---------------------------------------------------------------------------\n\nfunction prependNotifications(\n  notificationStore: NotificationStore | null,\n  projectHash: string,\n  responseText: string,\n): string {\n  if (!notificationStore) return responseText;\n  const pending = notificationStore.consumePending(projectHash);\n  if (pending.length === 0) return responseText;\n  const banner = pending.map(n => `[Laminark] ${n.message}`).join('\\n');\n  return banner + '\\n\\n' + responseText;\n}\n\nfunction textResponse(text: string) {\n  return { content: [{ type: 'text' as const, text }] };\n}\n\nfunction errorResponse(text: string) {\n  return { content: [{ type: 'text' as const, text }], isError: true };\n}\n\n// ---------------------------------------------------------------------------\n// registerRecall\n// ---------------------------------------------------------------------------\n\nexport function registerRecall(\n  server: McpServer,\n  db: BetterSqlite3.Database,\n  projectHash: string,\n  worker: AnalysisWorker | null = null,\n  embeddingStore: EmbeddingStore | null = null,\n  notificationStore: NotificationStore | null = null,\n): void {\n  server.registerTool(\n    'recall',\n    {\n      title: 'Recall Memories',\n      description:\n        'Search, view, purge, or restore memories. Search first to find matches, then act on specific results by ID.',\n      inputSchema: {\n        query: z\n          .string()\n          .optional()\n          .describe('FTS5 keyword search query'),\n        id: z.string().optional().describe('Direct lookup by observation ID'),\n        title: z\n          .string()\n          .optional()\n          .describe('Search by title (partial match)'),\n        action: z\n          .enum(['view', 'purge', 'restore'])\n          .default('view')\n          .describe(\n            'Action to take on results: view (show details), purge (soft-delete), restore (un-delete)',\n          ),\n        ids: z\n          .array(z.string())\n          .optional()\n          .describe(\n            'Specific observation IDs to act on (from a previous search result)',\n          ),\n        detail: z\n          .enum(['compact', 'timeline', 'full'])\n          .default('compact')\n          .describe(\n            'View detail level: compact (index ~80 tokens/result), timeline (date-grouped), full (complete text)',\n          ),\n        limit: z\n          .number()\n          .int()\n          .min(1)\n          .max(50)\n          .default(10)\n          .describe('Maximum results to return'),\n        include_purged: z\n          .boolean()\n          .default(false)\n          .describe(\n            'Include soft-deleted items in results (needed for restore)',\n          ),\n      },\n    },\n    async (args) => {\n      // Helper to wrap textResponse with pending notifications\n      const withNotifications = (text: string) =>\n        textResponse(prependNotifications(notificationStore, projectHash, text));\n\n      try {\n        const repo = new ObservationRepository(db, projectHash);\n        const searchEngine = new SearchEngine(db, projectHash);\n\n        // -------------------------------------------------------------------\n        // PHASE A: Input Validation\n        // -------------------------------------------------------------------\n        const hasSearch = args.query !== undefined || args.id !== undefined || args.title !== undefined;\n        if (args.ids && hasSearch) {\n          return errorResponse(\n            'Provide either a search query or IDs to act on, not both.',\n          );\n        }\n\n        if (\n          (args.action === 'purge' || args.action === 'restore') &&\n          !args.ids &&\n          !args.id\n        ) {\n          return errorResponse(\n            `Provide ids array or id to specify which memories to ${args.action}.`,\n          );\n        }\n\n        // -------------------------------------------------------------------\n        // PHASE B: Resolve Observations\n        // -------------------------------------------------------------------\n        let observations: Observation[] = [];\n        let searchResults: SearchResult[] | null = null;\n\n        if (args.ids) {\n          // Fetch each ID, track not-found\n          const notFound: string[] = [];\n          for (const itemId of args.ids) {\n            const obs = repo.getByIdIncludingDeleted(itemId);\n            if (obs) {\n              observations.push(obs);\n            } else {\n              notFound.push(itemId);\n            }\n          }\n          if (notFound.length > 0 && observations.length === 0) {\n            return withNotifications(\n              `No memories found matching '${notFound.join(', ')}'. Try broader search terms or check the ID.`,\n            );\n          }\n        } else if (args.id) {\n          const obs = args.include_purged\n            ? repo.getByIdIncludingDeleted(args.id)\n            : repo.getById(args.id);\n          if (!obs) {\n            return withNotifications(\n              `No memories found matching '${args.id}'. Try broader search terms or check the ID.`,\n            );\n          }\n          observations = [obs];\n        } else if (args.query) {\n          if (embeddingStore) {\n            searchResults = await hybridSearch({\n              searchEngine,\n              embeddingStore,\n              worker,\n              query: args.query,\n              db,\n              projectHash,\n              options: { limit: args.limit },\n            });\n          } else {\n            searchResults = searchEngine.searchKeyword(args.query, {\n              limit: args.limit,\n            });\n          }\n          observations = searchResults.map((r) => r.observation);\n        } else if (args.title) {\n          observations = repo.getByTitle(args.title, {\n            limit: args.limit,\n            includePurged: args.include_purged,\n          });\n        } else {\n          // No query, id, title, or ids -- list recent\n          observations = args.include_purged\n            ? repo.listIncludingDeleted({ limit: args.limit })\n            : repo.list({ limit: args.limit });\n        }\n\n        if (observations.length === 0) {\n          const searchTerm = args.query ?? args.title ?? args.id ?? '';\n          return withNotifications(\n            `No memories found matching '${searchTerm}'. Try broader search terms or check the ID.`,\n          );\n        }\n\n        // -------------------------------------------------------------------\n        // PHASE C: Execute Action\n        // -------------------------------------------------------------------\n\n        // --- VIEW ---\n        if (args.action === 'view') {\n          const viewResponse = formatViewResponse(\n            observations,\n            searchResults,\n            args.detail,\n            args.id !== undefined,\n          );\n          // Prepend notifications to view response text\n          const originalText = viewResponse.content[0].text;\n          return textResponse(prependNotifications(notificationStore, projectHash, originalText));\n        }\n\n        // --- PURGE ---\n        if (args.action === 'purge') {\n          const targetIds = args.ids ?? (args.id ? [args.id] : []);\n          let success = 0;\n          const failures: string[] = [];\n          for (const targetId of targetIds) {\n            if (repo.softDelete(targetId)) {\n              success++;\n            } else {\n              failures.push(targetId);\n            }\n          }\n          debug('mcp', 'recall: purge', { success, total: targetIds.length });\n          let msg = `Purged ${success}/${targetIds.length} memories.`;\n          if (failures.length > 0) {\n            msg += ` Not found or already purged: ${failures.join(', ')}`;\n          }\n          return withNotifications(msg);\n        }\n\n        // --- RESTORE ---\n        if (args.action === 'restore') {\n          const targetIds = args.ids ?? (args.id ? [args.id] : []);\n          let success = 0;\n          const failures: string[] = [];\n          for (const targetId of targetIds) {\n            if (repo.restore(targetId)) {\n              success++;\n            } else {\n              failures.push(targetId);\n            }\n          }\n          debug('mcp', 'recall: restore', {\n            success,\n            total: targetIds.length,\n          });\n          let msg = `Restored ${success}/${targetIds.length} memories.`;\n          if (failures.length > 0) {\n            msg += ` Not found: ${failures.join(', ')}`;\n          }\n          return withNotifications(msg);\n        }\n\n        // Should not reach here, but TypeScript exhaustiveness\n        return errorResponse(`Unknown action: ${args.action as string}`);\n      } catch (err) {\n        const message =\n          err instanceof Error ? err.message : 'Unknown error';\n        debug('mcp', 'recall: error', { error: message });\n        return errorResponse(`Recall error: ${message}`);\n      }\n    },\n  );\n}\n\n// ---------------------------------------------------------------------------\n// View formatting with token budget\n// ---------------------------------------------------------------------------\n\nfunction formatViewResponse(\n  observations: Observation[],\n  searchResults: SearchResult[] | null,\n  detail: 'compact' | 'timeline' | 'full',\n  isSingleIdLookup: boolean,\n): { content: { type: 'text'; text: string }[] } {\n  let body: string;\n  let truncated: boolean;\n  let tokenEstimate: number;\n\n  if (detail === 'compact') {\n    const scoreMap = buildScoreMap(searchResults);\n    const result = enforceTokenBudget(\n      observations,\n      (obs) => formatCompactItem(obs, observations.indexOf(obs) + 1, scoreMap.get(obs.id)),\n      TOKEN_BUDGET,\n    );\n    body = result.items\n      .map((obs, i) => formatCompactItem(obs, i + 1, scoreMap.get(obs.id)))\n      .join('\\n');\n    truncated = result.truncated;\n    tokenEstimate = result.tokenEstimate;\n  } else if (detail === 'timeline') {\n    // Group by date\n    const groups = new Map<string, { obs: Observation; score?: number }[]>();\n    const scoreMap = buildScoreMap(searchResults);\n    for (const obs of observations) {\n      const date = dateStr(obs.createdAt);\n      if (!groups.has(date)) {\n        groups.set(date, []);\n      }\n      groups.get(date)!.push({ obs, score: scoreMap.get(obs.id) });\n    }\n\n    const result = enforceTokenBudget(\n      observations,\n      (obs) => {\n        const time = timeStr(obs.createdAt);\n        const title = obs.title ?? 'untitled';\n        return `${time} | ${title} | ${obs.source} | ${snippetText(obs.content, 150)}`;\n      },\n      TOKEN_BUDGET,\n    );\n\n    // Re-group the budget-enforced items\n    const includedIds = new Set(result.items.map((o) => o.id));\n    const filteredGroups = new Map<string, { obs: Observation; score?: number }[]>();\n    for (const [date, items] of groups) {\n      const filtered = items.filter((item) => includedIds.has(item.obs.id));\n      if (filtered.length > 0) {\n        filteredGroups.set(date, filtered);\n      }\n    }\n\n    body = Array.from(filteredGroups.entries())\n      .map(([date, items]) => formatTimelineGroup(date, items))\n      .join('\\n\\n');\n    truncated = result.truncated;\n    tokenEstimate = result.tokenEstimate;\n  } else {\n    // detail === 'full'\n    const budget = isSingleIdLookup ? FULL_VIEW_BUDGET : TOKEN_BUDGET;\n\n    if (observations.length === 1) {\n      const formatted = formatFullItem(observations[0]);\n      tokenEstimate = estimateTokens(formatted);\n      if (tokenEstimate > budget) {\n        // Truncate single item to fit budget\n        const maxChars = budget * 4; // ~4 chars per token\n        body =\n          formatted.slice(0, maxChars) +\n          `\\n[...truncated at ~${budget} tokens]`;\n        truncated = true;\n        tokenEstimate = budget;\n      } else {\n        body = formatted;\n        truncated = false;\n      }\n    } else {\n      const result = enforceTokenBudget(\n        observations,\n        formatFullItem,\n        budget,\n      );\n      body = result.items.map(formatFullItem).join('\\n\\n');\n      truncated = result.truncated;\n      tokenEstimate = result.tokenEstimate;\n    }\n  }\n\n  // Metadata footer\n  let footer = `---\\n${observations.length} result(s) | ~${tokenEstimate} tokens | detail: ${detail}`;\n  if (truncated) {\n    footer += ' | truncated (use id for full view)';\n  }\n\n  return textResponse(`${body}\\n${footer}`);\n}\n\n// ---------------------------------------------------------------------------\n// Helpers\n// ---------------------------------------------------------------------------\n\nfunction buildScoreMap(\n  searchResults: SearchResult[] | null,\n): Map<string, number> {\n  const map = new Map<string, number>();\n  if (searchResults) {\n    for (const r of searchResults) {\n      map.set(r.observation.id, r.score);\n    }\n  }\n  return map;\n}\n","import type { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport type BetterSqlite3 from 'better-sqlite3';\nimport { z } from 'zod';\n\nimport { debug } from '../../shared/debug.js';\nimport { ObservationRepository } from '../../storage/observations.js';\nimport type { NotificationStore } from '../../storage/notifications.js';\n\n/**\n * Generates a title from observation content.\n * Extracts the first sentence (up to 100 chars) or first 80 chars with ellipsis.\n */\nexport function generateTitle(content: string): string {\n  const firstSentence = content.match(/^[^.!?\\n]+[.!?]?/);\n  if (firstSentence && firstSentence[0].length <= 100) {\n    return firstSentence[0].trim();\n  }\n  if (content.length <= 80) return content.trim();\n  return content.slice(0, 80).trim() + '...';\n}\n\n/**\n * Registers the save_memory tool on the MCP server.\n *\n * save_memory persists user-provided text as a new observation with an optional title.\n * If title is omitted, one is auto-generated from the text content.\n */\nexport function registerSaveMemory(\n  server: McpServer,\n  db: BetterSqlite3.Database,\n  projectHash: string,\n  notificationStore: NotificationStore | null = null,\n): void {\n  server.registerTool(\n    'save_memory',\n    {\n      title: 'Save Memory',\n      description:\n        'Save a new memory observation. Provide text content and an optional title. If title is omitted, one is auto-generated from the text.',\n      inputSchema: {\n        text: z\n          .string()\n          .min(1)\n          .max(10000)\n          .describe('The text content to save as a memory'),\n        title: z\n          .string()\n          .max(200)\n          .optional()\n          .describe(\n            'Optional title for the memory. Auto-generated from text if omitted.',\n          ),\n        source: z\n          .string()\n          .default('manual')\n          .describe(\"Source identifier (e.g., manual, hook:PostToolUse)\"),\n      },\n    },\n    async (args) => {\n      try {\n        const repo = new ObservationRepository(db, projectHash);\n        const resolvedTitle = args.title ?? generateTitle(args.text);\n        const obs = repo.create({\n          content: args.text,\n          title: resolvedTitle,\n          source: args.source,\n        });\n\n        debug('mcp', 'save_memory: saved', {\n          id: obs.id,\n          title: resolvedTitle,\n        });\n\n        // Prepend any pending notifications to the response\n        let responseText = `Saved memory \"${resolvedTitle}\" (id: ${obs.id})`;\n        if (notificationStore) {\n          const pending = notificationStore.consumePending(projectHash);\n          if (pending.length > 0) {\n            const banner = pending.map(n => `[Laminark] ${n.message}`).join('\\n');\n            responseText = banner + '\\n\\n' + responseText;\n          }\n        }\n\n        return {\n          content: [\n            {\n              type: 'text' as const,\n              text: responseText,\n            },\n          ],\n        };\n      } catch (err) {\n        const message =\n          err instanceof Error ? err.message : 'Unknown error';\n        return {\n          content: [\n            { type: 'text' as const, text: `Failed to save: ${message}` },\n          ],\n          isError: true,\n        };\n      }\n    },\n  );\n}\n","import type { StashManager } from '../storage/stash-manager.js';\nimport type { StashObservation } from '../types/stash.js';\n\n/**\n * Result of the /laminark:resume command.\n */\nexport interface ResumeResult {\n  success: boolean;\n  message: string;\n  context?: StashObservation[];\n}\n\n/**\n * Dependencies injected into the resume command handler.\n */\nexport interface ResumeDeps {\n  stashManager: StashManager;\n}\n\n/**\n * Returns a human-readable relative time string from an ISO date.\n * Examples: \"just now\", \"2 minutes ago\", \"3 hours ago\", \"yesterday\", \"5 days ago\"\n */\nexport function timeAgo(dateString: string, now?: Date): string {\n  const date = new Date(dateString);\n  const ref = now ?? new Date();\n  const diffMs = ref.getTime() - date.getTime();\n\n  if (diffMs < 0) return 'just now';\n\n  const seconds = Math.floor(diffMs / 1000);\n  if (seconds < 60) return 'just now';\n\n  const minutes = Math.floor(seconds / 60);\n  if (minutes === 1) return '1 minute ago';\n  if (minutes < 60) return `${minutes} minutes ago`;\n\n  const hours = Math.floor(minutes / 60);\n  if (hours === 1) return '1 hour ago';\n  if (hours < 24) return `${hours} hours ago`;\n\n  const days = Math.floor(hours / 24);\n  if (days === 1) return 'yesterday';\n  if (days < 30) return `${days} days ago`;\n\n  const months = Math.floor(days / 30);\n  if (months === 1) return '1 month ago';\n  return `${months} months ago`;\n}\n\n/**\n * Truncates a string to maxLen characters, appending \"...\" if truncated.\n */\nfunction truncate(text: string, maxLen: number): string {\n  if (text.length <= maxLen) return text;\n  return text.slice(0, maxLen) + '...';\n}\n\n/**\n * Handles the /laminark:resume slash command.\n *\n * Two modes:\n * 1. List mode (no stashId): shows stashed context threads\n * 2. Resume mode (stashId provided): restores context from a specific stash\n */\nexport async function handleResumeCommand(\n  args: { projectId: string; stashId?: string },\n  deps: ResumeDeps,\n): Promise<ResumeResult> {\n  const { stashManager } = deps;\n\n  // Resume mode: restore a specific stash\n  if (args.stashId) {\n    const stash = stashManager.getStash(args.stashId);\n    if (!stash) {\n      return { success: false, message: `Stash not found: ${args.stashId}` };\n    }\n\n    stashManager.resumeStash(args.stashId);\n\n    const count = stash.observationSnapshots.length;\n    return {\n      success: true,\n      message: `Resumed: \"${stash.topicLabel}\"\\n\\nContext restored with ${count} observations.`,\n      context: stash.observationSnapshots,\n    };\n  }\n\n  // List mode: show available stashed threads\n  const stashes = stashManager.listStashes(args.projectId, {\n    status: 'stashed',\n    limit: 5,\n  });\n\n  if (stashes.length === 0) {\n    return { success: true, message: 'No stashed context threads found.' };\n  }\n\n  const lines = ['Stashed context threads:'];\n  for (let i = 0; i < stashes.length; i++) {\n    const s = stashes[i];\n    const ago = timeAgo(s.createdAt);\n    const summary = truncate(s.summary, 80);\n    lines.push(`${i + 1}. ${s.topicLabel} (${ago}) - ${summary}`);\n  }\n  lines.push('');\n  lines.push('Use /laminark:resume {id} to restore a thread.');\n\n  return { success: true, message: lines.join('\\n') };\n}\n","import type { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport type BetterSqlite3 from 'better-sqlite3';\nimport { z } from 'zod';\n\nimport { debug } from '../../shared/debug.js';\nimport { StashManager } from '../../storage/stash-manager.js';\nimport type { NotificationStore } from '../../storage/notifications.js';\nimport type { ContextStash } from '../../types/stash.js';\nimport { timeAgo } from '../../commands/resume.js';\n\n// ---------------------------------------------------------------------------\n// Formatting helpers (progressive disclosure)\n// ---------------------------------------------------------------------------\n\nfunction truncate(text: string, maxLen: number): string {\n  if (text.length <= maxLen) return text;\n  return text.slice(0, maxLen) + '...';\n}\n\n/**\n * Compact format: numbered list of topic labels with relative time.\n */\nfunction formatCompact(stashes: ContextStash[]): string {\n  return stashes\n    .map(\n      (s, i) =>\n        `${i + 1}. ${s.topicLabel} (${timeAgo(s.createdAt)})`,\n    )\n    .join('\\n');\n}\n\n/**\n * Detail format: topic labels with summaries.\n */\nfunction formatDetail(stashes: ContextStash[]): string {\n  return stashes\n    .map(\n      (s, i) =>\n        `${i + 1}. **${s.topicLabel}** (${timeAgo(s.createdAt)})\\n   ${truncate(s.summary, 120)}`,\n    )\n    .join('\\n\\n');\n}\n\n/**\n * Full format: topic labels, summaries, observation count, and first few observation snippets.\n */\nfunction formatFull(stashes: ContextStash[]): string {\n  return stashes\n    .map((s, i) => {\n      const lines = [\n        `${i + 1}. **${s.topicLabel}** (${timeAgo(s.createdAt)})`,\n        `   ${s.summary}`,\n        `   Observations: ${s.observationSnapshots.length}`,\n      ];\n\n      // Show first 3 observation snippets\n      const previews = s.observationSnapshots.slice(0, 3);\n      for (const obs of previews) {\n        lines.push(`   - ${truncate(obs.content.replace(/\\n/g, ' '), 80)}`);\n      }\n      if (s.observationSnapshots.length > 3) {\n        lines.push(\n          `   ... and ${s.observationSnapshots.length - 3} more`,\n        );\n      }\n\n      return lines.join('\\n');\n    })\n    .join('\\n\\n');\n}\n\n/**\n * Formats stashes using progressive disclosure based on count.\n * - 1-3 stashes: full detail\n * - 4-8 stashes: detail (summaries)\n * - 9+: compact (labels only)\n */\nexport function formatStashes(stashes: ContextStash[]): string {\n  if (stashes.length <= 3) return formatFull(stashes);\n  if (stashes.length <= 8) return formatDetail(stashes);\n  return formatCompact(stashes);\n}\n\n// ---------------------------------------------------------------------------\n// Response helpers\n// ---------------------------------------------------------------------------\n\nfunction prependNotifications(\n  notificationStore: NotificationStore | null,\n  projectHash: string,\n  responseText: string,\n): string {\n  if (!notificationStore) return responseText;\n  const pending = notificationStore.consumePending(projectHash);\n  if (pending.length === 0) return responseText;\n  const banner = pending.map(n => `[Laminark] ${n.message}`).join('\\n');\n  return banner + '\\n\\n' + responseText;\n}\n\nfunction textResponse(text: string) {\n  return { content: [{ type: 'text' as const, text }] };\n}\n\n// ---------------------------------------------------------------------------\n// registerTopicContext\n// ---------------------------------------------------------------------------\n\n/**\n * Registers the topic_context MCP tool.\n *\n * Shows recently stashed context threads. Used when the user asks\n * \"where was I?\" or wants to see abandoned conversation threads.\n */\nexport function registerTopicContext(\n  server: McpServer,\n  db: BetterSqlite3.Database,\n  projectHash: string,\n  notificationStore: NotificationStore | null = null,\n): void {\n  const stashManager = new StashManager(db);\n\n  server.registerTool(\n    'topic_context',\n    {\n      title: 'Topic Context',\n      description:\n        \"Shows recently stashed context threads. Use when the user asks 'where was I?' or wants to see abandoned conversation threads.\",\n      inputSchema: {\n        query: z\n          .string()\n          .optional()\n          .describe('Optional search query to filter threads by topic label or summary'),\n        limit: z\n          .number()\n          .int()\n          .min(1)\n          .max(20)\n          .default(5)\n          .describe('Max threads to return'),\n      },\n    },\n    async (args) => {\n      // Helper to wrap textResponse with pending notifications\n      const withNotifications = (text: string) =>\n        textResponse(prependNotifications(notificationStore, projectHash, text));\n\n      try {\n        debug('mcp', 'topic_context: request', { query: args.query, limit: args.limit });\n\n        let stashes = stashManager.getRecentStashes(projectHash, args.limit);\n\n        // Filter by query if provided (case-insensitive match on topicLabel or summary)\n        if (args.query) {\n          const q = args.query.toLowerCase();\n          stashes = stashes.filter(\n            (s) =>\n              s.topicLabel.toLowerCase().includes(q) ||\n              s.summary.toLowerCase().includes(q),\n          );\n        }\n\n        if (stashes.length === 0) {\n          return withNotifications(\n            'No stashed context threads found. You\\'re working in a single thread.',\n          );\n        }\n\n        const formatted = formatStashes(stashes);\n        const footer = `\\n---\\n${stashes.length} stashed thread(s) | Use /laminark:resume {id} to restore`;\n\n        debug('mcp', 'topic_context: returning', { count: stashes.length });\n\n        return withNotifications(formatted + footer);\n      } catch (err) {\n        const message = err instanceof Error ? err.message : 'Unknown error';\n        debug('mcp', 'topic_context: error', { error: message });\n        return textResponse(`Error retrieving context threads: ${message}`);\n      }\n    },\n  );\n}\n","/**\n * Type definitions for the knowledge graph.\n *\n * Defines a fixed entity/relationship taxonomy using const arrays and\n * derived union types (NOT enums) for better type inference and runtime\n * validation. Every Phase 7 module imports from this file.\n */\n\n// =============================================================================\n// Entity Type Taxonomy (FIXED -- no other types allowed)\n// =============================================================================\n\nexport const ENTITY_TYPES = [\n  'Project',\n  'File',\n  'Decision',\n  'Problem',\n  'Solution',\n  'Tool',\n  'Person',\n] as const;\n\nexport type EntityType = (typeof ENTITY_TYPES)[number];\n\n// =============================================================================\n// Relationship Type Taxonomy (FIXED -- no other types allowed)\n// =============================================================================\n\nexport const RELATIONSHIP_TYPES = [\n  'uses',\n  'depends_on',\n  'decided_by',\n  'related_to',\n  'part_of',\n  'caused_by',\n  'solved_by',\n] as const;\n\nexport type RelationshipType = (typeof RELATIONSHIP_TYPES)[number];\n\n// =============================================================================\n// Graph Node Interface\n// =============================================================================\n\n/**\n * A node in the knowledge graph representing a named entity.\n *\n * - id: UUID (hex-encoded randomBytes)\n * - type: one of the 7 entity types\n * - name: canonical name (e.g., \"src/auth/login.ts\" for File, \"Use JWT\" for Decision)\n * - metadata: flexible JSON for type-specific data\n * - observation_ids: source observations this entity was extracted from\n * - created_at / updated_at: ISO 8601 timestamps\n */\nexport interface GraphNode {\n  id: string;\n  type: EntityType;\n  name: string;\n  metadata: Record<string, unknown>;\n  observation_ids: string[];\n  created_at: string;\n  updated_at: string;\n}\n\n// =============================================================================\n// Graph Edge Interface\n// =============================================================================\n\n/**\n * A directed edge in the knowledge graph connecting two nodes.\n *\n * - id: UUID (hex-encoded randomBytes)\n * - source_id / target_id: references to GraphNode.id\n * - type: one of the 7 relationship types\n * - weight: confidence/strength score between 0.0 and 1.0\n * - metadata: flexible JSON for relationship-specific data\n * - created_at: ISO 8601 timestamp\n */\nexport interface GraphEdge {\n  id: string;\n  source_id: string;\n  target_id: string;\n  type: RelationshipType;\n  weight: number;\n  metadata: Record<string, unknown>;\n  created_at: string;\n}\n\n// =============================================================================\n// Type Guards\n// =============================================================================\n\n/**\n * Runtime type guard for EntityType.\n * Uses the ENTITY_TYPES const array for O(n) lookup (n=7, negligible).\n */\nexport function isEntityType(s: string): s is EntityType {\n  return (ENTITY_TYPES as readonly string[]).includes(s);\n}\n\n/**\n * Runtime type guard for RelationshipType.\n * Uses the RELATIONSHIP_TYPES const array for O(n) lookup (n=7, negligible).\n */\nexport function isRelationshipType(s: string): s is RelationshipType {\n  return (RELATIONSHIP_TYPES as readonly string[]).includes(s);\n}\n\n// =============================================================================\n// Constants\n// =============================================================================\n\n/**\n * Maximum number of edges a single node can have.\n * Used by constraint enforcement in Plan 05 to prevent\n * hub nodes from dominating the graph.\n */\nexport const MAX_NODE_DEGREE = 50;\n","/**\n * Migration 001: Create graph_nodes and graph_edges tables.\n *\n * Graph tables are managed separately from the main observation/session tables\n * because the knowledge graph is a distinct subsystem (Phase 7) that operates\n * on extracted entities rather than raw observations.\n *\n * Tables:\n *   - graph_nodes: entities with type-checked taxonomy (7 types)\n *   - graph_edges: directed relationships with type-checked taxonomy (7 types),\n *     weight confidence, and unique constraint on (source_id, target_id, type)\n *\n * Indexes:\n *   - Nodes: type, name\n *   - Edges: source_id, target_id, type, unique(source_id, target_id, type)\n */\n\nexport const up = `\n  CREATE TABLE IF NOT EXISTS graph_nodes (\n    id TEXT PRIMARY KEY,\n    type TEXT NOT NULL CHECK(type IN ('Project','File','Decision','Problem','Solution','Tool','Person')),\n    name TEXT NOT NULL,\n    metadata TEXT DEFAULT '{}',\n    observation_ids TEXT DEFAULT '[]',\n    created_at TEXT NOT NULL DEFAULT (datetime('now')),\n    updated_at TEXT NOT NULL DEFAULT (datetime('now'))\n  );\n\n  CREATE TABLE IF NOT EXISTS graph_edges (\n    id TEXT PRIMARY KEY,\n    source_id TEXT NOT NULL REFERENCES graph_nodes(id) ON DELETE CASCADE,\n    target_id TEXT NOT NULL REFERENCES graph_nodes(id) ON DELETE CASCADE,\n    type TEXT NOT NULL CHECK(type IN ('uses','depends_on','decided_by','related_to','part_of','caused_by','solved_by')),\n    weight REAL NOT NULL DEFAULT 1.0 CHECK(weight >= 0.0 AND weight <= 1.0),\n    metadata TEXT DEFAULT '{}',\n    created_at TEXT NOT NULL DEFAULT (datetime('now'))\n  );\n\n  CREATE INDEX IF NOT EXISTS idx_graph_nodes_type ON graph_nodes(type);\n  CREATE INDEX IF NOT EXISTS idx_graph_nodes_name ON graph_nodes(name);\n  CREATE INDEX IF NOT EXISTS idx_graph_edges_source ON graph_edges(source_id);\n  CREATE INDEX IF NOT EXISTS idx_graph_edges_target ON graph_edges(target_id);\n  CREATE INDEX IF NOT EXISTS idx_graph_edges_type ON graph_edges(type);\n  CREATE UNIQUE INDEX IF NOT EXISTS idx_graph_edges_unique ON graph_edges(source_id, target_id, type);\n`;\n\nexport const down = `\n  DROP INDEX IF EXISTS idx_graph_edges_unique;\n  DROP INDEX IF EXISTS idx_graph_edges_type;\n  DROP INDEX IF EXISTS idx_graph_edges_target;\n  DROP INDEX IF EXISTS idx_graph_edges_source;\n  DROP INDEX IF EXISTS idx_graph_nodes_name;\n  DROP INDEX IF EXISTS idx_graph_nodes_type;\n  DROP TABLE IF EXISTS graph_edges;\n  DROP TABLE IF EXISTS graph_nodes;\n`;\n","/**\n * Knowledge graph schema initialization and query builders.\n *\n * All functions accept a better-sqlite3 Database handle and use prepared\n * statements for performance. Metadata and observation_ids are stored as\n * JSON TEXT in SQLite and parsed on read.\n *\n * Traversal uses recursive CTEs for efficient subgraph extraction up to\n * a configurable depth.\n */\n\nimport type BetterSqlite3 from 'better-sqlite3';\nimport { randomBytes } from 'node:crypto';\n\nimport type {\n  EntityType,\n  RelationshipType,\n  GraphNode,\n  GraphEdge,\n} from './types.js';\nimport { up as graphTablesDDL } from './migrations/001-graph-tables.js';\n\n// =============================================================================\n// Raw Row Types (snake_case, matches SQL columns)\n// =============================================================================\n\ninterface NodeRow {\n  id: string;\n  type: string;\n  name: string;\n  metadata: string; // JSON string\n  observation_ids: string; // JSON string\n  created_at: string;\n  updated_at: string;\n}\n\ninterface EdgeRow {\n  id: string;\n  source_id: string;\n  target_id: string;\n  type: string;\n  weight: number;\n  metadata: string; // JSON string\n  created_at: string;\n}\n\ninterface TraversalRow {\n  // Node columns (prefixed n_)\n  n_id: string;\n  n_type: string;\n  n_name: string;\n  n_metadata: string;\n  n_observation_ids: string;\n  n_created_at: string;\n  n_updated_at: string;\n  // Edge columns (prefixed e_) -- nullable for starting node\n  e_id: string | null;\n  e_source_id: string | null;\n  e_target_id: string | null;\n  e_type: string | null;\n  e_weight: number | null;\n  e_metadata: string | null;\n  e_created_at: string | null;\n  // Traversal depth\n  depth: number;\n}\n\n// =============================================================================\n// Row Mapping\n// =============================================================================\n\nfunction rowToNode(row: NodeRow): GraphNode {\n  return {\n    id: row.id,\n    type: row.type as EntityType,\n    name: row.name,\n    metadata: JSON.parse(row.metadata) as Record<string, unknown>,\n    observation_ids: JSON.parse(row.observation_ids) as string[],\n    created_at: row.created_at,\n    updated_at: row.updated_at,\n  };\n}\n\nfunction rowToEdge(row: EdgeRow): GraphEdge {\n  return {\n    id: row.id,\n    source_id: row.source_id,\n    target_id: row.target_id,\n    type: row.type as RelationshipType,\n    weight: row.weight,\n    metadata: JSON.parse(row.metadata) as Record<string, unknown>,\n    created_at: row.created_at,\n  };\n}\n\n// =============================================================================\n// Traversal Result\n// =============================================================================\n\nexport interface TraversalResult {\n  node: GraphNode;\n  edge: GraphEdge | null;\n  depth: number;\n}\n\n// =============================================================================\n// Schema Initialization\n// =============================================================================\n\n/**\n * Initializes graph tables if they do not exist.\n * Uses CREATE TABLE IF NOT EXISTS so it is safe to call multiple times.\n */\nexport function initGraphSchema(db: BetterSqlite3.Database): void {\n  db.exec(graphTablesDDL);\n}\n\n// =============================================================================\n// Traversal Queries\n// =============================================================================\n\n/**\n * Traverses the graph from a starting node using a recursive CTE.\n *\n * Supports directional traversal:\n *   - 'outgoing': follows edges where source_id matches (default)\n *   - 'incoming': follows edges where target_id matches\n *   - 'both': follows edges in either direction\n *\n * Returns nodes and the edges that connect them, up to the specified depth.\n * The starting node itself is NOT included in results (depth > 0 filter).\n *\n * @param db - better-sqlite3 Database handle\n * @param nodeId - starting node ID\n * @param opts - traversal options (depth, edgeTypes, direction)\n * @returns Array of { node, edge, depth } for each reachable node\n */\nexport function traverseFrom(\n  db: BetterSqlite3.Database,\n  nodeId: string,\n  opts: {\n    depth?: number;\n    edgeTypes?: RelationshipType[];\n    direction?: 'outgoing' | 'incoming' | 'both';\n  } = {},\n): TraversalResult[] {\n  const maxDepth = opts.depth ?? 2;\n  const direction = opts.direction ?? 'outgoing';\n\n  // Build edge type filter clause\n  let edgeTypeFilter = '';\n  const params: unknown[] = [nodeId, maxDepth];\n\n  if (opts.edgeTypes && opts.edgeTypes.length > 0) {\n    const placeholders = opts.edgeTypes.map(() => '?').join(', ');\n    edgeTypeFilter = `AND e.type IN (${placeholders})`;\n    // These params are used inside the recursive part\n  }\n\n  // Build the recursive step based on direction\n  let recursiveStep: string;\n\n  if (direction === 'outgoing') {\n    recursiveStep = `\n      SELECT e.target_id, t.depth + 1, e.id\n      FROM graph_edges e\n      JOIN traverse t ON e.source_id = t.node_id\n      WHERE t.depth < ?\n      ${edgeTypeFilter}\n    `;\n  } else if (direction === 'incoming') {\n    recursiveStep = `\n      SELECT e.source_id, t.depth + 1, e.id\n      FROM graph_edges e\n      JOIN traverse t ON e.target_id = t.node_id\n      WHERE t.depth < ?\n      ${edgeTypeFilter}\n    `;\n  } else {\n    // both directions\n    recursiveStep = `\n      SELECT e.target_id, t.depth + 1, e.id\n      FROM graph_edges e\n      JOIN traverse t ON e.source_id = t.node_id\n      WHERE t.depth < ?\n      ${edgeTypeFilter}\n      UNION ALL\n      SELECT e.source_id, t.depth + 1, e.id\n      FROM graph_edges e\n      JOIN traverse t ON e.target_id = t.node_id\n      WHERE t.depth < ?\n      ${edgeTypeFilter}\n    `;\n  }\n\n  const sql = `\n    WITH RECURSIVE traverse(node_id, depth, edge_id) AS (\n      SELECT ?, 0, NULL\n      UNION ALL\n      ${recursiveStep}\n    )\n    SELECT DISTINCT\n      n.id AS n_id, n.type AS n_type, n.name AS n_name,\n      n.metadata AS n_metadata, n.observation_ids AS n_observation_ids,\n      n.created_at AS n_created_at, n.updated_at AS n_updated_at,\n      e.id AS e_id, e.source_id AS e_source_id, e.target_id AS e_target_id,\n      e.type AS e_type, e.weight AS e_weight, e.metadata AS e_metadata,\n      e.created_at AS e_created_at,\n      t.depth\n    FROM traverse t\n    JOIN graph_nodes n ON n.id = t.node_id\n    LEFT JOIN graph_edges e ON e.id = t.edge_id\n    WHERE t.depth > 0\n  `;\n\n  // Build parameter list depending on direction\n  const queryParams: unknown[] = [nodeId]; // initial SELECT ?\n\n  if (direction === 'both') {\n    // Two recursive branches, each needs maxDepth + optional edgeTypes\n    queryParams.push(maxDepth);\n    if (opts.edgeTypes) queryParams.push(...opts.edgeTypes);\n    queryParams.push(maxDepth);\n    if (opts.edgeTypes) queryParams.push(...opts.edgeTypes);\n  } else {\n    queryParams.push(maxDepth);\n    if (opts.edgeTypes) queryParams.push(...opts.edgeTypes);\n  }\n\n  const rows = db.prepare(sql).all(...queryParams) as TraversalRow[];\n\n  return rows.map((row) => ({\n    node: {\n      id: row.n_id,\n      type: row.n_type as EntityType,\n      name: row.n_name,\n      metadata: JSON.parse(row.n_metadata) as Record<string, unknown>,\n      observation_ids: JSON.parse(row.n_observation_ids) as string[],\n      created_at: row.n_created_at,\n      updated_at: row.n_updated_at,\n    },\n    edge: row.e_id\n      ? {\n          id: row.e_id,\n          source_id: row.e_source_id!,\n          target_id: row.e_target_id!,\n          type: row.e_type as RelationshipType,\n          weight: row.e_weight!,\n          metadata: JSON.parse(row.e_metadata!) as Record<string, unknown>,\n          created_at: row.e_created_at!,\n        }\n      : null,\n    depth: row.depth,\n  }));\n}\n\n// =============================================================================\n// Node Queries\n// =============================================================================\n\n/**\n * Returns all nodes of a given entity type.\n */\nexport function getNodesByType(\n  db: BetterSqlite3.Database,\n  type: EntityType,\n): GraphNode[] {\n  const rows = db\n    .prepare('SELECT * FROM graph_nodes WHERE type = ?')\n    .all(type) as NodeRow[];\n  return rows.map(rowToNode);\n}\n\n/**\n * Looks up a node by name and type (composite natural key).\n * Returns null if no matching node exists.\n */\nexport function getNodeByNameAndType(\n  db: BetterSqlite3.Database,\n  name: string,\n  type: EntityType,\n): GraphNode | null {\n  const row = db\n    .prepare('SELECT * FROM graph_nodes WHERE name = ? AND type = ?')\n    .get(name, type) as NodeRow | undefined;\n  return row ? rowToNode(row) : null;\n}\n\n// =============================================================================\n// Edge Queries\n// =============================================================================\n\n/**\n * Returns edges connected to a node, filtered by direction.\n *\n * @param direction - 'outgoing' (source), 'incoming' (target), or 'both' (default: 'both')\n */\nexport function getEdgesForNode(\n  db: BetterSqlite3.Database,\n  nodeId: string,\n  opts?: { direction?: 'outgoing' | 'incoming' | 'both' },\n): GraphEdge[] {\n  const direction = opts?.direction ?? 'both';\n\n  let sql: string;\n  let params: unknown[];\n\n  if (direction === 'outgoing') {\n    sql = 'SELECT * FROM graph_edges WHERE source_id = ?';\n    params = [nodeId];\n  } else if (direction === 'incoming') {\n    sql = 'SELECT * FROM graph_edges WHERE target_id = ?';\n    params = [nodeId];\n  } else {\n    sql = 'SELECT * FROM graph_edges WHERE source_id = ? OR target_id = ?';\n    params = [nodeId, nodeId];\n  }\n\n  const rows = db.prepare(sql).all(...params) as EdgeRow[];\n  return rows.map(rowToEdge);\n}\n\n/**\n * Returns the total number of edges connected to a node (both directions).\n * Used for degree enforcement (MAX_NODE_DEGREE constraint).\n */\nexport function countEdgesForNode(\n  db: BetterSqlite3.Database,\n  nodeId: string,\n): number {\n  const result = db\n    .prepare(\n      'SELECT COUNT(*) as cnt FROM graph_edges WHERE source_id = ? OR target_id = ?',\n    )\n    .get(nodeId, nodeId) as { cnt: number };\n  return result.cnt;\n}\n\n// =============================================================================\n// Mutations\n// =============================================================================\n\n/**\n * Inserts or updates a node by name+type composite key.\n *\n * If a node with the same name and type already exists, updates its metadata\n * and merges observation_ids. Otherwise, inserts a new node with a generated UUID.\n *\n * @returns The upserted GraphNode\n */\nexport function upsertNode(\n  db: BetterSqlite3.Database,\n  node: Omit<GraphNode, 'id' | 'created_at' | 'updated_at'> & { id?: string },\n): GraphNode {\n  const existing = getNodeByNameAndType(db, node.name, node.type);\n\n  if (existing) {\n    // Merge observation_ids (deduplicated)\n    const mergedObsIds = [\n      ...new Set([...existing.observation_ids, ...node.observation_ids]),\n    ];\n\n    // Merge metadata (new values override existing)\n    const mergedMetadata = { ...existing.metadata, ...node.metadata };\n\n    db.prepare(\n      `UPDATE graph_nodes\n       SET metadata = ?, observation_ids = ?, updated_at = datetime('now')\n       WHERE id = ?`,\n    ).run(\n      JSON.stringify(mergedMetadata),\n      JSON.stringify(mergedObsIds),\n      existing.id,\n    );\n\n    const updated = db\n      .prepare('SELECT * FROM graph_nodes WHERE id = ?')\n      .get(existing.id) as NodeRow;\n    return rowToNode(updated);\n  }\n\n  // Insert new node\n  const id = node.id ?? randomBytes(16).toString('hex');\n  db.prepare(\n    `INSERT INTO graph_nodes (id, type, name, metadata, observation_ids)\n     VALUES (?, ?, ?, ?, ?)`,\n  ).run(\n    id,\n    node.type,\n    node.name,\n    JSON.stringify(node.metadata),\n    JSON.stringify(node.observation_ids),\n  );\n\n  const inserted = db\n    .prepare('SELECT * FROM graph_nodes WHERE id = ?')\n    .get(id) as NodeRow;\n  return rowToNode(inserted);\n}\n\n/**\n * Inserts an edge. On conflict (same source_id, target_id, type),\n * updates the weight to the maximum of existing and new values.\n *\n * @returns The inserted or updated GraphEdge\n */\nexport function insertEdge(\n  db: BetterSqlite3.Database,\n  edge: Omit<GraphEdge, 'id' | 'created_at'> & { id?: string },\n): GraphEdge {\n  const id = edge.id ?? randomBytes(16).toString('hex');\n\n  db.prepare(\n    `INSERT INTO graph_edges (id, source_id, target_id, type, weight, metadata)\n     VALUES (?, ?, ?, ?, ?, ?)\n     ON CONFLICT (source_id, target_id, type) DO UPDATE SET\n       weight = MAX(graph_edges.weight, excluded.weight),\n       metadata = excluded.metadata`,\n  ).run(\n    id,\n    edge.source_id,\n    edge.target_id,\n    edge.type,\n    edge.weight,\n    JSON.stringify(edge.metadata),\n  );\n\n  // Retrieve the actual row (may be the existing row if conflict occurred)\n  const inserted = db\n    .prepare(\n      'SELECT * FROM graph_edges WHERE source_id = ? AND target_id = ? AND type = ?',\n    )\n    .get(edge.source_id, edge.target_id, edge.type) as EdgeRow;\n  return rowToEdge(inserted);\n}\n","/**\n * MCP tool handler for knowledge graph queries.\n *\n * Allows Claude to search the knowledge graph for entities by name or type,\n * traverse relationships to a configurable depth, and see linked observation\n * excerpts. Results use progressive disclosure: entity list with connection\n * counts, then relationships, then observation excerpts.\n */\n\nimport type { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport type BetterSqlite3 from 'better-sqlite3';\nimport { z } from 'zod';\n\nimport { debug } from '../../shared/debug.js';\nimport type { NotificationStore } from '../../storage/notifications.js';\nimport {\n  type EntityType,\n  type RelationshipType,\n  ENTITY_TYPES,\n  RELATIONSHIP_TYPES,\n  isEntityType,\n  isRelationshipType,\n} from '../../graph/types.js';\nimport type { GraphNode, GraphEdge } from '../../graph/types.js';\nimport {\n  initGraphSchema,\n  traverseFrom,\n  getNodeByNameAndType,\n  getEdgesForNode,\n  type TraversalResult,\n} from '../../graph/schema.js';\n\n// =============================================================================\n// Types\n// =============================================================================\n\nexport interface QueryGraphInput {\n  query: string;\n  entity_type?: EntityType;\n  depth?: number;\n  relationship_types?: RelationshipType[];\n  limit?: number;\n}\n\nexport interface QueryGraphOutput {\n  entities: Array<{\n    node: GraphNode;\n    connectionCount: number;\n    relationships: Array<{\n      direction: 'outgoing' | 'incoming';\n      type: RelationshipType;\n      targetName: string;\n      targetType: EntityType;\n    }>;\n  }>;\n  observations: Array<{\n    text: string;\n    createdAt: string;\n  }>;\n  totalFound: number;\n}\n\n// =============================================================================\n// Internal Row Types\n// =============================================================================\n\ninterface NodeRow {\n  id: string;\n  type: string;\n  name: string;\n  metadata: string;\n  observation_ids: string;\n  created_at: string;\n  updated_at: string;\n}\n\ninterface ObsSnippetRow {\n  content: string;\n  created_at: string;\n}\n\n// =============================================================================\n// Formatting Helpers\n// =============================================================================\n\nfunction truncateText(text: string, maxLen: number): string {\n  if (text.length <= maxLen) return text;\n  return text.slice(0, maxLen).trimEnd() + '...';\n}\n\nfunction formatEntityType(type: EntityType): string {\n  return `[${type}]`;\n}\n\n/**\n * Formats query results as readable text for Claude consumption.\n * Uses progressive disclosure: entity list -> relationships -> observations.\n */\nfunction formatResults(\n  rootNodes: GraphNode[],\n  traversalsByNode: Map<string, TraversalResult[]>,\n  observations: Array<{ text: string; createdAt: string }>,\n  query: string,\n): string {\n  const lines: string[] = [];\n\n  // Section: Entities Found\n  lines.push('## Entities Found');\n  lines.push('');\n\n  for (const node of rootNodes) {\n    const traversals = traversalsByNode.get(node.id) ?? [];\n    const connectionCount = traversals.length;\n\n    lines.push(\n      `- ${formatEntityType(node.type)} ${node.name} (${connectionCount} connection${connectionCount !== 1 ? 's' : ''})`,\n    );\n\n    // Show relationships\n    for (const t of traversals) {\n      if (!t.edge) continue;\n      const direction = t.edge.source_id === node.id ? '->' : '<-';\n      lines.push(\n        `  ${direction} ${t.edge.type} ${formatEntityType(t.node.type)} ${t.node.name}`,\n      );\n    }\n\n    lines.push('');\n  }\n\n  // Section: Related Observations\n  if (observations.length > 0) {\n    lines.push('## Related Observations');\n    lines.push('');\n\n    for (const obs of observations) {\n      const age = formatAge(obs.createdAt);\n      const snippet = truncateText(obs.text.replace(/\\n/g, ' '), 200);\n      lines.push(`- \"${snippet}\" (${age})`);\n    }\n  }\n\n  return lines.join('\\n').trim();\n}\n\n/**\n * Simple relative time formatting.\n */\nfunction formatAge(isoDate: string): string {\n  const now = Date.now();\n  const then = new Date(isoDate).getTime();\n  const diffMs = now - then;\n\n  const hours = Math.floor(diffMs / (1000 * 60 * 60));\n  if (hours < 1) return 'just now';\n  if (hours < 24) return `${hours} hour${hours !== 1 ? 's' : ''} ago`;\n\n  const days = Math.floor(hours / 24);\n  if (days < 30) return `${days} day${days !== 1 ? 's' : ''} ago`;\n\n  const months = Math.floor(days / 30);\n  return `${months} month${months !== 1 ? 's' : ''} ago`;\n}\n\n// =============================================================================\n// Response Helpers\n// =============================================================================\n\nfunction prependNotifications(\n  notificationStore: NotificationStore | null,\n  projectHash: string,\n  responseText: string,\n): string {\n  if (!notificationStore) return responseText;\n  const pending = notificationStore.consumePending(projectHash);\n  if (pending.length === 0) return responseText;\n  const banner = pending.map((n) => `[Laminark] ${n.message}`).join('\\n');\n  return banner + '\\n\\n' + responseText;\n}\n\nfunction textResponse(text: string) {\n  return { content: [{ type: 'text' as const, text }] };\n}\n\nfunction errorResponse(text: string) {\n  return { content: [{ type: 'text' as const, text }], isError: true };\n}\n\n// =============================================================================\n// Tool Registration\n// =============================================================================\n\n/**\n * Registers the query_graph MCP tool on the server.\n *\n * Allows Claude to search entities by name (exact or fuzzy), filter by type,\n * traverse relationships to configurable depth, and see linked observations.\n */\nexport function registerQueryGraph(\n  server: McpServer,\n  db: BetterSqlite3.Database,\n  projectHash: string,\n  notificationStore: NotificationStore | null = null,\n): void {\n  // Ensure graph schema is initialized\n  initGraphSchema(db);\n\n  server.registerTool(\n    'query_graph',\n    {\n      title: 'Query Knowledge Graph',\n      description:\n        \"Query the knowledge graph to find entities and their relationships. Use to answer questions like 'what files does this decision affect?' or 'what tools does this project use?'\",\n      inputSchema: {\n        query: z\n          .string()\n          .min(1)\n          .describe('Entity name or search text to look for'),\n        entity_type: z\n          .string()\n          .optional()\n          .describe(\n            `Filter to entity type: ${ENTITY_TYPES.join(', ')}`,\n          ),\n        depth: z\n          .number()\n          .int()\n          .min(1)\n          .max(4)\n          .default(2)\n          .describe('Traversal depth (default: 2, max: 4)'),\n        relationship_types: z\n          .array(z.string())\n          .optional()\n          .describe(\n            `Filter to relationship types: ${RELATIONSHIP_TYPES.join(', ')}`,\n          ),\n        limit: z\n          .number()\n          .int()\n          .min(1)\n          .max(50)\n          .default(20)\n          .describe('Max root entities to return (default: 20, max: 50)'),\n      },\n    },\n    async (args) => {\n      const withNotifications = (text: string) =>\n        textResponse(\n          prependNotifications(notificationStore, projectHash, text),\n        );\n\n      try {\n        debug('mcp', 'query_graph: request', {\n          query: args.query,\n          entity_type: args.entity_type,\n          depth: args.depth,\n        });\n\n        // Validate entity_type if provided\n        if (args.entity_type !== undefined && !isEntityType(args.entity_type)) {\n          return errorResponse(\n            `Invalid entity_type \"${args.entity_type}\". Valid types: ${ENTITY_TYPES.join(', ')}`,\n          );\n        }\n        const entityType = args.entity_type as EntityType | undefined;\n\n        // Validate relationship_types if provided\n        if (args.relationship_types) {\n          for (const rt of args.relationship_types) {\n            if (!isRelationshipType(rt)) {\n              return errorResponse(\n                `Invalid relationship_type \"${rt}\". Valid types: ${RELATIONSHIP_TYPES.join(', ')}`,\n              );\n            }\n          }\n        }\n        const relationshipTypes = args.relationship_types as\n          | RelationshipType[]\n          | undefined;\n\n        // Search strategy: exact match first, then fuzzy LIKE search\n        const rootNodes: GraphNode[] = [];\n\n        // 1. Try exact name match (optionally filtered by type)\n        if (entityType) {\n          const exact = getNodeByNameAndType(db, args.query, entityType);\n          if (exact) rootNodes.push(exact);\n        } else {\n          // Try exact match across all types\n          for (const t of ENTITY_TYPES) {\n            const exact = getNodeByNameAndType(db, args.query, t);\n            if (exact) {\n              rootNodes.push(exact);\n              break; // Take first exact match\n            }\n          }\n        }\n\n        // 2. If no exact match, try case-insensitive LIKE search\n        if (rootNodes.length === 0) {\n          const likePattern = `%${args.query}%`;\n          let sql: string;\n          const params: unknown[] = [likePattern];\n\n          if (entityType) {\n            sql =\n              'SELECT * FROM graph_nodes WHERE name LIKE ? COLLATE NOCASE AND type = ? LIMIT ?';\n            params.push(entityType, args.limit);\n          } else {\n            sql =\n              'SELECT * FROM graph_nodes WHERE name LIKE ? COLLATE NOCASE LIMIT ?';\n            params.push(args.limit);\n          }\n\n          const rows = db.prepare(sql).all(...params) as NodeRow[];\n          for (const row of rows) {\n            rootNodes.push({\n              id: row.id,\n              type: row.type as EntityType,\n              name: row.name,\n              metadata: JSON.parse(row.metadata) as Record<string, unknown>,\n              observation_ids: JSON.parse(row.observation_ids) as string[],\n              created_at: row.created_at,\n              updated_at: row.updated_at,\n            });\n          }\n        }\n\n        // No results found\n        if (rootNodes.length === 0) {\n          const suggestions = entityType\n            ? `Try searching without the entity_type filter, or try a different name.`\n            : `Try: entity types ${ENTITY_TYPES.join(', ')}`;\n          return withNotifications(\n            `No entities matching \"${args.query}\" found. ${suggestions}`,\n          );\n        }\n\n        // 3. Traverse from each root node\n        const traversalsByNode = new Map<string, TraversalResult[]>();\n\n        for (const node of rootNodes) {\n          const results = traverseFrom(db, node.id, {\n            depth: args.depth,\n            edgeTypes: relationshipTypes,\n            direction: 'both',\n          });\n          traversalsByNode.set(node.id, results);\n        }\n\n        // 4. Collect observation snippets from root nodes\n        const allObsIds = new Set<string>();\n        for (const node of rootNodes) {\n          for (const obsId of node.observation_ids) {\n            allObsIds.add(obsId);\n          }\n        }\n\n        const observations: Array<{ text: string; createdAt: string }> = [];\n        if (allObsIds.size > 0) {\n          const obsIdList = [...allObsIds];\n          const placeholders = obsIdList.map(() => '?').join(', ');\n          const obsRows = db\n            .prepare(\n              `SELECT content, created_at FROM observations WHERE id IN (${placeholders}) AND deleted_at IS NULL ORDER BY created_at DESC LIMIT 10`,\n            )\n            .all(...obsIdList) as ObsSnippetRow[];\n\n          for (const row of obsRows) {\n            observations.push({\n              text: row.content,\n              createdAt: row.created_at,\n            });\n          }\n        }\n\n        // 5. Format and return\n        const formatted = formatResults(\n          rootNodes,\n          traversalsByNode,\n          observations,\n          args.query,\n        );\n\n        debug('mcp', 'query_graph: returning', {\n          rootNodes: rootNodes.length,\n          totalTraversals: [...traversalsByNode.values()].reduce(\n            (sum, arr) => sum + arr.length,\n            0,\n          ),\n          observations: observations.length,\n        });\n\n        return withNotifications(formatted);\n      } catch (err) {\n        const message =\n          err instanceof Error ? err.message : 'Unknown error';\n        debug('mcp', 'query_graph: error', { error: message });\n        return errorResponse(`Graph query error: ${message}`);\n      }\n    },\n  );\n}\n","/**\n * Staleness detection for the knowledge graph.\n *\n * Detects contradictions between observations linked to the same entity\n * using pattern matching (negation, replacement, status change). Stale\n * observations are FLAGGED but NEVER deleted -- the system surfaces both\n * observations and lets the user decide.\n *\n * This module is detection-only on the read path (detectStaleness) and\n * provides advisory flagging on the write path (flagStaleObservation).\n */\n\nimport type BetterSqlite3 from 'better-sqlite3';\n\nimport type { Observation, ObservationRow } from '../shared/types.js';\nimport { rowToObservation } from '../shared/types.js';\nimport type { EntityType } from './types.js';\n\n// =============================================================================\n// Types\n// =============================================================================\n\n/**\n * A staleness report documenting a detected contradiction between\n * two observations about the same entity.\n */\nexport interface StalenessReport {\n  entityId: string;\n  entityName: string;\n  entityType: EntityType;\n  newerObservation: { id: string; text: string; created_at: string };\n  olderObservation: { id: string; text: string; created_at: string };\n  reason: string;\n  detectedAt: string;\n}\n\n/**\n * A flagged stale observation with its staleness metadata.\n */\nexport interface StalenessFlag {\n  observation_id: string;\n  flagged_at: string;\n  reason: string;\n  resolved: boolean;\n}\n\n// =============================================================================\n// Staleness Detection Patterns\n// =============================================================================\n\n/**\n * Negation patterns: newer observation negates older one.\n * Matches when newer text contains negation keywords absent in older text\n * and both discuss similar subjects.\n */\nconst NEGATION_KEYWORDS = [\n  'not',\n  \"don't\",\n  'no longer',\n  'stopped',\n  'never',\n  \"doesn't\",\n  \"won't\",\n  \"isn't\",\n  \"aren't\",\n  'discontinued',\n] as const;\n\n/**\n * Replacement patterns: newer observation explicitly replaces older approach.\n */\nconst REPLACEMENT_PATTERNS = [\n  /switched\\s+(?:from\\s+\\S+\\s+)?to\\b/i,\n  /migrated\\s+(?:from\\s+\\S+\\s+)?to\\b/i,\n  /replaced\\s+(?:\\S+\\s+)?with\\b/i,\n  /changed\\s+from\\b/i,\n  /moved\\s+(?:from\\s+\\S+\\s+)?to\\b/i,\n  /upgraded\\s+(?:from\\s+\\S+\\s+)?to\\b/i,\n  /swapped\\s+(?:\\S+\\s+)?(?:for|with)\\b/i,\n] as const;\n\n/**\n * Status change patterns: newer observation marks something as inactive.\n */\nconst STATUS_CHANGE_KEYWORDS = [\n  'removed',\n  'deleted',\n  'deprecated',\n  'archived',\n  'dropped',\n  'disabled',\n  'decommissioned',\n  'sunset',\n  'abandoned',\n] as const;\n\n// =============================================================================\n// Schema Initialization\n// =============================================================================\n\n/**\n * Creates the staleness_flags table if it doesn't exist.\n * Uses a separate table rather than modifying the observations table,\n * keeping staleness metadata decoupled from core observation storage.\n */\nexport function initStalenessSchema(db: BetterSqlite3.Database): void {\n  db.exec(`\n    CREATE TABLE IF NOT EXISTS staleness_flags (\n      observation_id TEXT PRIMARY KEY,\n      flagged_at TEXT NOT NULL DEFAULT (datetime('now')),\n      reason TEXT NOT NULL,\n      resolved INTEGER NOT NULL DEFAULT 0\n    );\n    CREATE INDEX IF NOT EXISTS idx_staleness_resolved ON staleness_flags(resolved);\n  `);\n}\n\n// =============================================================================\n// Detection\n// =============================================================================\n\n/**\n * Detects potential staleness (contradictions) between observations\n * linked to a specific entity.\n *\n * Compares consecutive observation pairs chronologically and checks for:\n * 1. Negation patterns (newer negates older)\n * 2. Replacement patterns (newer replaces older approach)\n * 3. Status change patterns (newer marks something as inactive)\n *\n * This is DETECTION ONLY -- no data is modified.\n *\n * @param db - better-sqlite3 Database handle\n * @param entityId - Graph node ID to check observations for\n * @returns Array of StalenessReport for each detected contradiction\n */\nexport function detectStaleness(\n  db: BetterSqlite3.Database,\n  entityId: string,\n): StalenessReport[] {\n  // Get entity info\n  const node = db\n    .prepare('SELECT id, name, type, observation_ids FROM graph_nodes WHERE id = ?')\n    .get(entityId) as\n    | { id: string; name: string; type: string; observation_ids: string }\n    | undefined;\n\n  if (!node) return [];\n\n  const obsIds = JSON.parse(node.observation_ids) as string[];\n  if (obsIds.length < 2) return [];\n\n  // Fetch observations and sort by created_at\n  const placeholders = obsIds.map(() => '?').join(', ');\n  const rows = db\n    .prepare(\n      `SELECT * FROM observations WHERE id IN (${placeholders}) AND deleted_at IS NULL ORDER BY created_at ASC`,\n    )\n    .all(...obsIds) as ObservationRow[];\n\n  const observations = rows.map(rowToObservation);\n  if (observations.length < 2) return [];\n\n  const reports: StalenessReport[] = [];\n  const now = new Date().toISOString();\n\n  // Compare consecutive pairs\n  for (let i = 0; i < observations.length - 1; i++) {\n    const older = observations[i];\n    const newer = observations[i + 1];\n\n    const reason = detectContradiction(older.content, newer.content);\n    if (reason) {\n      reports.push({\n        entityId: node.id,\n        entityName: node.name,\n        entityType: node.type as EntityType,\n        newerObservation: {\n          id: newer.id,\n          text: newer.content,\n          created_at: newer.createdAt,\n        },\n        olderObservation: {\n          id: older.id,\n          text: older.content,\n          created_at: older.createdAt,\n        },\n        reason,\n        detectedAt: now,\n      });\n    }\n  }\n\n  return reports;\n}\n\n/**\n * Detects contradiction between two observation texts.\n * Returns a human-readable reason string, or null if no contradiction found.\n */\nfunction detectContradiction(\n  olderText: string,\n  newerText: string,\n): string | null {\n  const olderLower = olderText.toLowerCase();\n  const newerLower = newerText.toLowerCase();\n\n  // Check negation patterns\n  const negationResult = detectNegation(olderLower, newerLower);\n  if (negationResult) return negationResult;\n\n  // Check replacement patterns\n  const replacementResult = detectReplacement(newerLower);\n  if (replacementResult) return replacementResult;\n\n  // Check status change patterns\n  const statusResult = detectStatusChange(olderLower, newerLower);\n  if (statusResult) return statusResult;\n\n  return null;\n}\n\n/**\n * Detects negation: newer text contains negation keywords that are absent\n * in the older text, suggesting the newer observation contradicts the older.\n */\nfunction detectNegation(\n  olderLower: string,\n  newerLower: string,\n): string | null {\n  for (const keyword of NEGATION_KEYWORDS) {\n    if (newerLower.includes(keyword) && !olderLower.includes(keyword)) {\n      return `Newer observation contains negation (\"${keyword}\") not present in older observation`;\n    }\n  }\n  return null;\n}\n\n/**\n * Detects replacement: newer text explicitly mentions switching/replacing.\n */\nfunction detectReplacement(newerLower: string): string | null {\n  for (const pattern of REPLACEMENT_PATTERNS) {\n    const match = newerLower.match(pattern);\n    if (match) {\n      return `Newer observation indicates replacement (\"${match[0].trim()}\")`;\n    }\n  }\n  return null;\n}\n\n/**\n * Detects status change: newer text marks something as removed/deprecated\n * when the older text described it as active/present.\n */\nfunction detectStatusChange(\n  olderLower: string,\n  newerLower: string,\n): string | null {\n  for (const keyword of STATUS_CHANGE_KEYWORDS) {\n    if (newerLower.includes(keyword) && !olderLower.includes(keyword)) {\n      return `Newer observation indicates status change (\"${keyword}\")`;\n    }\n  }\n  return null;\n}\n\n// =============================================================================\n// Flagging\n// =============================================================================\n\n/**\n * Flags an observation as stale with an advisory reason.\n *\n * This flag is advisory -- search can use it to deprioritize but never hide\n * the observation. The observation remains fully queryable.\n *\n * Uses INSERT OR REPLACE to allow re-flagging with an updated reason.\n *\n * @param db - better-sqlite3 Database handle\n * @param observationId - ID of the observation to flag\n * @param reason - Human-readable explanation of why it's stale\n */\nexport function flagStaleObservation(\n  db: BetterSqlite3.Database,\n  observationId: string,\n  reason: string,\n): void {\n  initStalenessSchema(db);\n\n  db.prepare(\n    `INSERT OR REPLACE INTO staleness_flags (observation_id, reason, resolved)\n     VALUES (?, ?, 0)`,\n  ).run(observationId, reason);\n}\n\n// =============================================================================\n// Querying Flagged Observations\n// =============================================================================\n\n/**\n * Retrieves observations that have been flagged as stale.\n *\n * Optionally filtered by entity (via graph_nodes.observation_ids) or\n * resolution status.\n *\n * @param db - better-sqlite3 Database handle\n * @param opts - Filter options: entityId, resolved status\n * @returns Array of observation + staleness report pairs\n */\nexport function getStaleObservations(\n  db: BetterSqlite3.Database,\n  opts?: { entityId?: string; resolved?: boolean },\n): Array<{ observation: Observation; flag: StalenessFlag }> {\n  initStalenessSchema(db);\n\n  // Get candidate observation IDs (optionally scoped to entity)\n  let candidateIds: string[] | null = null;\n  if (opts?.entityId) {\n    const node = db\n      .prepare('SELECT observation_ids FROM graph_nodes WHERE id = ?')\n      .get(opts.entityId) as { observation_ids: string } | undefined;\n\n    if (!node) return [];\n    candidateIds = JSON.parse(node.observation_ids) as string[];\n    if (candidateIds.length === 0) return [];\n  }\n\n  // Build query\n  const conditions: string[] = [];\n  const params: unknown[] = [];\n\n  if (candidateIds) {\n    const placeholders = candidateIds.map(() => '?').join(', ');\n    conditions.push(`sf.observation_id IN (${placeholders})`);\n    params.push(...candidateIds);\n  }\n\n  if (opts?.resolved !== undefined) {\n    conditions.push('sf.resolved = ?');\n    params.push(opts.resolved ? 1 : 0);\n  }\n\n  const whereClause =\n    conditions.length > 0 ? `WHERE ${conditions.join(' AND ')}` : '';\n\n  const sql = `\n    SELECT o.*, sf.observation_id AS sf_observation_id, sf.flagged_at AS sf_flagged_at,\n           sf.reason AS sf_reason, sf.resolved AS sf_resolved\n    FROM staleness_flags sf\n    JOIN observations o ON o.id = sf.observation_id\n    ${whereClause}\n    ORDER BY sf.flagged_at DESC\n  `;\n\n  interface StaleRow extends ObservationRow {\n    sf_observation_id: string;\n    sf_flagged_at: string;\n    sf_reason: string;\n    sf_resolved: number;\n  }\n\n  const rows = db.prepare(sql).all(...params) as StaleRow[];\n\n  return rows.map((row) => ({\n    observation: rowToObservation(row),\n    flag: {\n      observation_id: row.sf_observation_id,\n      flagged_at: row.sf_flagged_at,\n      reason: row.sf_reason,\n      resolved: row.sf_resolved === 1,\n    },\n  }));\n}\n","/**\n * MCP tool handler for graph statistics and health metrics.\n *\n * Provides a dashboard view of the knowledge graph: node/edge counts,\n * entity type distribution, relationship type distribution, degree stats,\n * hotspots (nodes near edge limit), duplicate candidates, and staleness flags.\n *\n * No input parameters -- this is a read-only dashboard.\n */\n\nimport type { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport type BetterSqlite3 from 'better-sqlite3';\n\nimport { debug } from '../../shared/debug.js';\nimport type { NotificationStore } from '../../storage/notifications.js';\nimport {\n  type EntityType,\n  type RelationshipType,\n  ENTITY_TYPES,\n  RELATIONSHIP_TYPES,\n  MAX_NODE_DEGREE,\n} from '../../graph/types.js';\nimport { initGraphSchema } from '../../graph/schema.js';\nimport { initStalenessSchema } from '../../graph/staleness.js';\n\n// =============================================================================\n// Types\n// =============================================================================\n\nexport interface GraphStatsOutput {\n  total_nodes: number;\n  total_edges: number;\n  by_entity_type: Record<string, number>;\n  by_relationship_type: Record<string, number>;\n  avg_degree: number;\n  max_degree: { node_name: string; node_type: EntityType; degree: number } | null;\n  hotspots: Array<{ name: string; type: EntityType; degree: number }>;\n  duplicate_candidates: number;\n  staleness_flags: number;\n}\n\n// =============================================================================\n// Internal Row Types\n// =============================================================================\n\ninterface CountRow {\n  type: string;\n  cnt: number;\n}\n\ninterface DegreeRow {\n  node_id: string;\n  node_name: string;\n  node_type: string;\n  degree: number;\n}\n\n// =============================================================================\n// Stats Collection\n// =============================================================================\n\n/**\n * Collects comprehensive graph statistics directly from the database.\n * Does not depend on constraints module (which may not be built yet).\n */\nfunction collectGraphStats(db: BetterSqlite3.Database): GraphStatsOutput {\n  // Total counts\n  const totalNodes =\n    (db.prepare('SELECT COUNT(*) as cnt FROM graph_nodes').get() as { cnt: number })\n      .cnt;\n  const totalEdges =\n    (db.prepare('SELECT COUNT(*) as cnt FROM graph_edges').get() as { cnt: number })\n      .cnt;\n\n  // By entity type\n  const entityCounts = db\n    .prepare('SELECT type, COUNT(*) as cnt FROM graph_nodes GROUP BY type')\n    .all() as CountRow[];\n  const byEntityType: Record<string, number> = {};\n  for (const t of ENTITY_TYPES) {\n    byEntityType[t] = 0;\n  }\n  for (const row of entityCounts) {\n    byEntityType[row.type] = row.cnt;\n  }\n\n  // By relationship type\n  const relCounts = db\n    .prepare('SELECT type, COUNT(*) as cnt FROM graph_edges GROUP BY type')\n    .all() as CountRow[];\n  const byRelType: Record<string, number> = {};\n  for (const t of RELATIONSHIP_TYPES) {\n    byRelType[t] = 0;\n  }\n  for (const row of relCounts) {\n    byRelType[row.type] = row.cnt;\n  }\n\n  // Degree statistics\n  const avgDegree =\n    totalNodes > 0 ? (totalEdges * 2) / totalNodes : 0;\n\n  // Max degree node -- count edges in both directions per node\n  const degreeRows = db\n    .prepare(\n      `SELECT n.id as node_id, n.name as node_name, n.type as node_type,\n              (SELECT COUNT(*) FROM graph_edges WHERE source_id = n.id OR target_id = n.id) as degree\n       FROM graph_nodes n\n       ORDER BY degree DESC\n       LIMIT 10`,\n    )\n    .all() as DegreeRow[];\n\n  let maxDegreeEntry: { node_name: string; node_type: EntityType; degree: number } | null =\n    null;\n  const hotspots: Array<{ name: string; type: EntityType; degree: number }> = [];\n  const hotspotThreshold = Math.floor(MAX_NODE_DEGREE * 0.8); // 80% of limit\n\n  for (const row of degreeRows) {\n    if (!maxDegreeEntry || row.degree > maxDegreeEntry.degree) {\n      maxDegreeEntry = {\n        node_name: row.node_name,\n        node_type: row.node_type as EntityType,\n        degree: row.degree,\n      };\n    }\n    if (row.degree >= hotspotThreshold) {\n      hotspots.push({\n        name: row.node_name,\n        type: row.node_type as EntityType,\n        degree: row.degree,\n      });\n    }\n  }\n\n  // Duplicate candidates: nodes with same name but different type\n  const dupCount =\n    (\n      db\n        .prepare(\n          `SELECT COUNT(*) as cnt FROM (\n            SELECT name FROM graph_nodes GROUP BY name HAVING COUNT(DISTINCT type) > 1\n          )`,\n        )\n        .get() as { cnt: number }\n    ).cnt;\n\n  // Staleness flags count\n  let stalenessCount = 0;\n  try {\n    initStalenessSchema(db);\n    stalenessCount =\n      (\n        db\n          .prepare(\n            'SELECT COUNT(*) as cnt FROM staleness_flags WHERE resolved = 0',\n          )\n          .get() as { cnt: number }\n      ).cnt;\n  } catch {\n    // staleness_flags table may not exist yet\n    stalenessCount = 0;\n  }\n\n  return {\n    total_nodes: totalNodes,\n    total_edges: totalEdges,\n    by_entity_type: byEntityType,\n    by_relationship_type: byRelType,\n    avg_degree: Math.round(avgDegree * 10) / 10,\n    max_degree: maxDegreeEntry,\n    hotspots,\n    duplicate_candidates: dupCount,\n    staleness_flags: stalenessCount,\n  };\n}\n\n// =============================================================================\n// Formatting\n// =============================================================================\n\n/**\n * Formats graph stats as a readable dashboard for Claude.\n */\nfunction formatStats(stats: GraphStatsOutput): string {\n  const lines: string[] = [];\n\n  // Header\n  lines.push('## Knowledge Graph Stats');\n  lines.push(\n    `Nodes: ${stats.total_nodes} | Edges: ${stats.total_edges} | Avg degree: ${stats.avg_degree}`,\n  );\n  lines.push('');\n\n  // Entity Distribution\n  lines.push('### Entity Distribution');\n  const entityParts: string[] = [];\n  for (const t of ENTITY_TYPES) {\n    const count = stats.by_entity_type[t] ?? 0;\n    if (count > 0) {\n      entityParts.push(`${t}: ${count}`);\n    }\n  }\n  lines.push(entityParts.length > 0 ? entityParts.join(' | ') : 'No entities yet');\n  lines.push('');\n\n  // Relationship Distribution\n  lines.push('### Relationship Distribution');\n  const relParts: string[] = [];\n  for (const t of RELATIONSHIP_TYPES) {\n    const count = stats.by_relationship_type[t] ?? 0;\n    if (count > 0) {\n      relParts.push(`${t}: ${count}`);\n    }\n  }\n  lines.push(relParts.length > 0 ? relParts.join(' | ') : 'No relationships yet');\n  lines.push('');\n\n  // Health\n  lines.push('### Health');\n  if (stats.hotspots.length > 0) {\n    const hotspotStr = stats.hotspots\n      .map((h) => `${h.name} (${h.degree} edges)`)\n      .join(', ');\n    lines.push(`Hotspots (near ${MAX_NODE_DEGREE}-edge limit): ${hotspotStr}`);\n  } else {\n    lines.push('Hotspots: none (all nodes well within edge limits)');\n  }\n  lines.push(`Duplicate candidates: ${stats.duplicate_candidates} name${stats.duplicate_candidates !== 1 ? 's' : ''}`);\n  lines.push(`Stale observations: ${stats.staleness_flags}`);\n\n  if (stats.max_degree) {\n    lines.push('');\n    lines.push(\n      `Most connected: ${stats.max_degree.node_name} (${stats.max_degree.node_type}, ${stats.max_degree.degree} edges)`,\n    );\n  }\n\n  return lines.join('\\n');\n}\n\n// =============================================================================\n// Response Helpers\n// =============================================================================\n\nfunction prependNotifications(\n  notificationStore: NotificationStore | null,\n  projectHash: string,\n  responseText: string,\n): string {\n  if (!notificationStore) return responseText;\n  const pending = notificationStore.consumePending(projectHash);\n  if (pending.length === 0) return responseText;\n  const banner = pending.map((n) => `[Laminark] ${n.message}`).join('\\n');\n  return banner + '\\n\\n' + responseText;\n}\n\nfunction textResponse(text: string) {\n  return { content: [{ type: 'text' as const, text }] };\n}\n\n// =============================================================================\n// Tool Registration\n// =============================================================================\n\n/**\n * Registers the graph_stats MCP tool on the server.\n *\n * Returns comprehensive knowledge graph health metrics: entity/relationship\n * type distribution, degree statistics, hotspot nodes, duplicate candidates,\n * and staleness flags. No input parameters -- dashboard view.\n */\nexport function registerGraphStats(\n  server: McpServer,\n  db: BetterSqlite3.Database,\n  projectHash: string,\n  notificationStore: NotificationStore | null = null,\n): void {\n  // Ensure graph schema is initialized\n  initGraphSchema(db);\n\n  server.registerTool(\n    'graph_stats',\n    {\n      title: 'Graph Statistics',\n      description:\n        'Get knowledge graph statistics: entity counts, relationship distribution, health metrics. Use to understand the state of accumulated knowledge.',\n      inputSchema: {},\n    },\n    async () => {\n      try {\n        debug('mcp', 'graph_stats: request');\n\n        const stats = collectGraphStats(db);\n        const formatted = formatStats(stats);\n\n        debug('mcp', 'graph_stats: returning', {\n          nodes: stats.total_nodes,\n          edges: stats.total_edges,\n        });\n\n        return textResponse(\n          prependNotifications(notificationStore, projectHash, formatted),\n        );\n      } catch (err) {\n        const message =\n          err instanceof Error ? err.message : 'Unknown error';\n        debug('mcp', 'graph_stats: error', { error: message });\n        return textResponse(`Graph stats error: ${message}`);\n      }\n    },\n  );\n}\n","/**\n * Main-thread bridge for the embedding worker.\n *\n * AnalysisWorker provides a Promise-based API (embed/embedBatch) that sends\n * messages to the worker thread and resolves when results arrive. All methods\n * degrade gracefully -- returning null on error/timeout rather than throwing.\n */\n\nimport { Worker } from 'node:worker_threads';\nimport { fileURLToPath } from 'node:url';\nimport { dirname, join } from 'node:path';\n\nimport { debug } from '../shared/debug.js';\n\n/** Timeout for worker startup (model loading). */\nconst STARTUP_TIMEOUT_MS = 30_000;\n\n/** Timeout for individual embed requests. */\nconst REQUEST_TIMEOUT_MS = 30_000;\n\ninterface PendingRequest<T> {\n  resolve: (value: T) => void;\n  timer: ReturnType<typeof setTimeout>;\n}\n\ninterface ReadyMessage {\n  type: 'ready';\n  engineName: string;\n  dimensions: number;\n}\n\ninterface EmbedResultMessage {\n  type: 'embed_result';\n  id: string;\n  embedding: Float32Array | null;\n}\n\ninterface EmbedBatchResultMessage {\n  type: 'embed_batch_result';\n  id: string;\n  embeddings: (Float32Array | null)[];\n}\n\ntype WorkerResponse = ReadyMessage | EmbedResultMessage | EmbedBatchResultMessage;\n\n/**\n * Main-thread API for sending embed requests to the worker thread.\n *\n * Usage:\n * ```ts\n * const worker = new AnalysisWorker();\n * await worker.start();\n * const embedding = await worker.embed(\"some text\");\n * await worker.shutdown();\n * ```\n */\nexport class AnalysisWorker {\n  private worker: Worker | null = null;\n  private pending = new Map<string, PendingRequest<unknown>>();\n  private nextId = 0;\n  private ready = false;\n  private engineName = 'unknown';\n  private dimensions = 0;\n  private workerPath: string;\n\n  constructor(workerPath?: string) {\n    if (workerPath) {\n      this.workerPath = workerPath;\n    } else {\n      // Resolve worker.js relative to this file's location in dist/\n      const thisDir = dirname(fileURLToPath(import.meta.url));\n      this.workerPath = join(thisDir, 'worker.js');\n    }\n  }\n\n  /**\n   * Starts the worker thread and waits for the 'ready' message.\n   *\n   * Resolves once the worker reports its engine name and dimensions.\n   * Times out after 30 seconds if the worker never becomes ready.\n   */\n  async start(): Promise<void> {\n    return new Promise<void>((resolve, reject) => {\n      const timer = setTimeout(() => {\n        debug('embed', 'Worker startup timed out');\n        this.ready = false;\n        reject(new Error('Worker startup timed out'));\n      }, STARTUP_TIMEOUT_MS);\n\n      try {\n        this.worker = new Worker(this.workerPath);\n      } catch (err) {\n        clearTimeout(timer);\n        debug('embed', 'Failed to create worker', { error: String(err) });\n        reject(err);\n        return;\n      }\n\n      // One-time handler for the ready message\n      const onReady = (msg: WorkerResponse) => {\n        if (msg.type === 'ready') {\n          clearTimeout(timer);\n          this.ready = true;\n          this.engineName = msg.engineName;\n          this.dimensions = msg.dimensions;\n          debug('embed', 'Worker ready', { engineName: msg.engineName, dimensions: msg.dimensions });\n\n          // Switch to the permanent message handler\n          this.worker!.off('message', onReady);\n          this.worker!.on('message', (m: WorkerResponse) => this.handleMessage(m));\n          resolve();\n        }\n      };\n\n      this.worker.on('message', onReady);\n\n      this.worker.on('error', (err) => {\n        clearTimeout(timer);\n        debug('embed', 'Worker error', { error: String(err) });\n        this.resolveAllPending();\n        this.ready = false;\n      });\n\n      this.worker.on('exit', (code) => {\n        debug('embed', 'Worker exited', { code });\n        this.resolveAllPending();\n        this.ready = false;\n        this.worker = null;\n      });\n    });\n  }\n\n  /**\n   * Embeds a single text string via the worker thread.\n   *\n   * Returns null if the worker is not ready, not started, or if the\n   * request times out.\n   */\n  async embed(text: string): Promise<Float32Array | null> {\n    if (!this.worker || !this.ready) {\n      return null;\n    }\n\n    const id = String(this.nextId++);\n\n    return new Promise<Float32Array | null>((resolve) => {\n      const timer = setTimeout(() => {\n        debug('embed', 'Embed request timed out', { id });\n        this.pending.delete(id);\n        resolve(null);\n      }, REQUEST_TIMEOUT_MS);\n\n      this.pending.set(id, { resolve: resolve as (value: unknown) => void, timer });\n      this.worker!.postMessage({ type: 'embed', id, text });\n    });\n  }\n\n  /**\n   * Embeds multiple texts via the worker thread.\n   *\n   * Returns an array of nulls if the worker is not ready or times out.\n   */\n  async embedBatch(texts: string[]): Promise<(Float32Array | null)[]> {\n    if (!this.worker || !this.ready) {\n      return texts.map(() => null);\n    }\n\n    const id = String(this.nextId++);\n\n    return new Promise<(Float32Array | null)[]>((resolve) => {\n      const timer = setTimeout(() => {\n        debug('embed', 'Batch embed request timed out', { id });\n        this.pending.delete(id);\n        resolve(texts.map(() => null));\n      }, REQUEST_TIMEOUT_MS);\n\n      this.pending.set(id, { resolve: resolve as (value: unknown) => void, timer });\n      this.worker!.postMessage({ type: 'embed_batch', id, texts });\n    });\n  }\n\n  /**\n   * Sends a shutdown message and waits for the worker to exit.\n   */\n  async shutdown(): Promise<void> {\n    if (!this.worker) {\n      return;\n    }\n\n    return new Promise<void>((resolve) => {\n      const w = this.worker!;\n      w.once('exit', () => {\n        this.worker = null;\n        this.ready = false;\n        this.resolveAllPending();\n        resolve();\n      });\n\n      w.postMessage({ type: 'shutdown' });\n\n      // Safety timeout -- force terminate if shutdown hangs\n      setTimeout(() => {\n        if (this.worker) {\n          debug('embed', 'Worker shutdown timed out, terminating');\n          this.worker.terminate();\n        }\n      }, 5_000);\n    });\n  }\n\n  /** Whether the worker is started and ready. */\n  isReady(): boolean {\n    return this.ready;\n  }\n\n  /** The engine name reported by the worker. */\n  getEngineName(): string {\n    return this.engineName;\n  }\n\n  /** The embedding dimensions reported by the worker. */\n  getDimensions(): number {\n    return this.dimensions;\n  }\n\n  /**\n   * Dispatches worker responses to the correct pending promise.\n   */\n  private handleMessage(msg: WorkerResponse): void {\n    if (msg.type === 'embed_result' || msg.type === 'embed_batch_result') {\n      const id = msg.id;\n      const req = this.pending.get(id);\n\n      if (req) {\n        clearTimeout(req.timer);\n        this.pending.delete(id);\n\n        if (msg.type === 'embed_result') {\n          req.resolve(msg.embedding);\n        } else {\n          req.resolve(msg.embeddings);\n        }\n      }\n    }\n  }\n\n  /**\n   * Resolves all pending requests with null (graceful degradation).\n   *\n   * Called on worker error or unexpected exit.\n   */\n  private resolveAllPending(): void {\n    for (const [id, req] of this.pending) {\n      clearTimeout(req.timer);\n      req.resolve(null);\n      this.pending.delete(id);\n    }\n  }\n}\n","// ---------------------------------------------------------------------------\n// Topic Shift Handler -- Integration Layer\n// ---------------------------------------------------------------------------\n// Orchestrates topic detection and context stashing. When a new observation\n// embedding has high cosine distance from the previous one, the current\n// context thread is automatically stashed and the user is notified.\n//\n// This is the integration layer that makes topic detection active:\n//   - Plan 01 built the detector (TopicShiftDetector)\n//   - Plan 02 built storage (StashManager)\n//   - Plan 05 built adaptive threshold (AdaptiveThresholdManager)\n//   - Plan 06 adds config (TopicDetectionConfig) and logging (DecisionLogger)\n//   - This module connects them into the live hook flow\n// ---------------------------------------------------------------------------\n\nimport { debug } from '../shared/debug.js';\nimport type { TopicShiftDetector } from '../intelligence/topic-detector.js';\nimport type { AdaptiveThresholdManager } from '../intelligence/adaptive-threshold.js';\nimport type { TopicShiftDecisionLogger, ShiftDecision } from '../intelligence/decision-logger.js';\nimport type { TopicDetectionConfig } from '../config/topic-detection-config.js';\nimport type { StashManager } from '../storage/stash-manager.js';\nimport type { ObservationRepository } from '../storage/observations.js';\nimport type { StashObservation } from '../types/stash.js';\nimport type { Observation } from '../shared/types.js';\n\n/**\n * Result of handling an observation through the topic shift pipeline.\n */\nexport interface TopicShiftHandlerResult {\n  /** Whether a stash was created due to topic shift */\n  stashed: boolean;\n  /** Notification message to surface to the user, or null if no shift */\n  notification: string | null;\n}\n\n/**\n * Dependencies required by TopicShiftHandler.\n *\n * Core dependencies (detector, stashManager, observationStore) are required.\n * Optional dependencies (config, decisionLogger, adaptiveManager) enable\n * additional functionality when provided. When omitted, the handler falls\n * back to simpler behavior for backward compatibility and easier test setups.\n */\nexport interface TopicShiftHandlerDeps {\n  detector: TopicShiftDetector;\n  stashManager: StashManager;\n  observationStore: ObservationRepository;\n  /** Optional: config for enable/disable, manual override, sensitivity */\n  config?: TopicDetectionConfig;\n  /** Optional: logs every detection decision for debugging */\n  decisionLogger?: TopicShiftDecisionLogger;\n  /** Optional: adaptive threshold manager for EWMA updates */\n  adaptiveManager?: AdaptiveThresholdManager;\n}\n\n/**\n * Orchestrates topic shift detection and automatic stashing.\n *\n * Full pipeline when all dependencies provided:\n *   1. Check config (enabled? manual override?)\n *   2. Run detector.detect(embedding)\n *   3. If adaptive manager: update EWMA and set new threshold\n *   4. Log decision via decision logger\n *   5. If shifted: gather observations, create stash, notify\n *   6. If not shifted: return no-op result\n *\n * When optional deps are omitted, steps 1/3/4 are skipped gracefully.\n */\nexport class TopicShiftHandler {\n  private readonly detector: TopicShiftDetector;\n  private readonly stashManager: StashManager;\n  private readonly observationStore: ObservationRepository;\n  private readonly config?: TopicDetectionConfig;\n  private readonly decisionLogger?: TopicShiftDecisionLogger;\n  private readonly adaptiveManager?: AdaptiveThresholdManager;\n\n  constructor(deps: TopicShiftHandlerDeps) {\n    this.detector = deps.detector;\n    this.stashManager = deps.stashManager;\n    this.observationStore = deps.observationStore;\n    this.config = deps.config;\n    this.decisionLogger = deps.decisionLogger;\n    this.adaptiveManager = deps.adaptiveManager;\n\n    debug('hook', 'TopicShiftHandler initialized', {\n      hasConfig: !!deps.config,\n      hasDecisionLogger: !!deps.decisionLogger,\n      hasAdaptiveManager: !!deps.adaptiveManager,\n    });\n  }\n\n  /**\n   * Evaluate an observation for topic shift.\n   *\n   * If a shift is detected, gathers recent observations from the previous\n   * topic, creates a stash snapshot, and returns a notification message\n   * for the user.\n   */\n  async handleObservation(\n    observation: Observation,\n    sessionId: string,\n    projectId: string,\n  ): Promise<TopicShiftHandlerResult> {\n    // 0. Config check: if detection is disabled, return early\n    if (this.config && !this.config.enabled) {\n      debug('hook', 'TopicShiftHandler: detection disabled by config');\n      return { stashed: false, notification: null };\n    }\n\n    // 1. No embedding -> skip detection\n    if (!observation.embedding) {\n      debug('hook', 'TopicShiftHandler: no embedding, skipping', {\n        id: observation.id,\n      });\n      return { stashed: false, notification: null };\n    }\n\n    // 2. Apply manual threshold override if configured\n    if (this.config?.manualThreshold !== undefined && this.config.manualThreshold !== null) {\n      this.detector.setThreshold(this.config.manualThreshold);\n    }\n\n    // 3. Run detector -- convert Float32Array to number[] for cosine distance\n    const embeddingArray = Array.from(observation.embedding);\n    const result = this.detector.detect(embeddingArray);\n\n    debug('hook', 'TopicShiftHandler: detection result', {\n      shifted: result.shifted,\n      distance: result.distance,\n      threshold: result.threshold,\n    });\n\n    // 4. Adaptive threshold update (if adaptive manager present and no manual override)\n    if (this.adaptiveManager && !(this.config?.manualThreshold !== undefined && this.config.manualThreshold !== null)) {\n      const newThreshold = this.adaptiveManager.update(result.distance);\n      this.detector.setThreshold(newThreshold);\n      debug('hook', 'TopicShiftHandler: adaptive threshold updated', {\n        newThreshold,\n      });\n    }\n\n    // 5. Determine stash ID (only available after stash creation below)\n    let stashId: string | null = null;\n\n    // 6. Handle shift: gather observations and create stash\n    if (result.shifted) {\n      // Gather previous topic observations\n      const recentObservations = this.observationStore.list({\n        sessionId,\n        limit: 20,\n      });\n\n      // Filter to observations before the current one (by timestamp)\n      const previousObservations = recentObservations.filter(\n        (obs) => obs.createdAt < observation.createdAt,\n      );\n\n      // Generate topic label from first previous observation\n      const topicLabel = this.generateTopicLabel(previousObservations);\n\n      // Generate summary\n      const summary = this.generateSummary(previousObservations);\n\n      // Create stash observation snapshots\n      const snapshots: StashObservation[] = previousObservations.map((obs) => ({\n        id: obs.id,\n        content: obs.content,\n        type: obs.source,\n        timestamp: obs.createdAt,\n        embedding: obs.embedding ? Array.from(obs.embedding) : null,\n      }));\n\n      // Create stash\n      const stash = this.stashManager.createStash({\n        projectId,\n        sessionId,\n        topicLabel,\n        summary,\n        observations: snapshots,\n      });\n\n      stashId = stash.id;\n\n      debug('hook', 'TopicShiftHandler: stash created', { topicLabel, stashId });\n\n      // 7. Log decision (if logger present)\n      if (this.decisionLogger) {\n        const decision: ShiftDecision = {\n          projectId,\n          sessionId,\n          observationId: observation.id,\n          distance: result.distance,\n          threshold: result.threshold,\n          ewmaDistance: this.adaptiveManager?.getState().ewmaDistance ?? null,\n          ewmaVariance: this.adaptiveManager?.getState().ewmaVariance ?? null,\n          sensitivityMultiplier: this.config?.sensitivityMultiplier ?? 1.5,\n          shifted: true,\n          confidence: result.confidence,\n          stashId,\n        };\n        this.decisionLogger.log(decision);\n      }\n\n      // Return notification\n      const notification = `Topic shift detected. Previous context stashed: \"${topicLabel}\". Use /laminark:resume to return.`;\n      return { stashed: true, notification };\n    }\n\n    // 8. Not shifted -- log decision and return no-op\n    if (this.decisionLogger) {\n      const decision: ShiftDecision = {\n        projectId,\n        sessionId,\n        observationId: observation.id,\n        distance: result.distance,\n        threshold: result.threshold,\n        ewmaDistance: this.adaptiveManager?.getState().ewmaDistance ?? null,\n        ewmaVariance: this.adaptiveManager?.getState().ewmaVariance ?? null,\n        sensitivityMultiplier: this.config?.sensitivityMultiplier ?? 1.5,\n        shifted: false,\n        confidence: result.confidence,\n        stashId: null,\n      };\n      this.decisionLogger.log(decision);\n    }\n\n    return { stashed: false, notification: null };\n  }\n\n  /**\n   * Generate a topic label from the first observation's content.\n   * Uses first 50 characters, trimmed and cleaned.\n   */\n  private generateTopicLabel(observations: Observation[]): string {\n    if (observations.length === 0) {\n      return 'Unknown topic';\n    }\n\n    const first = observations[observations.length - 1]; // oldest (list is DESC)\n    const raw = first.content.replace(/\\n/g, ' ').trim();\n    return raw.slice(0, 50) || 'Unknown topic';\n  }\n\n  /**\n   * Generate a brief summary by concatenating the first 3 observation contents,\n   * truncated to 200 characters total.\n   */\n  private generateSummary(observations: Observation[]): string {\n    if (observations.length === 0) {\n      return '';\n    }\n\n    // Take up to 3 oldest observations (list is DESC, so take from the end)\n    const oldest = observations.slice(-3).reverse();\n    const joined = oldest\n      .map((obs) => obs.content.replace(/\\n/g, ' ').trim())\n      .join(' | ');\n\n    return joined.slice(0, 200);\n  }\n}\n","// ---------------------------------------------------------------------------\n// Topic Shift Detection -- Static Threshold\n// ---------------------------------------------------------------------------\n// Computes cosine distance between consecutive observation embeddings and\n// determines whether a topic shift has occurred based on a static threshold.\n// Foundation for all topic detection in Phase 6 -- adaptive EWMA layered on\n// in Plan 05.\n// ---------------------------------------------------------------------------\n\n/**\n * Result of a topic shift detection check.\n */\nexport interface TopicShiftResult {\n  /** Whether a topic shift was detected */\n  shifted: boolean;\n  /** Cosine distance between current and previous embedding */\n  distance: number;\n  /** Threshold used for this detection */\n  threshold: number;\n  /** Confidence 0-1 -- how far past the threshold (0 if not shifted) */\n  confidence: number;\n  /** Previous embedding (null if first observation) */\n  previousEmbedding: number[] | null;\n  /** Current embedding that was evaluated */\n  currentEmbedding: number[];\n}\n\n/**\n * Compute cosine distance between two vectors.\n *\n * Returns 1 - cosineSimilarity(a, b).\n * Range: [0, 2] where 0 = identical, 1 = orthogonal, 2 = opposite.\n * Handles zero vectors gracefully by returning 0 (not NaN).\n */\nexport function cosineDistance(a: number[], b: number[]): number {\n  let dot = 0;\n  let magA = 0;\n  let magB = 0;\n\n  for (let i = 0; i < a.length; i++) {\n    dot += a[i] * b[i];\n    magA += a[i] * a[i];\n    magB += b[i] * b[i];\n  }\n\n  const magnitudeProduct = Math.sqrt(magA) * Math.sqrt(magB);\n\n  // Zero vector: treat as no distance (graceful, no NaN)\n  if (magnitudeProduct === 0) {\n    return 0;\n  }\n\n  const similarity = dot / magnitudeProduct;\n\n  // Clamp to [-1, 1] to handle floating-point rounding\n  const clampedSimilarity = Math.max(-1, Math.min(1, similarity));\n\n  return 1 - clampedSimilarity;\n}\n\n/**\n * Detects topic shifts by comparing consecutive observation embeddings\n * against a static cosine distance threshold.\n */\nexport class TopicShiftDetector {\n  private lastEmbedding: number[] | null = null;\n  private threshold: number;\n\n  constructor(options?: { threshold?: number }) {\n    this.threshold = options?.threshold ?? 0.3;\n  }\n\n  /**\n   * Evaluate a new embedding for topic shift against the previous one.\n   * Updates internal state with the new embedding after evaluation.\n   */\n  detect(embedding: number[]): TopicShiftResult {\n    const previous = this.lastEmbedding;\n    this.lastEmbedding = embedding;\n\n    // First observation -- no prior to compare against\n    if (previous === null) {\n      return {\n        shifted: false,\n        distance: 0,\n        threshold: this.threshold,\n        confidence: 0,\n        previousEmbedding: null,\n        currentEmbedding: embedding,\n      };\n    }\n\n    const distance = cosineDistance(previous, embedding);\n    const shifted = distance > this.threshold;\n    const confidence = shifted\n      ? Math.min((distance - this.threshold) / this.threshold, 1.0)\n      : 0;\n\n    return {\n      shifted,\n      distance,\n      threshold: this.threshold,\n      confidence,\n      previousEmbedding: previous,\n      currentEmbedding: embedding,\n    };\n  }\n\n  /** Clear last embedding state -- next detect is treated as first observation */\n  reset(): void {\n    this.lastEmbedding = null;\n  }\n\n  /** Get current threshold value */\n  getThreshold(): number {\n    return this.threshold;\n  }\n\n  /** Set threshold value, bounded to [0.05, 0.95] */\n  setThreshold(value: number): void {\n    this.threshold = Math.max(0.05, Math.min(0.95, value));\n  }\n}\n","// ---------------------------------------------------------------------------\n// EWMA Adaptive Topic Threshold\n// ---------------------------------------------------------------------------\n// Adjusts the topic shift detection threshold per-session based on observed\n// cosine distances using an Exponentially Weighted Moving Average (EWMA).\n//\n// A scattered session (high distances) raises the threshold over time.\n// A focused session (low distances) lowers the threshold over time.\n// New sessions seed from historical averages for cold start handling.\n// ---------------------------------------------------------------------------\n\n/**\n * Internal state of the adaptive threshold computation.\n */\nexport interface ThresholdState {\n  /** Exponentially weighted moving average of observed distances */\n  ewmaDistance: number;\n  /** Exponentially weighted variance of observed distances */\n  ewmaVariance: number;\n  /** Decay factor for EWMA (0 < alpha <= 1) */\n  alpha: number;\n  /** Standard deviations above the mean for threshold */\n  sensitivityMultiplier: number;\n  /** Number of distance observations processed */\n  observationCount: number;\n}\n\n/** Default EWMA distance when no history exists */\nconst DEFAULT_EWMA_DISTANCE = 0.3;\n\n/** Default EWMA variance when no history exists */\nconst DEFAULT_EWMA_VARIANCE = 0.01;\n\n/** Default decay factor */\nconst DEFAULT_ALPHA = 0.3;\n\n/** Default sensitivity (standard deviations above mean) */\nconst DEFAULT_SENSITIVITY_MULTIPLIER = 1.5;\n\n/** Hard lower bound for threshold -- prevents over-sensitive detection */\nconst THRESHOLD_MIN = 0.15;\n\n/** Hard upper bound for threshold -- prevents ignoring real shifts */\nconst THRESHOLD_MAX = 0.6;\n\n/**\n * Manages an EWMA-based adaptive threshold for topic shift detection.\n *\n * After each topic distance observation, call `update(distance)` to refine\n * the threshold. The threshold adapts:\n * - High distances (scattered topics) push the threshold up\n * - Low distances (focused topics) push the threshold down\n * - Threshold is bounded within [0.15, 0.6] to prevent extreme drift\n *\n * For cold start, call `seedFromHistory(avgDistance, avgVariance)` with\n * averages loaded from the ThresholdStore.\n */\nexport class AdaptiveThresholdManager {\n  private ewmaDistance: number;\n  private ewmaVariance: number;\n  private alpha: number;\n  private sensitivityMultiplier: number;\n  private observationCount: number;\n\n  constructor(options?: {\n    alpha?: number;\n    sensitivityMultiplier?: number;\n  }) {\n    this.alpha = options?.alpha ?? DEFAULT_ALPHA;\n    this.sensitivityMultiplier =\n      options?.sensitivityMultiplier ?? DEFAULT_SENSITIVITY_MULTIPLIER;\n    this.ewmaDistance = DEFAULT_EWMA_DISTANCE;\n    this.ewmaVariance = DEFAULT_EWMA_VARIANCE;\n    this.observationCount = 0;\n  }\n\n  /**\n   * Feed a new cosine distance observation and update the EWMA state.\n   *\n   * EWMA update formula:\n   * 1. ewmaDistance = alpha * distance + (1 - alpha) * ewmaDistance\n   * 2. diff = distance - ewmaDistance (after update)\n   * 3. ewmaVariance = alpha * (diff * diff) + (1 - alpha) * ewmaVariance\n   * 4. threshold = clamp(ewmaDistance + sensitivityMultiplier * sqrt(ewmaVariance), 0.15, 0.6)\n   *\n   * @param distance - Cosine distance from the latest topic detection\n   * @returns The new adaptive threshold value\n   */\n  update(distance: number): number {\n    // Step 1: Update EWMA distance\n    this.ewmaDistance =\n      this.alpha * distance + (1 - this.alpha) * this.ewmaDistance;\n\n    // Step 2: Compute deviation from new mean\n    const diff = distance - this.ewmaDistance;\n\n    // Step 3: Update EWMA variance\n    this.ewmaVariance =\n      this.alpha * (diff * diff) + (1 - this.alpha) * this.ewmaVariance;\n\n    // Step 4: Increment observation count\n    this.observationCount++;\n\n    // Step 5: Return clamped threshold\n    return this.getThreshold();\n  }\n\n  /**\n   * Seed the EWMA state from historical session averages (cold start).\n   * Does not reset observation count -- only updates the statistical seed.\n   */\n  seedFromHistory(averageDistance: number, averageVariance: number): void {\n    this.ewmaDistance = averageDistance;\n    this.ewmaVariance = averageVariance;\n  }\n\n  /**\n   * Compute the current threshold from EWMA state, clamped to bounds.\n   *\n   * Formula: ewmaDistance + sensitivityMultiplier * sqrt(ewmaVariance)\n   * Bounded to [0.15, 0.6]\n   */\n  getThreshold(): number {\n    const raw =\n      this.ewmaDistance +\n      this.sensitivityMultiplier * Math.sqrt(this.ewmaVariance);\n    return Math.max(THRESHOLD_MIN, Math.min(THRESHOLD_MAX, raw));\n  }\n\n  /**\n   * Return a snapshot of the current EWMA state.\n   */\n  getState(): ThresholdState {\n    return {\n      ewmaDistance: this.ewmaDistance,\n      ewmaVariance: this.ewmaVariance,\n      alpha: this.alpha,\n      sensitivityMultiplier: this.sensitivityMultiplier,\n      observationCount: this.observationCount,\n    };\n  }\n\n  /**\n   * Reset all EWMA state to defaults.\n   */\n  reset(): void {\n    this.ewmaDistance = DEFAULT_EWMA_DISTANCE;\n    this.ewmaVariance = DEFAULT_EWMA_VARIANCE;\n    this.observationCount = 0;\n  }\n}\n","// ---------------------------------------------------------------------------\n// Topic Shift Decision Logger\n// ---------------------------------------------------------------------------\n// Logs every topic shift decision (shifted or not) with all inputs for\n// debugging and threshold tuning. Requirement DQ-05 demands full\n// observability of the decision pipeline.\n//\n// Each decision record captures: distance, threshold, EWMA state,\n// sensitivity multiplier, shifted boolean, confidence, and optional\n// stash ID (when a shift triggered stashing).\n// ---------------------------------------------------------------------------\n\nimport type BetterSqlite3 from 'better-sqlite3';\nimport { randomBytes } from 'node:crypto';\n\nimport { debug } from '../shared/debug.js';\n\n/**\n * A single topic shift decision with all inputs recorded.\n */\nexport interface ShiftDecision {\n  /** Project scoping */\n  projectId: string;\n  /** Session scoping */\n  sessionId: string;\n  /** The observation that triggered this decision (null for synthetic events) */\n  observationId: string | null;\n  /** Cosine distance between consecutive embeddings */\n  distance: number;\n  /** Threshold used for this detection */\n  threshold: number;\n  /** EWMA distance at decision time (null if adaptive disabled) */\n  ewmaDistance: number | null;\n  /** EWMA variance at decision time (null if adaptive disabled) */\n  ewmaVariance: number | null;\n  /** Sensitivity multiplier in effect */\n  sensitivityMultiplier: number;\n  /** Whether a topic shift was detected */\n  shifted: boolean;\n  /** Confidence score (0-1) */\n  confidence: number;\n  /** Stash ID created by this shift (null if not shifted) */\n  stashId: string | null;\n}\n\n/**\n * Persists topic shift decisions for debugging and threshold tuning.\n *\n * Every call to detect() in the topic shift pipeline should result in\n * a corresponding log() call here, regardless of whether a shift was\n * detected. This provides complete visibility into the decision process.\n *\n * All SQL statements are prepared once in the constructor and reused\n * for every call (better-sqlite3 performance best practice).\n */\nexport class TopicShiftDecisionLogger {\n  private readonly db: BetterSqlite3.Database;\n\n  // Prepared statements\n  private readonly stmtInsert: BetterSqlite3.Statement;\n  private readonly stmtGetSessionDecisions: BetterSqlite3.Statement;\n  private readonly stmtGetShiftRate: BetterSqlite3.Statement;\n\n  constructor(db: BetterSqlite3.Database) {\n    this.db = db;\n\n    this.stmtInsert = db.prepare(`\n      INSERT INTO shift_decisions\n        (id, project_id, session_id, observation_id, distance, threshold,\n         ewma_distance, ewma_variance, sensitivity_multiplier, shifted,\n         confidence, stash_id)\n      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n    `);\n\n    this.stmtGetSessionDecisions = db.prepare(`\n      SELECT * FROM shift_decisions\n      WHERE project_id = ? AND session_id = ?\n      ORDER BY created_at DESC\n      LIMIT ?\n    `);\n\n    this.stmtGetShiftRate = db.prepare(`\n      SELECT\n        COUNT(*) AS total,\n        SUM(shifted) AS shifted_count\n      FROM (\n        SELECT shifted\n        FROM shift_decisions\n        WHERE project_id = ?\n        ORDER BY created_at DESC\n        LIMIT ?\n      )\n    `);\n\n    debug('db', 'TopicShiftDecisionLogger initialized');\n  }\n\n  /**\n   * Log a topic shift decision with all inputs.\n   *\n   * Should be called after every detect() call, regardless of outcome.\n   */\n  log(decision: ShiftDecision): void {\n    const id = randomBytes(16).toString('hex');\n\n    this.stmtInsert.run(\n      id,\n      decision.projectId,\n      decision.sessionId,\n      decision.observationId,\n      decision.distance,\n      decision.threshold,\n      decision.ewmaDistance,\n      decision.ewmaVariance,\n      decision.sensitivityMultiplier,\n      decision.shifted ? 1 : 0,\n      decision.confidence,\n      decision.stashId,\n    );\n\n    debug('db', 'Shift decision logged', {\n      shifted: decision.shifted,\n      distance: decision.distance,\n      threshold: decision.threshold,\n    });\n  }\n\n  /**\n   * Retrieve decisions for a specific session, ordered by recency.\n   *\n   * Useful for debugging: \"What happened in this session?\"\n   */\n  getSessionDecisions(\n    projectId: string,\n    sessionId: string,\n    limit: number = 50,\n  ): ShiftDecision[] {\n    const rows = this.stmtGetSessionDecisions.all(\n      projectId,\n      sessionId,\n      limit,\n    ) as DecisionRow[];\n\n    return rows.map(rowToDecision);\n  }\n\n  /**\n   * Compute shift rate statistics across recent decisions for a project.\n   *\n   * Returns the total number of decisions, how many were shifts, and\n   * the rate (0-1). Useful for tuning: a rate of 0.3 means 30% of\n   * observations triggered a topic shift.\n   */\n  getShiftRate(\n    projectId: string,\n    lastN: number = 100,\n  ): { total: number; shifted: number; rate: number } {\n    const row = this.stmtGetShiftRate.get(projectId, lastN) as {\n      total: number;\n      shifted_count: number | null;\n    };\n\n    const total = row.total;\n    const shifted = row.shifted_count ?? 0;\n    const rate = total > 0 ? shifted / total : 0;\n\n    return { total, shifted, rate };\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Internal row mapping\n// ---------------------------------------------------------------------------\n\ninterface DecisionRow {\n  id: string;\n  project_id: string;\n  session_id: string;\n  observation_id: string | null;\n  distance: number;\n  threshold: number;\n  ewma_distance: number | null;\n  ewma_variance: number | null;\n  sensitivity_multiplier: number;\n  shifted: number; // 0 or 1\n  confidence: number;\n  stash_id: string | null;\n  created_at: string;\n}\n\nfunction rowToDecision(row: DecisionRow): ShiftDecision {\n  return {\n    projectId: row.project_id,\n    sessionId: row.session_id,\n    observationId: row.observation_id,\n    distance: row.distance,\n    threshold: row.threshold,\n    ewmaDistance: row.ewma_distance,\n    ewmaVariance: row.ewma_variance,\n    sensitivityMultiplier: row.sensitivity_multiplier,\n    shifted: row.shifted === 1,\n    confidence: row.confidence,\n    stashId: row.stash_id,\n  };\n}\n","// ---------------------------------------------------------------------------\n// Topic Detection Configuration\n// ---------------------------------------------------------------------------\n// User-configurable sensitivity dial for topic shift detection.\n// Supports three presets (sensitive/balanced/relaxed), custom multipliers,\n// manual threshold override, and an enable/disable toggle.\n//\n// Configuration is loaded from .laminark/topic-detection.json with\n// safe defaults when the file does not exist.\n// ---------------------------------------------------------------------------\n\nimport { readFileSync } from 'node:fs';\nimport { join } from 'node:path';\n\nimport { debug } from '../shared/debug.js';\nimport { getConfigDir } from '../shared/config.js';\nimport type { TopicShiftDetector } from '../intelligence/topic-detector.js';\nimport type { AdaptiveThresholdManager } from '../intelligence/adaptive-threshold.js';\n\n/**\n * Sensitivity preset names for topic detection.\n */\nexport type SensitivityPreset = 'sensitive' | 'balanced' | 'relaxed';\n\n/**\n * Full configuration for topic detection behavior.\n */\nexport interface TopicDetectionConfig {\n  /** Named sensitivity preset */\n  sensitivityPreset: SensitivityPreset;\n  /** Derived from preset or custom -- multiplied with EWMA stddev for threshold */\n  sensitivityMultiplier: number;\n  /** If set, overrides adaptive threshold entirely */\n  manualThreshold: number | null;\n  /** EWMA decay factor (0 < alpha <= 1) */\n  ewmaAlpha: number;\n  /** Hard bounds for adaptive threshold */\n  thresholdBounds: { min: number; max: number };\n  /** Master toggle -- when false, topic detection is disabled */\n  enabled: boolean;\n}\n\n/**\n * Raw JSON shape for the configuration file.\n * All fields are optional -- missing fields use defaults.\n */\ninterface RawConfigJson {\n  sensitivityPreset?: string;\n  sensitivityMultiplier?: number;\n  manualThreshold?: number | null;\n  ewmaAlpha?: number;\n  thresholdBounds?: { min?: number; max?: number };\n  enabled?: boolean;\n}\n\n/**\n * Maps a sensitivity preset to its multiplier value.\n *\n * - sensitive (1.0): Detects smaller shifts -- lower bar for topic change\n * - balanced (1.5): Default -- moderate sensitivity\n * - relaxed (2.5): Only detects large shifts -- higher bar\n */\nexport function sensitivityPresetToMultiplier(preset: SensitivityPreset): number {\n  switch (preset) {\n    case 'sensitive':\n      return 1.0;\n    case 'balanced':\n      return 1.5;\n    case 'relaxed':\n      return 2.5;\n  }\n}\n\n/** Default configuration values */\nconst DEFAULTS: TopicDetectionConfig = {\n  sensitivityPreset: 'balanced',\n  sensitivityMultiplier: 1.5,\n  manualThreshold: null,\n  ewmaAlpha: 0.3,\n  thresholdBounds: { min: 0.15, max: 0.6 },\n  enabled: true,\n};\n\n/**\n * Loads topic detection configuration from disk.\n *\n * Reads .laminark/topic-detection.json (relative to the Laminark data\n * directory). Falls back to defaults if the file does not exist or\n * cannot be parsed. Validates threshold bounds constraints.\n */\nexport function loadTopicDetectionConfig(): TopicDetectionConfig {\n  const configPath = join(getConfigDir(), 'topic-detection.json');\n\n  let raw: RawConfigJson = {};\n\n  try {\n    const content = readFileSync(configPath, 'utf-8');\n    raw = JSON.parse(content) as RawConfigJson;\n    debug('config', 'Loaded topic detection config', { path: configPath });\n  } catch {\n    // File doesn't exist or is invalid -- use all defaults\n    debug('config', 'No topic detection config found, using defaults');\n    return { ...DEFAULTS };\n  }\n\n  // Resolve sensitivity preset\n  const validPresets: SensitivityPreset[] = ['sensitive', 'balanced', 'relaxed'];\n  const preset: SensitivityPreset = validPresets.includes(raw.sensitivityPreset as SensitivityPreset)\n    ? (raw.sensitivityPreset as SensitivityPreset)\n    : DEFAULTS.sensitivityPreset;\n\n  // Multiplier: explicit value > preset-derived > default\n  const multiplier =\n    typeof raw.sensitivityMultiplier === 'number' && raw.sensitivityMultiplier > 0\n      ? raw.sensitivityMultiplier\n      : sensitivityPresetToMultiplier(preset);\n\n  // Manual threshold: null means use adaptive\n  const manualThreshold =\n    typeof raw.manualThreshold === 'number' ? raw.manualThreshold : null;\n\n  // EWMA alpha\n  const ewmaAlpha =\n    typeof raw.ewmaAlpha === 'number' && raw.ewmaAlpha > 0 && raw.ewmaAlpha <= 1\n      ? raw.ewmaAlpha\n      : DEFAULTS.ewmaAlpha;\n\n  // Threshold bounds with validation\n  let boundsMin =\n    typeof raw.thresholdBounds?.min === 'number'\n      ? raw.thresholdBounds.min\n      : DEFAULTS.thresholdBounds.min;\n  let boundsMax =\n    typeof raw.thresholdBounds?.max === 'number'\n      ? raw.thresholdBounds.max\n      : DEFAULTS.thresholdBounds.max;\n\n  // Validate bounds: min >= 0.05, max <= 0.95, min < max\n  if (boundsMin < 0.05) boundsMin = 0.05;\n  if (boundsMax > 0.95) boundsMax = 0.95;\n  if (boundsMin >= boundsMax) {\n    boundsMin = DEFAULTS.thresholdBounds.min;\n    boundsMax = DEFAULTS.thresholdBounds.max;\n  }\n\n  // Enabled toggle\n  const enabled = typeof raw.enabled === 'boolean' ? raw.enabled : DEFAULTS.enabled;\n\n  return {\n    sensitivityPreset: preset,\n    sensitivityMultiplier: multiplier,\n    manualThreshold,\n    ewmaAlpha,\n    thresholdBounds: { min: boundsMin, max: boundsMax },\n    enabled,\n  };\n}\n\n/**\n * Applies a TopicDetectionConfig to a detector and adaptive manager.\n *\n * - If config.enabled is false, sets detector threshold to 999 (never triggers)\n * - If config.manualThreshold is set, uses it directly (bypasses adaptive)\n * - Otherwise, configures the adaptive manager with the sensitivity multiplier\n */\nexport function applyConfig(\n  config: TopicDetectionConfig,\n  detector: TopicShiftDetector,\n  adaptiveManager: AdaptiveThresholdManager,\n): void {\n  if (!config.enabled) {\n    // Disabled mode: set threshold so high that nothing triggers\n    detector.setThreshold(999);\n    debug('config', 'Topic detection disabled -- threshold set to 999');\n    return;\n  }\n\n  if (config.manualThreshold !== null) {\n    // Manual override: bypass adaptive entirely\n    detector.setThreshold(config.manualThreshold);\n    debug('config', 'Manual threshold override applied', {\n      threshold: config.manualThreshold,\n    });\n    return;\n  }\n\n  // Adaptive mode: configure the manager's sensitivity\n  // The adaptive manager uses sensitivityMultiplier internally via constructor,\n  // but we can influence it by seeding or updating its state.\n  // For now, apply the threshold from the adaptive manager to the detector.\n  const adaptiveThreshold = adaptiveManager.getThreshold();\n  detector.setThreshold(adaptiveThreshold);\n\n  debug('config', 'Adaptive config applied', {\n    preset: config.sensitivityPreset,\n    multiplier: config.sensitivityMultiplier,\n    threshold: adaptiveThreshold,\n  });\n}\n","import type BetterSqlite3 from 'better-sqlite3';\nimport { randomBytes } from 'node:crypto';\nimport { debug } from '../shared/debug.js';\n\nexport interface PendingNotification {\n  id: string;\n  projectId: string;\n  message: string;\n  createdAt: string;\n}\n\nexport class NotificationStore {\n  private readonly stmtInsert: BetterSqlite3.Statement;\n  private readonly stmtConsume: BetterSqlite3.Statement;\n  private readonly stmtSelect: BetterSqlite3.Statement;\n\n  constructor(db: BetterSqlite3.Database) {\n    // Create table inline (no migration needed -- simple transient store)\n    db.exec(`\n      CREATE TABLE IF NOT EXISTS pending_notifications (\n        id TEXT PRIMARY KEY,\n        project_id TEXT NOT NULL,\n        message TEXT NOT NULL,\n        created_at TEXT NOT NULL DEFAULT (datetime('now'))\n      )\n    `);\n\n    this.stmtInsert = db.prepare(\n      'INSERT INTO pending_notifications (id, project_id, message) VALUES (?, ?, ?)'\n    );\n    this.stmtSelect = db.prepare(\n      'SELECT * FROM pending_notifications WHERE project_id = ? ORDER BY created_at ASC LIMIT 10'\n    );\n    this.stmtConsume = db.prepare(\n      'DELETE FROM pending_notifications WHERE project_id = ?'\n    );\n    debug('db', 'NotificationStore initialized');\n  }\n\n  add(projectId: string, message: string): void {\n    const id = randomBytes(16).toString('hex');\n    this.stmtInsert.run(id, projectId, message);\n    debug('db', 'Notification added', { projectId });\n  }\n\n  /** Fetch and delete all pending notifications for a project (consume pattern). */\n  consumePending(projectId: string): PendingNotification[] {\n    const rows = this.stmtSelect.all(projectId) as Array<{\n      id: string; project_id: string; message: string; created_at: string;\n    }>;\n    if (rows.length > 0) {\n      this.stmtConsume.run(projectId);\n    }\n    return rows.map(r => ({\n      id: r.id,\n      projectId: r.project_id,\n      message: r.message,\n      createdAt: r.created_at,\n    }));\n  }\n}\n","/**\n * Rule-based pattern matchers for entity extraction.\n *\n * Each rule is a function that takes observation text and returns an array of\n * matched entities with type, confidence, and span information. Rules are\n * conservative -- only explicit patterns are matched, ambiguous text is skipped.\n *\n * Confidence scoring guidelines:\n *   0.95 - File paths (very reliable, unambiguous syntax)\n *   0.9  - Tool names (curated list, word-boundary matched)\n *   0.8  - Project names (org/repo pattern)\n *   0.7  - Decisions (language indicators)\n *   0.65 - Problems, Solutions (context-dependent phrases)\n *   0.6  - Person names (ambiguous, kept conservative)\n */\n\nimport type { EntityType } from './types.js';\n\n// =============================================================================\n// Types\n// =============================================================================\n\nexport interface ExtractionMatch {\n  name: string;\n  type: EntityType;\n  confidence: number;\n  span: [number, number];\n}\n\nexport type ExtractionRule = (text: string) => ExtractionMatch[];\n\n// =============================================================================\n// File Path Rule\n// =============================================================================\n\n/**\n * Matches file paths like src/foo/bar.ts, ./config.json, /absolute/path.ext, package.json\n *\n * Regex: paths with at least one dot-extension, allowing /, ., -, _ in path segments.\n * Confidence: 0.95 (file paths are very reliable)\n */\nexport const filePathRule: ExtractionRule = (text: string): ExtractionMatch[] => {\n  const matches: ExtractionMatch[] = [];\n  // Match file paths: optional leading ./ or /, then segments with alphanumeric/-/_ separated by /,\n  // ending with .extension. Must have at least one / or start with . or contain a known code extension.\n  const regex = /(?<![a-zA-Z0-9@#])(?:\\.\\/|\\/)?(?:[a-zA-Z0-9_-]+\\/)+[a-zA-Z0-9_.-]+\\.[a-zA-Z0-9]+(?![a-zA-Z0-9/])/g;\n  // Also match standalone filenames with extensions that are common in dev contexts\n  const standaloneRegex = /(?<![a-zA-Z0-9@#/])(?:[a-zA-Z0-9_-]+\\.(?:ts|tsx|js|jsx|json|md|yml|yaml|toml|css|scss|html|sql|sh|py|rs|go|java|rb|php|c|cpp|h|hpp|vue|svelte|astro|prisma|graphql|gql|env|lock|config|xml|csv|txt|log|gitignore|dockerignore|editorconfig))(?![a-zA-Z0-9/])/g;\n\n  let match: RegExpExecArray | null;\n\n  while ((match = regex.exec(text)) !== null) {\n    let name = match[0];\n    // Normalize: strip leading ./\n    if (name.startsWith('./')) name = name.slice(2);\n    // Collapse //\n    name = name.replace(/\\/\\//g, '/');\n\n    matches.push({\n      name,\n      type: 'File',\n      confidence: 0.95,\n      span: [match.index, match.index + match[0].length],\n    });\n  }\n\n  // Standalone files (e.g., \"package.json\", \"tsconfig.json\")\n  while ((match = standaloneRegex.exec(text)) !== null) {\n    const name = match[0];\n    // Skip if already captured by the path regex (check for overlap)\n    const overlaps = matches.some(\n      (m) => match!.index >= m.span[0] && match!.index < m.span[1],\n    );\n    if (!overlaps) {\n      matches.push({\n        name,\n        type: 'File',\n        confidence: 0.95,\n        span: [match.index, match.index + match[0].length],\n      });\n    }\n  }\n\n  return matches;\n};\n\n// =============================================================================\n// Decision Rule\n// =============================================================================\n\n/**\n * Matches phrases following decision indicators: \"decided to\", \"chose\", \"went with\",\n * \"selected\", \"opted for\", \"choosing between\", \"decision:\", \"the decision was\".\n *\n * Extracts the clause following the indicator (up to period, comma, or end of sentence).\n * Confidence: 0.7 (decision language can be ambiguous)\n */\nexport const decisionRule: ExtractionRule = (text: string): ExtractionMatch[] => {\n  const matches: ExtractionMatch[] = [];\n  const indicators = [\n    /\\bdecided\\s+to\\s+/gi,\n    /\\bchose\\s+(?:to\\s+)?/gi,\n    /\\bwent\\s+with\\s+/gi,\n    /\\bselected\\s+/gi,\n    /\\bopted\\s+for\\s+/gi,\n    /\\bchoosing\\s+between\\s+/gi,\n    /\\bdecision:\\s*/gi,\n    /\\bthe\\s+decision\\s+was\\s+(?:to\\s+)?/gi,\n  ];\n\n  for (const pattern of indicators) {\n    let match: RegExpExecArray | null;\n    while ((match = pattern.exec(text)) !== null) {\n      const clauseStart = match.index + match[0].length;\n      // Extract until period, semicolon, newline, or end of text\n      const remaining = text.slice(clauseStart);\n      const clauseEnd = remaining.search(/[.;\\n]|,\\s+(?:and|but|so|which|because|since)/);\n      let clause = clauseEnd >= 0 ? remaining.slice(0, clauseEnd) : remaining;\n      clause = clause.trim();\n\n      // Trim to max 100 chars\n      if (clause.length > 100) clause = clause.slice(0, 100).trim();\n      // Skip very short or empty clauses\n      if (clause.length < 3) continue;\n\n      matches.push({\n        name: clause,\n        type: 'Decision',\n        confidence: 0.7,\n        span: [match.index, clauseStart + (clauseEnd >= 0 ? clauseEnd : remaining.length)],\n      });\n    }\n  }\n\n  return matches;\n};\n\n// =============================================================================\n// Tool Rule\n// =============================================================================\n\n/**\n * Known tool/technology names -- curated list for conservative matching.\n * Case-insensitive, word-boundary aware.\n * Confidence: 0.9\n */\nconst KNOWN_TOOLS = [\n  // Linters & Formatters\n  'eslint', 'prettier', 'biome', 'stylelint', 'oxlint',\n  // Languages & Runtimes\n  'typescript', 'javascript', 'python', 'rust', 'golang',\n  'node', 'deno', 'bun',\n  // Package Managers\n  'npm', 'pnpm', 'yarn', 'cargo', 'pip',\n  // Bundlers & Build Tools\n  'webpack', 'vite', 'rollup', 'esbuild', 'tsup', 'tsdown', 'turbopack', 'parcel', 'swc',\n  // Test Frameworks\n  'jest', 'vitest', 'mocha', 'cypress', 'playwright', 'pytest',\n  // Frontend Frameworks\n  'react', 'vue', 'svelte', 'angular', 'solid', 'astro', 'next', 'nuxt', 'remix', 'gatsby',\n  // CSS Frameworks\n  'tailwind', 'tailwindcss', 'bootstrap', 'chakra',\n  // Databases\n  'sqlite', 'postgres', 'postgresql', 'mysql', 'mongodb', 'redis', 'supabase', 'dynamodb',\n  // ORMs & Query Builders\n  'prisma', 'drizzle', 'typeorm', 'sequelize', 'knex', 'kysely',\n  // Containers & Infrastructure\n  'docker', 'kubernetes', 'terraform', 'nginx', 'caddy',\n  // Version Control & CI\n  'git', 'github', 'gitlab', 'circleci', 'jenkins',\n  // Auth & Security\n  'jwt', 'oauth', 'bcrypt', 'argon2', 'jose',\n  // API & Communication\n  'graphql', 'grpc', 'trpc', 'express', 'fastify', 'hono', 'koa',\n  // AI/ML\n  'openai', 'anthropic', 'langchain', 'huggingface', 'onnx',\n  // Misc\n  'zod', 'ajv', 'winston', 'pino', 'socket.io', 'rxjs',\n  'storybook', 'chromatic', 'figma',\n] as const;\n\n// Build a regex that matches any tool name at word boundaries\nconst toolPattern = new RegExp(\n  `\\\\b(${KNOWN_TOOLS.map((t) => t.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&')).join('|')})\\\\b`,\n  'gi',\n);\n\nexport const toolRule: ExtractionRule = (text: string): ExtractionMatch[] => {\n  const matches: ExtractionMatch[] = [];\n  const seen = new Set<string>();\n\n  let match: RegExpExecArray | null;\n  // Reset regex state\n  toolPattern.lastIndex = 0;\n  while ((match = toolPattern.exec(text)) !== null) {\n    const name = match[1].toLowerCase();\n    if (seen.has(name)) continue;\n    seen.add(name);\n\n    matches.push({\n      name,\n      type: 'Tool',\n      confidence: 0.9,\n      span: [match.index, match.index + match[0].length],\n    });\n  }\n\n  return matches;\n};\n\n// =============================================================================\n// Person Rule\n// =============================================================================\n\n/**\n * Matches @-mentions and \"by/with [Capitalized Name]\" patterns.\n * Confidence: 0.6 (names are tricky, keep conservative)\n */\nexport const personRule: ExtractionRule = (text: string): ExtractionMatch[] => {\n  const matches: ExtractionMatch[] = [];\n  const seen = new Set<string>();\n\n  // @-mentions: @username (alphanumeric, hyphens, underscores)\n  const mentionRegex = /@([a-zA-Z][a-zA-Z0-9_-]{1,38})\\b/g;\n  let match: RegExpExecArray | null;\n  while ((match = mentionRegex.exec(text)) !== null) {\n    const name = match[1];\n    const lower = name.toLowerCase();\n    if (seen.has(lower)) continue;\n    seen.add(lower);\n\n    matches.push({\n      name: `@${name}`,\n      type: 'Person',\n      confidence: 0.6,\n      span: [match.index, match.index + match[0].length],\n    });\n  }\n\n  // \"by [Capitalized Name]\" -- e.g., \"reviewed by John Smith\"\n  const byRegex = /\\bby\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){0,2})\\b/g;\n  while ((match = byRegex.exec(text)) !== null) {\n    const name = match[1];\n    const lower = name.toLowerCase();\n    if (seen.has(lower)) continue;\n    seen.add(lower);\n\n    matches.push({\n      name,\n      type: 'Person',\n      confidence: 0.6,\n      span: [match.index, match.index + match[0].length],\n    });\n  }\n\n  // \"with [Capitalized Name]\" when preceded by interaction verbs\n  // Two-stage match: first find the verb+with phrase, then extract capitalized name after it\n  const withVerbRegex = /\\b(?:[Dd]ecided|[Ww]orked|[Pp]aired|[Cc]ollaborated|[Dd]iscussed|[Mm]et)\\s+with\\s+/g;\n  while ((match = withVerbRegex.exec(text)) !== null) {\n    // Extract capitalized name from text after the verb phrase\n    const afterVerb = text.slice(match.index + match[0].length);\n    const nameMatch = afterVerb.match(/^([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){0,2})\\b/);\n    if (!nameMatch) continue;\n\n    const name = nameMatch[1];\n    const lower = name.toLowerCase();\n    if (seen.has(lower)) continue;\n    seen.add(lower);\n\n    const fullEnd = match.index + match[0].length + nameMatch[0].length;\n    matches.push({\n      name,\n      type: 'Person',\n      confidence: 0.6,\n      span: [match.index, fullEnd],\n    });\n  }\n\n  return matches;\n};\n\n// =============================================================================\n// Problem Rule\n// =============================================================================\n\n/**\n * Matches phrases following problem indicators: \"bug in\", \"issue with\",\n * \"problem:\", \"error:\", \"failing\", \"broken\", \"doesn't work\", \"can't\", etc.\n * Confidence: 0.65\n */\nexport const problemRule: ExtractionRule = (text: string): ExtractionMatch[] => {\n  const matches: ExtractionMatch[] = [];\n  const indicators = [\n    /\\bbug\\s+in\\s+/gi,\n    /\\bissue\\s+with\\s+/gi,\n    /\\bproblem:\\s*/gi,\n    /\\berror:\\s*/gi,\n    /\\bfailing\\s+(?:to\\s+)?/gi,\n    /\\bbroken\\s+/gi,\n    /\\bdoesn'?t\\s+work\\s*/gi,\n    /\\bcan'?t\\s+/gi,\n    /\\bunable\\s+to\\s+/gi,\n    /\\bcrash(?:es|ing|ed)?\\s+(?:in|on|when|during)\\s+/gi,\n  ];\n\n  for (const pattern of indicators) {\n    let match: RegExpExecArray | null;\n    while ((match = pattern.exec(text)) !== null) {\n      const clauseStart = match.index + match[0].length;\n      const remaining = text.slice(clauseStart);\n      const clauseEnd = remaining.search(/[.;\\n]|,\\s+(?:and|but|so|which|because|since)/);\n      let clause = clauseEnd >= 0 ? remaining.slice(0, clauseEnd) : remaining;\n      clause = clause.trim();\n\n      if (clause.length > 100) clause = clause.slice(0, 100).trim();\n      if (clause.length < 3) continue;\n\n      matches.push({\n        name: clause,\n        type: 'Problem',\n        confidence: 0.65,\n        span: [match.index, clauseStart + (clauseEnd >= 0 ? clauseEnd : remaining.length)],\n      });\n    }\n  }\n\n  return matches;\n};\n\n// =============================================================================\n// Solution Rule\n// =============================================================================\n\n/**\n * Matches phrases following solution indicators: \"fixed by\", \"solved by\",\n * \"the fix was\", \"solution:\", \"resolved by\", \"workaround:\".\n * Confidence: 0.65\n */\nexport const solutionRule: ExtractionRule = (text: string): ExtractionMatch[] => {\n  const matches: ExtractionMatch[] = [];\n  const indicators = [\n    /\\bfixed\\s+by\\s+/gi,\n    /\\bsolved\\s+by\\s+/gi,\n    /\\bthe\\s+fix\\s+was\\s+/gi,\n    /\\bsolution:\\s*/gi,\n    /\\bresolved\\s+by\\s+/gi,\n    /\\bworkaround:\\s*/gi,\n  ];\n\n  for (const pattern of indicators) {\n    let match: RegExpExecArray | null;\n    while ((match = pattern.exec(text)) !== null) {\n      const clauseStart = match.index + match[0].length;\n      const remaining = text.slice(clauseStart);\n      const clauseEnd = remaining.search(/[.;\\n]|,\\s+(?:and|but|so|which|because|since)/);\n      let clause = clauseEnd >= 0 ? remaining.slice(0, clauseEnd) : remaining;\n      clause = clause.trim();\n\n      if (clause.length > 100) clause = clause.slice(0, 100).trim();\n      if (clause.length < 3) continue;\n\n      matches.push({\n        name: clause,\n        type: 'Solution',\n        confidence: 0.65,\n        span: [match.index, clauseStart + (clauseEnd >= 0 ? clauseEnd : remaining.length)],\n      });\n    }\n  }\n\n  return matches;\n};\n\n// =============================================================================\n// Project Rule\n// =============================================================================\n\n/**\n * Matches repository-style names (org/repo), project names in quotes after \"project\" keyword,\n * and package.json name references.\n * Confidence: 0.8\n */\nexport const projectRule: ExtractionRule = (text: string): ExtractionMatch[] => {\n  const matches: ExtractionMatch[] = [];\n  const seen = new Set<string>();\n\n  // org/repo pattern (e.g., \"facebook/react\", \"vercel/next.js\")\n  // Must have org/ prefix to distinguish from file paths (file paths have extensions)\n  // Negative lookbehind for @ to avoid matching scoped packages (handled separately)\n  const orgRepoRegex = /(?<![@a-zA-Z0-9])\\b([a-zA-Z0-9_-]+\\/[a-zA-Z0-9_.-]+)(?!\\.[a-zA-Z]{1,4}(?:\\b|\\/))(?!\\/)/g;\n  let match: RegExpExecArray | null;\n  while ((match = orgRepoRegex.exec(text)) !== null) {\n    const candidate = match[1];\n    // Filter out things that look like file paths (contain dot-extension patterns mid-path)\n    if (/\\.[a-zA-Z]{1,6}$/.test(candidate) && !/\\.js$/.test(candidate)) continue;\n    // Filter out common non-project patterns\n    if (/^(src|dist|lib|test|tests|node_modules|build|public)\\//.test(candidate)) continue;\n\n    const lower = candidate.toLowerCase();\n    if (seen.has(lower)) continue;\n    seen.add(lower);\n\n    matches.push({\n      name: candidate,\n      type: 'Project',\n      confidence: 0.8,\n      span: [match.index, match.index + match[0].length],\n    });\n  }\n\n  // \"project [name]\" or \"project: [name]\" with quoted name\n  const projectNameRegex = /\\bproject\\s*[:]\\s*[\"']([^\"']+)[\"']/gi;\n  while ((match = projectNameRegex.exec(text)) !== null) {\n    const name = match[1].trim();\n    const lower = name.toLowerCase();\n    if (seen.has(lower)) continue;\n    seen.add(lower);\n\n    matches.push({\n      name,\n      type: 'Project',\n      confidence: 0.8,\n      span: [match.index, match.index + match[0].length],\n    });\n  }\n\n  // @scope/package pattern (npm scoped packages)\n  const scopedRegex = /@([a-zA-Z0-9_-]+\\/[a-zA-Z0-9_.-]+)\\b/g;\n  while ((match = scopedRegex.exec(text)) !== null) {\n    const name = `@${match[1]}`;\n    const lower = name.toLowerCase();\n    if (seen.has(lower)) continue;\n    seen.add(lower);\n\n    matches.push({\n      name,\n      type: 'Project',\n      confidence: 0.8,\n      span: [match.index, match.index + match[0].length],\n    });\n  }\n\n  return matches;\n};\n\n// =============================================================================\n// Aggregated Rules Array\n// =============================================================================\n\n/**\n * All extraction rules in priority order (higher confidence first).\n * Use this for iteration in the extraction pipeline.\n */\nexport const ALL_RULES: ExtractionRule[] = [\n  filePathRule,\n  toolRule,\n  projectRule,\n  decisionRule,\n  problemRule,\n  solutionRule,\n  personRule,\n];\n","/**\n * Entity extraction pipeline.\n *\n * Transforms observation text into typed GraphNode entities by running\n * rule-based pattern matchers and persisting results to the knowledge graph.\n *\n * Pipeline:\n *   1. Run ALL_RULES against text\n *   2. Deduplicate by name (keep highest confidence)\n *   3. Resolve overlapping spans (higher confidence wins)\n *   4. Filter by minimum confidence threshold\n *   5. Return sorted by confidence descending\n */\n\nimport type BetterSqlite3 from 'better-sqlite3';\n\nimport type { EntityType } from './types.js';\nimport type { GraphNode } from './types.js';\nimport { upsertNode } from './schema.js';\nimport { ALL_RULES, type ExtractionMatch } from './extraction-rules.js';\n\n// =============================================================================\n// Types\n// =============================================================================\n\nexport interface EntityExtractionResult {\n  entities: Array<{ name: string; type: EntityType; confidence: number }>;\n  observationId: string;\n  extractedAt: string;\n}\n\n// =============================================================================\n// Constants\n// =============================================================================\n\nconst DEFAULT_MIN_CONFIDENCE = 0.5;\n\n// =============================================================================\n// Core Extraction\n// =============================================================================\n\n/**\n * Extracts entities from observation text using all registered rules.\n *\n * - Runs every rule against the text\n * - Deduplicates: same name from multiple rules keeps highest confidence\n * - Resolves overlapping spans: higher confidence wins\n * - Filters by minimum confidence threshold\n * - Returns sorted by confidence descending\n */\nexport function extractEntities(\n  text: string,\n  observationId: string,\n  opts?: { minConfidence?: number },\n): EntityExtractionResult {\n  const minConfidence = opts?.minConfidence ?? DEFAULT_MIN_CONFIDENCE;\n\n  // Step 1: Run all rules and collect matches\n  const allMatches: ExtractionMatch[] = [];\n  for (const rule of ALL_RULES) {\n    try {\n      const results = rule(text);\n      allMatches.push(...results);\n    } catch {\n      // Never fail the whole extraction because one rule had issues\n      continue;\n    }\n  }\n\n  // Step 2: Resolve overlapping spans (higher confidence wins)\n  const nonOverlapping = resolveOverlaps(allMatches);\n\n  // Step 3: Deduplicate by name+type (keep highest confidence)\n  const deduped = deduplicateByName(nonOverlapping);\n\n  // Step 4: Filter by minimum confidence threshold\n  const filtered = deduped.filter((m) => m.confidence >= minConfidence);\n\n  // Step 5: Sort by confidence descending\n  filtered.sort((a, b) => b.confidence - a.confidence);\n\n  return {\n    entities: filtered.map((m) => ({\n      name: m.name,\n      type: m.type,\n      confidence: m.confidence,\n    })),\n    observationId,\n    extractedAt: new Date().toISOString(),\n  };\n}\n\n// =============================================================================\n// Persistence\n// =============================================================================\n\n/**\n * Extracts entities from text and persists them as graph nodes.\n *\n * For each extracted entity:\n *   - Calls upsertNode (creates or merges with existing node)\n *   - Appends observationId to the node's observation_ids array\n *\n * Wrapped in a transaction for atomicity. Individual entity failures\n * are logged and skipped (never fail the whole batch).\n *\n * @returns Array of persisted GraphNode objects\n */\nexport function extractAndPersist(\n  db: BetterSqlite3.Database,\n  text: string,\n  observationId: string,\n  opts?: { minConfidence?: number },\n): GraphNode[] {\n  const result = extractEntities(text, observationId, opts);\n  const persisted: GraphNode[] = [];\n\n  const persist = db.transaction(() => {\n    for (const entity of result.entities) {\n      try {\n        const node = upsertNode(db, {\n          type: entity.type,\n          name: entity.name,\n          metadata: { confidence: entity.confidence },\n          observation_ids: [observationId],\n        });\n        persisted.push(node);\n      } catch {\n        // Log warning and continue with remaining entities\n        // Never fail the whole extraction because one entity had issues\n        continue;\n      }\n    }\n  });\n\n  persist();\n  return persisted;\n}\n\n// =============================================================================\n// Internal Helpers\n// =============================================================================\n\n/**\n * Resolves overlapping spans between same-type entities.\n *\n * Only removes overlapping matches when they share the same entity type\n * (e.g., two Tool entities on overlapping text). Different types are\n * allowed to overlap since they represent different semantic information\n * (e.g., a Decision span can contain a Tool name within it).\n *\n * When same-type spans overlap, the one with higher confidence wins.\n */\nfunction resolveOverlaps(matches: ExtractionMatch[]): ExtractionMatch[] {\n  if (matches.length <= 1) return [...matches];\n\n  // Sort by confidence descending so higher-confidence matches are added first\n  const sorted = [...matches].sort((a, b) => b.confidence - a.confidence);\n\n  const result: ExtractionMatch[] = [];\n\n  for (const match of sorted) {\n    // Only check for overlap with same-type entities\n    const sameTypeOverlap = result.findIndex(\n      (kept) =>\n        kept.type === match.type &&\n        match.span[0] < kept.span[1] &&\n        match.span[1] > kept.span[0],\n    );\n\n    if (sameTypeOverlap === -1) {\n      // No same-type overlap -- keep it\n      result.push(match);\n    }\n    // Otherwise skip (same-type overlap, existing has higher or equal confidence)\n  }\n\n  return result;\n}\n\n/**\n * Deduplicates matches by name+type. When the same entity name appears\n * multiple times (possibly from different rules), keeps the one with\n * the highest confidence score.\n */\nfunction deduplicateByName(matches: ExtractionMatch[]): ExtractionMatch[] {\n  const byKey = new Map<string, ExtractionMatch>();\n\n  for (const match of matches) {\n    const key = `${match.type}:${match.name.toLowerCase()}`;\n    const existing = byKey.get(key);\n    if (!existing || match.confidence > existing.confidence) {\n      byKey.set(key, match);\n    }\n  }\n\n  return [...byKey.values()];\n}\n","/**\n * Graph constraint enforcement.\n *\n * Maintains graph health through:\n *   - Entity type taxonomy validation (defense-in-depth with SQL CHECK)\n *   - Relationship type taxonomy validation\n *   - Max degree enforcement (prune lowest-weight edges when cap exceeded)\n *   - Entity deduplication detection and merging\n *   - Graph health dashboard metrics\n *\n * All enforcement functions use transactions for atomicity.\n * Significant actions (pruning, merging) are logged with [laminark:graph] prefix.\n */\n\nimport type BetterSqlite3 from 'better-sqlite3';\n\nimport type { EntityType, RelationshipType, GraphNode, GraphEdge } from './types.js';\nimport {\n  ENTITY_TYPES,\n  RELATIONSHIP_TYPES,\n  MAX_NODE_DEGREE,\n} from './types.js';\nimport {\n  getEdgesForNode,\n  countEdgesForNode,\n  getNodesByType,\n} from './schema.js';\n\n// =============================================================================\n// Type Validation\n// =============================================================================\n\n/**\n * Runtime validation for entity types. Defense-in-depth alongside SQL CHECK.\n */\nexport function validateEntityType(type: string): type is EntityType {\n  return (ENTITY_TYPES as readonly string[]).includes(type);\n}\n\n/**\n * Runtime validation for relationship types. Defense-in-depth alongside SQL CHECK.\n */\nexport function validateRelationshipType(type: string): type is RelationshipType {\n  return (RELATIONSHIP_TYPES as readonly string[]).includes(type);\n}\n\n// =============================================================================\n// Max Degree Enforcement\n// =============================================================================\n\n/**\n * Enforces maximum edge count on a node by pruning lowest-weight edges.\n *\n * When a node exceeds maxDegree edges:\n *   1. Get all edges for the node\n *   2. Sort by weight ascending (lowest first)\n *   3. Delete lowest-weight edges until count <= maxDegree\n *   4. Log pruned count with [laminark:graph] prefix\n *\n * Runs in a transaction to prevent race conditions.\n *\n * @param db - Database handle\n * @param nodeId - The node to enforce degree cap on\n * @param maxDegree - Maximum allowed edges (default: MAX_NODE_DEGREE = 50)\n * @returns Object with pruned count and remaining count\n */\nexport function enforceMaxDegree(\n  db: BetterSqlite3.Database,\n  nodeId: string,\n  maxDegree: number = MAX_NODE_DEGREE,\n): { pruned: number; remaining: number } {\n  const enforce = db.transaction(() => {\n    const currentCount = countEdgesForNode(db, nodeId);\n\n    if (currentCount <= maxDegree) {\n      return { pruned: 0, remaining: currentCount };\n    }\n\n    // Get all edges, sorted by weight ascending (lowest first)\n    const edges = getEdgesForNode(db, nodeId);\n    edges.sort((a, b) => a.weight - b.weight);\n\n    const toPrune = currentCount - maxDegree;\n    const edgesToDelete = edges.slice(0, toPrune);\n\n    const deleteStmt = db.prepare('DELETE FROM graph_edges WHERE id = ?');\n    for (const edge of edgesToDelete) {\n      deleteStmt.run(edge.id);\n    }\n\n    const remaining = currentCount - toPrune;\n\n    process.stderr.write(\n      `[laminark:graph] Pruned ${toPrune} lowest-weight edges from node ${nodeId} (${remaining} remaining)\\n`,\n    );\n\n    return { pruned: toPrune, remaining };\n  });\n\n  return enforce();\n}\n\n// =============================================================================\n// Entity Merging\n// =============================================================================\n\n/**\n * Merges one entity node into another. The keepId node survives.\n *\n * Steps:\n *   1. Union observation_ids from both nodes (no duplicates)\n *   2. Reroute all edges from mergeId to keepId\n *   3. Handle duplicate edge conflicts (keep higher weight)\n *   4. Delete the mergeId node\n *\n * Runs in a transaction for atomicity.\n *\n * @param db - Database handle\n * @param keepId - The node to keep (survives merge)\n * @param mergeId - The node to merge and delete\n */\nexport function mergeEntities(\n  db: BetterSqlite3.Database,\n  keepId: string,\n  mergeId: string,\n): void {\n  const merge = db.transaction(() => {\n    // Step 1: Get both nodes and merge observation_ids\n    const keepRow = db\n      .prepare('SELECT * FROM graph_nodes WHERE id = ?')\n      .get(keepId) as { observation_ids: string; metadata: string } | undefined;\n    const mergeRow = db\n      .prepare('SELECT * FROM graph_nodes WHERE id = ?')\n      .get(mergeId) as { observation_ids: string; metadata: string } | undefined;\n\n    if (!keepRow || !mergeRow) {\n      throw new Error(`Cannot merge: one or both nodes not found (keep=${keepId}, merge=${mergeId})`);\n    }\n\n    const keepObsIds = JSON.parse(keepRow.observation_ids) as string[];\n    const mergeObsIds = JSON.parse(mergeRow.observation_ids) as string[];\n    const mergedObsIds = [...new Set([...keepObsIds, ...mergeObsIds])];\n\n    // Merge metadata (merge node values fill gaps, keep node values take priority)\n    const keepMeta = JSON.parse(keepRow.metadata) as Record<string, unknown>;\n    const mergeMeta = JSON.parse(mergeRow.metadata) as Record<string, unknown>;\n    const mergedMeta = { ...mergeMeta, ...keepMeta };\n\n    db.prepare(\n      `UPDATE graph_nodes SET observation_ids = ?, metadata = ?, updated_at = datetime('now') WHERE id = ?`,\n    ).run(JSON.stringify(mergedObsIds), JSON.stringify(mergedMeta), keepId);\n\n    // Step 2: Get all edges connected to the merge node\n    const mergeEdges = getEdgesForNode(db, mergeId);\n\n    // Step 3: Reroute edges from mergeId to keepId\n    for (const edge of mergeEdges) {\n      let newSourceId = edge.source_id;\n      let newTargetId = edge.target_id;\n\n      if (edge.source_id === mergeId) {\n        newSourceId = keepId;\n      }\n      if (edge.target_id === mergeId) {\n        newTargetId = keepId;\n      }\n\n      // Skip self-loops (would happen if both source and target are the merge/keep pair)\n      if (newSourceId === newTargetId) {\n        db.prepare('DELETE FROM graph_edges WHERE id = ?').run(edge.id);\n        continue;\n      }\n\n      // Check if rerouted edge would create a duplicate\n      const existing = db\n        .prepare(\n          'SELECT id, weight FROM graph_edges WHERE source_id = ? AND target_id = ? AND type = ?',\n        )\n        .get(newSourceId, newTargetId, edge.type) as\n        | { id: string; weight: number }\n        | undefined;\n\n      if (existing && existing.id !== edge.id) {\n        // Duplicate: keep higher weight, delete the lower one\n        if (edge.weight > existing.weight) {\n          db.prepare('UPDATE graph_edges SET weight = ? WHERE id = ?').run(\n            edge.weight,\n            existing.id,\n          );\n        }\n        db.prepare('DELETE FROM graph_edges WHERE id = ?').run(edge.id);\n      } else if (!existing) {\n        // No duplicate: update the edge to point to keepId\n        db.prepare(\n          'UPDATE graph_edges SET source_id = ?, target_id = ? WHERE id = ?',\n        ).run(newSourceId, newTargetId, edge.id);\n      }\n      // If existing.id === edge.id, it's already correct (no-op)\n    }\n\n    // Step 4: Delete the merged node\n    db.prepare('DELETE FROM graph_nodes WHERE id = ?').run(mergeId);\n\n    process.stderr.write(\n      `[laminark:graph] Merged entity ${mergeId} into ${keepId} (${mergeEdges.length} edges rerouted)\\n`,\n    );\n  });\n\n  merge();\n}\n\n// =============================================================================\n// Duplicate Detection\n// =============================================================================\n\n/**\n * Common abbreviation mappings for duplicate detection.\n * Maps lowercase abbreviation -> lowercase full name.\n */\nconst ABBREVIATION_MAP: Record<string, string> = {\n  ts: 'typescript',\n  js: 'javascript',\n  py: 'python',\n  rb: 'ruby',\n  rs: 'rust',\n  pg: 'postgresql',\n  postgres: 'postgresql',\n  mongo: 'mongodb',\n  k8s: 'kubernetes',\n  tf: 'terraform',\n  gh: 'github',\n  gl: 'gitlab',\n  ci: 'circleci',\n  gql: 'graphql',\n  tw: 'tailwind',\n  tailwindcss: 'tailwind',\n  sw: 'swc',\n  np: 'numpy',\n  pd: 'pandas',\n  wp: 'webpack',\n  nx: 'next',\n};\n\n/**\n * Finds potential duplicate entities in the graph.\n *\n * Detection strategies:\n *   a. Case-insensitive name match (e.g., \"React\" and \"react\")\n *   b. Common abbreviation match (e.g., \"TS\" and \"TypeScript\")\n *   c. Path normalization for Files (strip ./, normalize separators)\n *\n * Returns grouped duplicate candidates with reasons. This is a\n * SUGGESTION function -- use mergeEntities() to act on results.\n *\n * @param db - Database handle\n * @param opts - Optional filter by entity type\n * @returns Array of duplicate groups with entities and reason\n */\nexport function findDuplicateEntities(\n  db: BetterSqlite3.Database,\n  opts?: { type?: EntityType },\n): Array<{ entities: GraphNode[]; reason: string }> {\n  // Get all nodes, optionally filtered by type\n  let nodes: GraphNode[];\n  if (opts?.type) {\n    nodes = getNodesByType(db, opts.type);\n  } else {\n    const allTypes = ENTITY_TYPES;\n    nodes = [];\n    for (const type of allTypes) {\n      nodes.push(...getNodesByType(db, type));\n    }\n  }\n\n  const duplicates: Array<{ entities: GraphNode[]; reason: string }> = [];\n  const seen = new Set<string>(); // Track already-grouped node IDs\n\n  // Strategy A: Case-insensitive name match within same type\n  const byTypeAndLowerName = new Map<string, GraphNode[]>();\n  for (const node of nodes) {\n    const key = `${node.type}:${node.name.toLowerCase()}`;\n    const group = byTypeAndLowerName.get(key) ?? [];\n    group.push(node);\n    byTypeAndLowerName.set(key, group);\n  }\n\n  for (const [, group] of byTypeAndLowerName) {\n    if (group.length > 1) {\n      const ids = group.map((n) => n.id).sort().join(',');\n      if (!seen.has(ids)) {\n        seen.add(ids);\n        duplicates.push({\n          entities: group,\n          reason: `Case-insensitive name match: \"${group[0].name}\" and \"${group[1].name}\"`,\n        });\n      }\n    }\n  }\n\n  // Strategy B: Common abbreviation match within same type\n  const byTypeAndCanonical = new Map<string, GraphNode[]>();\n  for (const node of nodes) {\n    const lower = node.name.toLowerCase();\n    const canonical = ABBREVIATION_MAP[lower] ?? lower;\n    const key = `${node.type}:${canonical}`;\n    const group = byTypeAndCanonical.get(key) ?? [];\n    group.push(node);\n    byTypeAndCanonical.set(key, group);\n  }\n\n  for (const [, group] of byTypeAndCanonical) {\n    if (group.length > 1) {\n      // Check if this is a genuine abbreviation match (not just case match already found)\n      const names = new Set(group.map((n) => n.name.toLowerCase()));\n      if (names.size > 1) {\n        const ids = group.map((n) => n.id).sort().join(',');\n        if (!seen.has(ids)) {\n          seen.add(ids);\n          duplicates.push({\n            entities: group,\n            reason: `Common abbreviation match: \"${group[0].name}\" and \"${group[1].name}\"`,\n          });\n        }\n      }\n    }\n  }\n\n  // Strategy C: Path normalization for File type\n  if (!opts?.type || opts.type === 'File') {\n    const fileNodes = nodes.filter((n) => n.type === 'File');\n    const byNormalizedPath = new Map<string, GraphNode[]>();\n\n    for (const node of fileNodes) {\n      let normalized = node.name;\n      // Strip leading ./\n      if (normalized.startsWith('./')) normalized = normalized.slice(2);\n      // Normalize separators\n      normalized = normalized.replace(/\\\\/g, '/');\n      // Collapse double slashes\n      normalized = normalized.replace(/\\/\\//g, '/');\n      // Lowercase for comparison\n      normalized = normalized.toLowerCase();\n\n      const key = `File:${normalized}`;\n      const group = byNormalizedPath.get(key) ?? [];\n      group.push(node);\n      byNormalizedPath.set(key, group);\n    }\n\n    for (const [, group] of byNormalizedPath) {\n      if (group.length > 1) {\n        const ids = group.map((n) => n.id).sort().join(',');\n        if (!seen.has(ids)) {\n          seen.add(ids);\n          duplicates.push({\n            entities: group,\n            reason: `Path normalization match: \"${group[0].name}\" and \"${group[1].name}\"`,\n          });\n        }\n      }\n    }\n  }\n\n  return duplicates;\n}\n\n// =============================================================================\n// Graph Health Dashboard\n// =============================================================================\n\ninterface NodeRow {\n  id: string;\n  type: string;\n  name: string;\n  metadata: string;\n  observation_ids: string;\n  created_at: string;\n  updated_at: string;\n}\n\n/**\n * Returns dashboard-style health metrics for the knowledge graph.\n *\n * Metrics:\n *   - totalNodes: total entity count\n *   - totalEdges: total relationship count\n *   - avgDegree: average edges per node\n *   - maxDegree: highest edge count on any single node\n *   - hotspots: nodes with degree > 0.8 * MAX_NODE_DEGREE\n *   - duplicateCandidates: number of detected duplicate groups\n */\nexport function getGraphHealth(db: BetterSqlite3.Database): {\n  totalNodes: number;\n  totalEdges: number;\n  avgDegree: number;\n  maxDegree: number;\n  hotspots: Array<{ node: GraphNode; degree: number }>;\n  duplicateCandidates: number;\n} {\n  const totalNodes =\n    (db.prepare('SELECT COUNT(*) as cnt FROM graph_nodes').get() as { cnt: number }).cnt;\n  const totalEdges =\n    (db.prepare('SELECT COUNT(*) as cnt FROM graph_edges').get() as { cnt: number }).cnt;\n\n  const avgDegree = totalNodes > 0 ? (totalEdges * 2) / totalNodes : 0;\n\n  // Find max degree and hotspot nodes\n  const hotspotThreshold = Math.floor(0.8 * MAX_NODE_DEGREE);\n  let maxDeg = 0;\n  const hotspots: Array<{ node: GraphNode; degree: number }> = [];\n\n  if (totalNodes > 0) {\n    // Get degree for each node using a correlated subquery\n    const degreeRows = db\n      .prepare(\n        `SELECT n.*,\n          (SELECT COUNT(*) FROM graph_edges WHERE source_id = n.id OR target_id = n.id) as degree\n        FROM graph_nodes n\n        WHERE (SELECT COUNT(*) FROM graph_edges WHERE source_id = n.id OR target_id = n.id) > 0\n        ORDER BY degree DESC`,\n      )\n      .all() as Array<NodeRow & { degree: number }>;\n\n    for (const row of degreeRows) {\n      if (row.degree > maxDeg) maxDeg = row.degree;\n\n      if (row.degree > hotspotThreshold) {\n        hotspots.push({\n          node: {\n            id: row.id,\n            type: row.type as EntityType,\n            name: row.name,\n            metadata: JSON.parse(row.metadata) as Record<string, unknown>,\n            observation_ids: JSON.parse(row.observation_ids) as string[],\n            created_at: row.created_at,\n            updated_at: row.updated_at,\n          },\n          degree: row.degree,\n        });\n      }\n    }\n  }\n\n  // Count duplicate candidates\n  const dupes = findDuplicateEntities(db);\n\n  return {\n    totalNodes,\n    totalEdges,\n    avgDegree,\n    maxDegree: maxDeg,\n    hotspots,\n    duplicateCandidates: dupes.length,\n  };\n}\n","/**\n * Relationship detection between co-occurring entities.\n *\n * Takes observation text and already-extracted entities, then infers typed\n * relationships based on entity type pairs and context signals in the text.\n * Proximity and sentence co-occurrence boost confidence scores.\n *\n * Pipeline:\n *   1. For each unique entity pair, check type-pair inference rules\n *   2. Scan text for context signals to override default relationship type\n *   3. Apply proximity and sentence co-occurrence confidence boosts\n *   4. Filter self-relationships and low-confidence results\n */\n\nimport type BetterSqlite3 from 'better-sqlite3';\n\nimport type { EntityType, RelationshipType, GraphEdge } from './types.js';\nimport { getNodeByNameAndType, insertEdge } from './schema.js';\nimport { enforceMaxDegree } from './constraints.js';\n\n// =============================================================================\n// Types\n// =============================================================================\n\nexport interface RelationshipCandidate {\n  sourceEntity: { name: string; type: EntityType };\n  targetEntity: { name: string; type: EntityType };\n  relationshipType: RelationshipType;\n  confidence: number; // 0.0-1.0\n  evidence: string; // the text snippet that led to this inference\n}\n\n// =============================================================================\n// Context Signal Patterns\n// =============================================================================\n\n/**\n * Ordered by specificity. First match wins.\n */\nconst CONTEXT_SIGNALS: Array<{\n  pattern: RegExp;\n  type: RelationshipType;\n}> = [\n  // Most specific signals first (multi-word phrases and directional indicators)\n  { pattern: /\\b(?:decided|chose|selected)\\b/i, type: 'decided_by' },\n  { pattern: /\\b(?:solved\\s+by|fixed\\s+by|resolved\\s+by)\\b/i, type: 'solved_by' },\n  { pattern: /\\b(?:caused\\s+by|because\\s+of|due\\s+to)\\b/i, type: 'caused_by' },\n  { pattern: /\\b(?:depends?\\s+on|requires?|imports?)\\b/i, type: 'depends_on' },\n  { pattern: /\\b(?:part\\s+of|belongs?\\s+to|inside)\\b/i, type: 'part_of' },\n  // Least specific signals last (common words that could be coincidental)\n  { pattern: /\\b(?:uses?|using)\\b/i, type: 'uses' },\n];\n\n// =============================================================================\n// Type-Pair Inference Rules\n// =============================================================================\n\n/**\n * Default relationship type based on entity type pair.\n * Key format: \"SourceType->TargetType\"\n */\nconst TYPE_PAIR_DEFAULTS: Record<string, RelationshipType> = {\n  'File->Tool': 'uses',\n  'Tool->File': 'uses',\n  'File->File': 'related_to', // overridden by context if import/require present\n  'Decision->Tool': 'related_to',\n  'Tool->Decision': 'related_to',\n  'Decision->Person': 'decided_by',\n  'Person->Decision': 'decided_by',\n  'Problem->File': 'part_of',\n  'File->Problem': 'part_of',\n  'Problem->Solution': 'solved_by',\n  'Solution->Problem': 'solved_by',\n  'Solution->Tool': 'uses',\n  'Tool->Solution': 'uses',\n  'Project->File': 'part_of',\n  'File->Project': 'part_of',\n};\n\n// =============================================================================\n// Core Detection\n// =============================================================================\n\n/**\n * Detects typed relationships between co-occurring entities in observation text.\n *\n * For each unique entity pair:\n *   1. Determine base relationship type from type-pair rules\n *   2. Check text context signals to refine relationship type\n *   3. Apply proximity boost (+0.1 for entities within 50 chars)\n *   4. Apply sentence co-occurrence boost (+0.15 for same sentence)\n *   5. Filter out self-relationships\n *\n * @param text - The observation text containing the entities\n * @param entities - Already-extracted entities with name and type\n * @returns Array of relationship candidates with confidence scores\n */\nexport function detectRelationships(\n  text: string,\n  entities: Array<{ name: string; type: EntityType }>,\n): RelationshipCandidate[] {\n  if (entities.length < 2) return [];\n\n  const candidates: RelationshipCandidate[] = [];\n\n  // For each unique pair of entities\n  for (let i = 0; i < entities.length; i++) {\n    for (let j = i + 1; j < entities.length; j++) {\n      const source = entities[i];\n      const target = entities[j];\n\n      // Filter self-relationships (same name AND same type)\n      if (source.name === target.name && source.type === target.type) {\n        continue;\n      }\n\n      // Find entity positions in text for proximity analysis\n      const sourcePos = text.toLowerCase().indexOf(source.name.toLowerCase());\n      const targetPos = text.toLowerCase().indexOf(target.name.toLowerCase());\n\n      // Both entities must appear in the text\n      if (sourcePos === -1 || targetPos === -1) continue;\n\n      // Get the context text surrounding both entities for signal analysis.\n      // Expand window by up to 50 chars before first entity and after last entity\n      // to capture context signals like \"Decided by @matt to use Tailwind CSS\"\n      // where \"Decided\" precedes both entities.\n      const minPos = Math.min(sourcePos, targetPos);\n      const maxPos = Math.max(\n        sourcePos + source.name.length,\n        targetPos + target.name.length,\n      );\n      const contextStart = Math.max(0, minPos - 50);\n      const contextEnd = Math.min(text.length, maxPos + 50);\n      const contextText = text.slice(contextStart, contextEnd);\n\n      // Determine relationship type\n      const pairKey = `${source.type}->${target.type}`;\n      let relationshipType = TYPE_PAIR_DEFAULTS[pairKey] ?? 'related_to';\n\n      // Check context signals in the text between entities\n      for (const signal of CONTEXT_SIGNALS) {\n        if (signal.pattern.test(contextText)) {\n          relationshipType = signal.type;\n          break;\n        }\n      }\n\n      // Special case: File->File with import/require language\n      if (source.type === 'File' && target.type === 'File') {\n        if (/\\b(?:imports?|requires?|from)\\b/i.test(contextText)) {\n          relationshipType = 'depends_on';\n        }\n      }\n\n      // Calculate base confidence\n      let confidence: number;\n      if (relationshipType === 'related_to' && !TYPE_PAIR_DEFAULTS[pairKey]) {\n        // Default fallback for unknown type pairs\n        confidence = 0.3;\n      } else {\n        confidence = 0.5;\n      }\n\n      // Proximity boost: entities within 50 characters of each other\n      const distance = Math.abs(sourcePos - targetPos);\n      if (distance <= 50) {\n        confidence += 0.1;\n      }\n\n      // Sentence co-occurrence boost: entities in the same sentence\n      if (areInSameSentence(text, sourcePos, targetPos)) {\n        confidence += 0.15;\n      }\n\n      // Cap at 1.0\n      confidence = Math.min(confidence, 1.0);\n\n      candidates.push({\n        sourceEntity: { name: source.name, type: source.type },\n        targetEntity: { name: target.name, type: target.type },\n        relationshipType,\n        confidence,\n        evidence: contextText.slice(0, 200), // Truncate evidence to 200 chars\n      });\n    }\n  }\n\n  return candidates;\n}\n\n// =============================================================================\n// Persistence\n// =============================================================================\n\n/**\n * Detects relationships, resolves entity names to node IDs, and persists edges.\n *\n * - Calls detectRelationships to find candidates\n * - Resolves each entity to a graph node via getNodeByNameAndType\n * - Inserts edges for candidates with confidence > 0.3\n * - Enforces max degree on affected nodes after insertion\n *\n * @returns Array of persisted GraphEdge objects\n */\nexport function detectAndPersist(\n  db: BetterSqlite3.Database,\n  text: string,\n  entities: Array<{ name: string; type: EntityType }>,\n): GraphEdge[] {\n  const candidates = detectRelationships(text, entities);\n  const persisted: GraphEdge[] = [];\n  const affectedNodeIds = new Set<string>();\n\n  const persist = db.transaction(() => {\n    for (const candidate of candidates) {\n      // Filter low-confidence candidates\n      if (candidate.confidence <= 0.3) continue;\n\n      // Resolve entity names to node IDs\n      const sourceNode = getNodeByNameAndType(\n        db,\n        candidate.sourceEntity.name,\n        candidate.sourceEntity.type,\n      );\n      const targetNode = getNodeByNameAndType(\n        db,\n        candidate.targetEntity.name,\n        candidate.targetEntity.type,\n      );\n\n      // Both nodes must exist in the graph\n      if (!sourceNode || !targetNode) continue;\n\n      try {\n        const edge = insertEdge(db, {\n          source_id: sourceNode.id,\n          target_id: targetNode.id,\n          type: candidate.relationshipType,\n          weight: candidate.confidence,\n          metadata: { evidence: candidate.evidence },\n        });\n        persisted.push(edge);\n        affectedNodeIds.add(sourceNode.id);\n        affectedNodeIds.add(targetNode.id);\n      } catch {\n        // Skip individual edge failures, continue with remaining\n        continue;\n      }\n    }\n\n    // Enforce max degree on all affected nodes\n    for (const nodeId of affectedNodeIds) {\n      enforceMaxDegree(db, nodeId);\n    }\n  });\n\n  persist();\n  return persisted;\n}\n\n// =============================================================================\n// Internal Helpers\n// =============================================================================\n\n/**\n * Checks if two positions in the text are within the same sentence.\n * A sentence boundary is defined by '.', '!', '?', or newline followed by\n * optional whitespace.\n */\nfunction areInSameSentence(\n  text: string,\n  pos1: number,\n  pos2: number,\n): boolean {\n  const start = Math.min(pos1, pos2);\n  const end = Math.max(pos1, pos2);\n  const between = text.slice(start, end);\n\n  // If there's a sentence boundary between the two positions, they're in different sentences\n  return !/[.!?\\n]/.test(between);\n}\n","/**\n * Observation merging for knowledge graph curation.\n *\n * Detects near-duplicate observations linked to the same entity using:\n *   - Cosine similarity on embeddings (threshold 0.95)\n *   - Jaccard similarity on tokenized words (threshold 0.85) as fallback\n *\n * Merging creates consolidated summaries, preserves audit trails via\n * soft-deletion, and computes mean embeddings for consolidated observations.\n *\n * Low-value pruning removes very short, unlinked, old, auto-captured\n * observations using conservative AND-logic (all criteria must match).\n */\n\nimport type BetterSqlite3 from 'better-sqlite3';\nimport { randomBytes } from 'node:crypto';\n\n// =============================================================================\n// Types\n// =============================================================================\n\n/**\n * A cluster of observations that are similar enough to merge.\n */\nexport interface MergeCluster {\n  entityId: string;\n  observations: Array<{\n    id: string;\n    text: string;\n    embedding: number[] | null;\n    created_at: string;\n  }>;\n  similarity: number;\n  suggestedSummary: string;\n}\n\ninterface ObsRow {\n  id: string;\n  content: string;\n  embedding: Buffer | null;\n  created_at: string;\n  source: string;\n  deleted_at: string | null;\n}\n\ninterface NodeRow {\n  id: string;\n  observation_ids: string;\n}\n\n// =============================================================================\n// Similarity Functions\n// =============================================================================\n\n/**\n * Computes cosine similarity between two number arrays.\n * Returns 0 for zero-length or zero-norm vectors.\n */\nfunction cosineSimilarity(a: number[], b: number[]): number {\n  if (a.length !== b.length || a.length === 0) return 0;\n\n  let dot = 0;\n  let normA = 0;\n  let normB = 0;\n\n  for (let i = 0; i < a.length; i++) {\n    dot += a[i] * b[i];\n    normA += a[i] * a[i];\n    normB += b[i] * b[i];\n  }\n\n  const denom = Math.sqrt(normA) * Math.sqrt(normB);\n  if (denom === 0) return 0;\n\n  return dot / denom;\n}\n\n/**\n * Computes Jaccard similarity between two texts based on tokenized words.\n * Words are lowercased and split on whitespace/punctuation.\n */\nfunction jaccardSimilarity(textA: string, textB: string): number {\n  const tokenize = (t: string): Set<string> =>\n    new Set(\n      t\n        .toLowerCase()\n        .split(/[\\s,.!?;:'\"()\\[\\]{}<>\\/\\\\|@#$%^&*+=~`]+/)\n        .filter((w) => w.length > 0),\n    );\n\n  const setA = tokenize(textA);\n  const setB = tokenize(textB);\n\n  if (setA.size === 0 && setB.size === 0) return 1;\n  if (setA.size === 0 || setB.size === 0) return 0;\n\n  let intersection = 0;\n  for (const w of setA) {\n    if (setB.has(w)) intersection++;\n  }\n\n  const union = setA.size + setB.size - intersection;\n  return union === 0 ? 0 : intersection / union;\n}\n\n/**\n * Converts a Buffer of Float32 values to a number array.\n */\nfunction bufferToNumbers(buf: Buffer): number[] {\n  const floats = new Float32Array(\n    buf.buffer,\n    buf.byteOffset,\n    buf.byteLength / 4,\n  );\n  return Array.from(floats);\n}\n\n// =============================================================================\n// Clustering\n// =============================================================================\n\n/**\n * Generates a consolidated summary from a cluster of observations.\n *\n * Strategy:\n *   1. Take the longest observation as the base\n *   2. Find unique keywords in shorter observations\n *   3. Append unique info in parentheses\n *   4. Prepend \"[Consolidated from N observations]\"\n */\nfunction generateSummary(\n  observations: Array<{ text: string }>,\n): string {\n  if (observations.length === 0) return '';\n  if (observations.length === 1) return observations[0].text;\n\n  // Find longest observation as base\n  const sorted = [...observations].sort(\n    (a, b) => b.text.length - a.text.length,\n  );\n  const base = sorted[0];\n  const baseWords = new Set(\n    base.text\n      .toLowerCase()\n      .split(/\\s+/)\n      .filter((w) => w.length > 2),\n  );\n\n  // Collect unique keywords from shorter observations\n  const uniqueKeywords: string[] = [];\n  for (let i = 1; i < sorted.length; i++) {\n    const words = sorted[i].text\n      .split(/\\s+/)\n      .filter((w) => w.length > 2);\n    for (const word of words) {\n      if (\n        !baseWords.has(word.toLowerCase()) &&\n        !uniqueKeywords.includes(word.toLowerCase())\n      ) {\n        uniqueKeywords.push(word.toLowerCase());\n      }\n    }\n  }\n\n  let summary = base.text;\n  if (uniqueKeywords.length > 0) {\n    const extras = uniqueKeywords.slice(0, 10).join(', ');\n    summary += ` (also: ${extras})`;\n  }\n\n  return `[Consolidated from ${observations.length} observations] ${summary}`;\n}\n\n/**\n * Finds clusters of similar observations for the same entity.\n *\n * For each entity with 3+ observations:\n *   1. Compute pairwise similarities (cosine on embeddings, Jaccard on text)\n *   2. Cluster observations where ALL pairwise similarities exceed threshold\n *   3. Generate suggested summaries for each cluster\n *\n * Only clusters with 2+ observations are returned, sorted by size DESC.\n *\n * @param db - better-sqlite3 Database handle\n * @param opts - threshold (default 0.95 cosine / 0.85 Jaccard), entityId filter\n * @returns Mergeable observation clusters sorted by size descending\n */\nexport function findMergeableClusters(\n  db: BetterSqlite3.Database,\n  opts?: { threshold?: number; entityId?: string },\n): MergeCluster[] {\n  const embeddingThreshold = opts?.threshold ?? 0.95;\n  const textThreshold = 0.85;\n\n  // Get entity nodes with 3+ observations\n  let nodes: NodeRow[];\n  if (opts?.entityId) {\n    const row = db\n      .prepare('SELECT id, observation_ids FROM graph_nodes WHERE id = ?')\n      .get(opts.entityId) as NodeRow | undefined;\n    nodes = row ? [row] : [];\n  } else {\n    nodes = db\n      .prepare('SELECT id, observation_ids FROM graph_nodes')\n      .all() as NodeRow[];\n  }\n\n  const clusters: MergeCluster[] = [];\n\n  for (const node of nodes) {\n    const obsIds = JSON.parse(node.observation_ids) as string[];\n    if (obsIds.length < 3) continue;\n\n    // Fetch observations (only non-deleted, non-merged)\n    const placeholders = obsIds.map(() => '?').join(', ');\n    const rows = db\n      .prepare(\n        `SELECT id, content, embedding, created_at, source, deleted_at\n         FROM observations\n         WHERE id IN (${placeholders}) AND deleted_at IS NULL`,\n      )\n      .all(...obsIds) as ObsRow[];\n\n    if (rows.length < 2) continue;\n\n    // Build observation objects with parsed embeddings\n    const observations = rows.map((r) => ({\n      id: r.id,\n      text: r.content,\n      embedding: r.embedding ? bufferToNumbers(r.embedding) : null,\n      created_at: r.created_at,\n    }));\n\n    // Find clusters using greedy algorithm\n    const used = new Set<string>();\n\n    for (let i = 0; i < observations.length; i++) {\n      if (used.has(observations[i].id)) continue;\n\n      const cluster = [observations[i]];\n      let totalSim = 0;\n      let pairCount = 0;\n\n      for (let j = i + 1; j < observations.length; j++) {\n        if (used.has(observations[j].id)) continue;\n\n        // Check if candidate is similar to ALL members of the current cluster\n        let allSimilar = true;\n        let candidateSim = 0;\n        let candidatePairs = 0;\n\n        for (const member of cluster) {\n          const sim = computeSimilarity(\n            member,\n            observations[j],\n            embeddingThreshold,\n            textThreshold,\n          );\n\n          if (sim === null) {\n            allSimilar = false;\n            break;\n          }\n\n          candidateSim += sim;\n          candidatePairs++;\n        }\n\n        if (allSimilar && candidatePairs > 0) {\n          cluster.push(observations[j]);\n          totalSim += candidateSim;\n          pairCount += candidatePairs;\n        }\n      }\n\n      if (cluster.length >= 2) {\n        // Mark all observations in this cluster as used\n        for (const obs of cluster) {\n          used.add(obs.id);\n        }\n\n        const avgSim = pairCount > 0 ? totalSim / pairCount : 0;\n\n        clusters.push({\n          entityId: node.id,\n          observations: cluster,\n          similarity: avgSim,\n          suggestedSummary: generateSummary(cluster),\n        });\n      }\n    }\n  }\n\n  // Sort by cluster size DESC (largest first)\n  clusters.sort((a, b) => b.observations.length - a.observations.length);\n\n  return clusters;\n}\n\n/**\n * Computes similarity between two observations.\n * Returns the similarity score if it exceeds the threshold, or null if not.\n */\nfunction computeSimilarity(\n  a: { text: string; embedding: number[] | null },\n  b: { text: string; embedding: number[] | null },\n  embeddingThreshold: number,\n  textThreshold: number,\n): number | null {\n  // Prefer cosine similarity on embeddings\n  if (a.embedding && b.embedding) {\n    const sim = cosineSimilarity(a.embedding, b.embedding);\n    return sim >= embeddingThreshold ? sim : null;\n  }\n\n  // Fallback to Jaccard similarity on text\n  const sim = jaccardSimilarity(a.text, b.text);\n  return sim >= textThreshold ? sim : null;\n}\n\n// =============================================================================\n// Merging\n// =============================================================================\n\n/**\n * Merges a cluster of similar observations into a consolidated observation.\n *\n * Steps:\n *   1. Create new consolidated observation with suggestedSummary text\n *   2. Store merge metadata (merged_from, merged_at, original_count)\n *   3. Update entity's observation_ids: remove old, add new merged ID\n *   4. Soft-delete originals (set deleted_at, do NOT hard delete)\n *   5. Compute mean embedding if originals have embeddings\n *\n * Runs in a transaction for atomicity.\n *\n * @param db - better-sqlite3 Database handle\n * @param cluster - The cluster to merge\n * @returns The new merged observation ID and removed IDs\n */\nexport function mergeObservationCluster(\n  db: BetterSqlite3.Database,\n  cluster: MergeCluster,\n): { mergedId: string; removedIds: string[] } {\n  const merge = db.transaction(() => {\n    const mergedId = randomBytes(16).toString('hex');\n    const now = new Date().toISOString();\n    const removedIds = cluster.observations.map((o) => o.id);\n\n    // Step 1 & 2: Create consolidated observation with merge metadata\n    const metadata = JSON.stringify({\n      merged_from: removedIds,\n      merged_at: now,\n      original_count: cluster.observations.length,\n    });\n\n    // Compute mean embedding if available\n    let meanEmbedding: Buffer | null = null;\n    const embeddingsWithValues = cluster.observations.filter(\n      (o) => o.embedding !== null,\n    );\n\n    if (embeddingsWithValues.length > 0) {\n      const dim = embeddingsWithValues[0].embedding!.length;\n      const mean = new Float32Array(dim);\n\n      for (const obs of embeddingsWithValues) {\n        const emb = obs.embedding!;\n        for (let i = 0; i < dim; i++) {\n          mean[i] += emb[i];\n        }\n      }\n\n      for (let i = 0; i < dim; i++) {\n        mean[i] /= embeddingsWithValues.length;\n      }\n\n      meanEmbedding = Buffer.from(mean.buffer);\n    }\n\n    // Get project_hash from the first original observation\n    const firstObs = db\n      .prepare('SELECT project_hash, source FROM observations WHERE id = ?')\n      .get(cluster.observations[0].id) as\n      | { project_hash: string; source: string }\n      | undefined;\n\n    const projectHash = firstObs?.project_hash ?? 'unknown';\n\n    db.prepare(\n      `INSERT INTO observations (id, project_hash, content, title, source, session_id, embedding, created_at, updated_at)\n       VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)`,\n    ).run(\n      mergedId,\n      projectHash,\n      cluster.suggestedSummary,\n      `[Merged] ${metadata}`,\n      'curation:merge',\n      null,\n      meanEmbedding,\n      now,\n      now,\n    );\n\n    // Step 3: Update entity's observation_ids\n    const nodeRow = db\n      .prepare('SELECT observation_ids FROM graph_nodes WHERE id = ?')\n      .get(cluster.entityId) as { observation_ids: string } | undefined;\n\n    if (nodeRow) {\n      const currentIds = JSON.parse(nodeRow.observation_ids) as string[];\n      const removedSet = new Set(removedIds);\n      const updatedIds = currentIds.filter((id) => !removedSet.has(id));\n      updatedIds.push(mergedId);\n\n      db.prepare(\n        `UPDATE graph_nodes SET observation_ids = ?, updated_at = datetime('now') WHERE id = ?`,\n      ).run(JSON.stringify(updatedIds), cluster.entityId);\n    }\n\n    // Step 4: Soft-delete original observations\n    const softDeleteStmt = db.prepare(\n      `UPDATE observations SET deleted_at = ? WHERE id = ?`,\n    );\n    for (const obsId of removedIds) {\n      softDeleteStmt.run(now, obsId);\n    }\n\n    return { mergedId, removedIds };\n  });\n\n  return merge();\n}\n\n// =============================================================================\n// Low-Value Pruning\n// =============================================================================\n\n/**\n * Prunes low-value observations using conservative AND-logic.\n *\n * An observation is pruned ONLY if ALL of:\n *   a. Very short (< minTextLength characters, default 20)\n *   b. No linked entities (not in any graph_node's observation_ids)\n *   c. Older than maxAge days (default 90)\n *   d. Auto-captured (source is NOT 'mcp:save_memory' or 'slash:remember')\n *   e. Not already deleted\n *\n * Pruning is soft-delete only -- sets deleted_at, never hard deletes.\n *\n * @param db - better-sqlite3 Database handle\n * @param opts - Configurable thresholds\n * @returns Count of pruned observations\n */\nexport function pruneLowValue(\n  db: BetterSqlite3.Database,\n  opts?: { minTextLength?: number; maxAge?: number },\n): { pruned: number } {\n  const minTextLength = opts?.minTextLength ?? 20;\n  const maxAgeDays = opts?.maxAge ?? 90;\n\n  const now = new Date();\n  const cutoffDate = new Date(\n    now.getTime() - maxAgeDays * 24 * 60 * 60 * 1000,\n  );\n  const cutoffISO = cutoffDate.toISOString();\n\n  // Find candidate observations: short, old, auto-captured, not deleted\n  const candidates = db\n    .prepare(\n      `SELECT id, content, source, created_at\n       FROM observations\n       WHERE deleted_at IS NULL\n         AND LENGTH(content) < ?\n         AND created_at < ?\n         AND source NOT IN ('mcp:save_memory', 'slash:remember')`,\n    )\n    .all(minTextLength, cutoffISO) as Array<{\n    id: string;\n    content: string;\n    source: string;\n    created_at: string;\n  }>;\n\n  if (candidates.length === 0) return { pruned: 0 };\n\n  // Check which candidates have NO linked entities\n  // Build a set of all observation IDs referenced by any graph node\n  const allNodeObsIds = new Set<string>();\n  const nodes = db\n    .prepare('SELECT observation_ids FROM graph_nodes')\n    .all() as Array<{ observation_ids: string }>;\n\n  for (const node of nodes) {\n    const ids = JSON.parse(node.observation_ids) as string[];\n    for (const id of ids) {\n      allNodeObsIds.add(id);\n    }\n  }\n\n  // Filter: only prune candidates that are NOT linked to any entity\n  const toPrune = candidates.filter((c) => !allNodeObsIds.has(c.id));\n\n  if (toPrune.length === 0) return { pruned: 0 };\n\n  // Soft-delete\n  const nowISO = now.toISOString();\n  const softDeleteStmt = db.prepare(\n    'UPDATE observations SET deleted_at = ? WHERE id = ?',\n  );\n\n  const prune = db.transaction(() => {\n    for (const obs of toPrune) {\n      softDeleteStmt.run(nowISO, obs.id);\n    }\n    return toPrune.length;\n  });\n\n  const pruned = prune();\n\n  return { pruned };\n}\n","/**\n * Background curation agent for knowledge graph maintenance.\n *\n * Runs during quiet periods (session end, long pauses) to keep the\n * knowledge base high-quality as it grows. Performs:\n *   1. Observation merging (near-duplicate consolidation)\n *   2. Entity deduplication (case-insensitive, abbreviation, path)\n *   3. Graph constraint enforcement (approaching degree cap)\n *   4. Staleness sweep (contradiction flagging)\n *   5. Low-value pruning (short + unlinked + old + auto-captured)\n *\n * Each step is isolated -- one failure does not stop others.\n * The agent is idempotent: running twice produces the same result.\n * Curation NEVER crashes the main process.\n */\n\nimport type BetterSqlite3 from 'better-sqlite3';\n\nimport {\n  findMergeableClusters,\n  mergeObservationCluster,\n  pruneLowValue,\n} from './observation-merger.js';\nimport {\n  findDuplicateEntities,\n  mergeEntities,\n  enforceMaxDegree,\n} from './constraints.js';\nimport { countEdgesForNode } from './schema.js';\nimport {\n  detectStaleness,\n  flagStaleObservation,\n  initStalenessSchema,\n} from './staleness.js';\nimport { MAX_NODE_DEGREE } from './types.js';\n\n// =============================================================================\n// Types\n// =============================================================================\n\n/**\n * Report of a completed curation cycle.\n */\nexport interface CurationReport {\n  startedAt: string;\n  completedAt: string;\n  observationsMerged: number;\n  entitiesDeduplicated: number;\n  stalenessFlagsAdded: number;\n  lowValuePruned: number;\n  errors: string[];\n}\n\n// =============================================================================\n// Standalone Curation Function\n// =============================================================================\n\n/**\n * Runs a single curation cycle on the knowledge graph.\n *\n * Executes five steps in order:\n *   1. Merge similar observations\n *   2. Deduplicate entities\n *   3. Enforce graph constraints (approaching degree cap)\n *   4. Staleness sweep\n *   5. Low-value pruning\n *\n * Each step is wrapped in try/catch -- if one fails, the rest continue.\n * Returns a CurationReport documenting all actions taken.\n *\n * This function is idempotent: running it twice in a row produces the\n * same result (merged observations do not re-merge, already-flagged\n * stale observations do not get re-flagged, etc.)\n *\n * @param db - better-sqlite3 Database handle\n * @returns CurationReport with counts and any errors\n */\nexport async function runCuration(\n  db: BetterSqlite3.Database,\n): Promise<CurationReport> {\n  const startedAt = new Date().toISOString();\n  const errors: string[] = [];\n  let observationsMerged = 0;\n  let entitiesDeduplicated = 0;\n  let stalenessFlagsAdded = 0;\n  let lowValuePruned = 0;\n\n  // Ensure staleness schema exists\n  try {\n    initStalenessSchema(db);\n  } catch (err) {\n    errors.push(`Schema init: ${err instanceof Error ? err.message : String(err)}`);\n  }\n\n  // -----------------------------------------------------------------------\n  // Step 1: Merge similar observations\n  // -----------------------------------------------------------------------\n  try {\n    const clusters = findMergeableClusters(db);\n    for (const cluster of clusters) {\n      try {\n        const result = mergeObservationCluster(db, cluster);\n        observationsMerged += result.removedIds.length;\n      } catch (err) {\n        errors.push(\n          `Merge cluster (entity ${cluster.entityId}): ${err instanceof Error ? err.message : String(err)}`,\n        );\n      }\n    }\n  } catch (err) {\n    errors.push(\n      `Step 1 (merge): ${err instanceof Error ? err.message : String(err)}`,\n    );\n  }\n\n  // -----------------------------------------------------------------------\n  // Step 2: Deduplicate entities\n  // -----------------------------------------------------------------------\n  try {\n    const duplicates = findDuplicateEntities(db);\n    for (const group of duplicates) {\n      if (group.entities.length < 2) continue;\n\n      // Keep the entity with more observation_ids\n      const sorted = [...group.entities].sort(\n        (a, b) => b.observation_ids.length - a.observation_ids.length,\n      );\n      const keepId = sorted[0].id;\n\n      for (let i = 1; i < sorted.length; i++) {\n        try {\n          mergeEntities(db, keepId, sorted[i].id);\n          entitiesDeduplicated++;\n        } catch (err) {\n          errors.push(\n            `Dedup (${sorted[0].name} <- ${sorted[i].name}): ${err instanceof Error ? err.message : String(err)}`,\n          );\n        }\n      }\n    }\n  } catch (err) {\n    errors.push(\n      `Step 2 (dedup): ${err instanceof Error ? err.message : String(err)}`,\n    );\n  }\n\n  // -----------------------------------------------------------------------\n  // Step 3: Enforce graph constraints (nodes approaching degree cap)\n  // -----------------------------------------------------------------------\n  try {\n    const threshold = Math.floor(MAX_NODE_DEGREE * 0.9);\n    const nodeRows = db\n      .prepare('SELECT id FROM graph_nodes')\n      .all() as Array<{ id: string }>;\n\n    for (const row of nodeRows) {\n      try {\n        const degree = countEdgesForNode(db, row.id);\n        if (degree > threshold) {\n          enforceMaxDegree(db, row.id);\n        }\n      } catch (err) {\n        errors.push(\n          `Constraint (node ${row.id}): ${err instanceof Error ? err.message : String(err)}`,\n        );\n      }\n    }\n  } catch (err) {\n    errors.push(\n      `Step 3 (constraints): ${err instanceof Error ? err.message : String(err)}`,\n    );\n  }\n\n  // -----------------------------------------------------------------------\n  // Step 4: Staleness sweep\n  // -----------------------------------------------------------------------\n  try {\n    // Check recently updated entities for contradictions\n    const recentNodes = db\n      .prepare(\n        `SELECT id FROM graph_nodes WHERE updated_at >= datetime('now', '-24 hours')`,\n      )\n      .all() as Array<{ id: string }>;\n\n    // Get already-flagged observation IDs to avoid re-flagging\n    const existingFlags = new Set<string>();\n    try {\n      const flagRows = db\n        .prepare('SELECT observation_id FROM staleness_flags WHERE resolved = 0')\n        .all() as Array<{ observation_id: string }>;\n      for (const row of flagRows) {\n        existingFlags.add(row.observation_id);\n      }\n    } catch {\n      // Table might not exist yet -- that's fine\n    }\n\n    for (const node of recentNodes) {\n      try {\n        const reports = detectStaleness(db, node.id);\n        for (const report of reports) {\n          // Only flag if not already flagged (idempotency)\n          if (!existingFlags.has(report.olderObservation.id)) {\n            flagStaleObservation(db, report.olderObservation.id, report.reason);\n            existingFlags.add(report.olderObservation.id);\n            stalenessFlagsAdded++;\n          }\n        }\n      } catch (err) {\n        errors.push(\n          `Staleness (node ${node.id}): ${err instanceof Error ? err.message : String(err)}`,\n        );\n      }\n    }\n  } catch (err) {\n    errors.push(\n      `Step 4 (staleness): ${err instanceof Error ? err.message : String(err)}`,\n    );\n  }\n\n  // -----------------------------------------------------------------------\n  // Step 5: Low-value pruning\n  // -----------------------------------------------------------------------\n  try {\n    const result = pruneLowValue(db);\n    lowValuePruned = result.pruned;\n  } catch (err) {\n    errors.push(\n      `Step 5 (prune): ${err instanceof Error ? err.message : String(err)}`,\n    );\n  }\n\n  const completedAt = new Date().toISOString();\n\n  const report: CurationReport = {\n    startedAt,\n    completedAt,\n    observationsMerged,\n    entitiesDeduplicated,\n    stalenessFlagsAdded,\n    lowValuePruned,\n    errors,\n  };\n\n  process.stderr.write(\n    `[laminark:curation] Cycle complete: ${observationsMerged} merged, ${entitiesDeduplicated} deduped, ${stalenessFlagsAdded} flagged stale, ${lowValuePruned} pruned\\n`,\n  );\n\n  return report;\n}\n\n// =============================================================================\n// CurationAgent Class\n// =============================================================================\n\n/**\n * Background curation agent that runs periodically or on-demand.\n *\n * Manages scheduling, lifecycle, and reporting. Uses the standalone\n * runCuration() function for the actual curation logic.\n */\nexport class CurationAgent {\n  private db: BetterSqlite3.Database;\n  private intervalMs: number;\n  private onComplete?: (report: CurationReport) => void;\n  private running: boolean = false;\n  private lastRun: string | null = null;\n  private timer: ReturnType<typeof setInterval> | null = null;\n\n  constructor(\n    db: BetterSqlite3.Database,\n    opts?: {\n      intervalMs?: number;\n      onComplete?: (report: CurationReport) => void;\n    },\n  ) {\n    this.db = db;\n    this.intervalMs = opts?.intervalMs ?? 300_000; // 5 minutes default\n    this.onComplete = opts?.onComplete;\n  }\n\n  /**\n   * Start periodic curation on setInterval.\n   */\n  start(): void {\n    if (this.running) return;\n\n    this.running = true;\n    this.timer = setInterval(() => {\n      void this.runOnce();\n    }, this.intervalMs);\n\n    process.stderr.write(\n      `[laminark:curation] Agent started, interval: ${this.intervalMs}ms\\n`,\n    );\n  }\n\n  /**\n   * Stop the periodic curation timer.\n   */\n  stop(): void {\n    if (this.timer) {\n      clearInterval(this.timer);\n      this.timer = null;\n    }\n    this.running = false;\n\n    process.stderr.write('[laminark:curation] Agent stopped\\n');\n  }\n\n  /**\n   * Execute one curation cycle. This is the main entry point.\n   */\n  async runOnce(): Promise<CurationReport> {\n    const report = await runCuration(this.db);\n    this.lastRun = report.completedAt;\n\n    if (this.onComplete) {\n      this.onComplete(report);\n    }\n\n    return report;\n  }\n\n  /**\n   * Whether the agent is currently running.\n   */\n  isRunning(): boolean {\n    return this.running;\n  }\n\n  /**\n   * Timestamp of the last completed curation run.\n   */\n  getLastRun(): string | null {\n    return this.lastRun;\n  }\n}\n\n// =============================================================================\n// Integration Trigger Functions\n// =============================================================================\n\n/**\n * Triggered at session end. Runs a targeted curation cycle\n * focusing on the current session's observations only (faster\n * than a full sweep).\n *\n * Note: Currently runs the full cycle since targeted per-session\n * filtering would require session_id awareness in all curation steps.\n * The full cycle is fast enough for session-end triggers.\n */\nexport async function onSessionEnd(\n  db: BetterSqlite3.Database,\n): Promise<CurationReport> {\n  return runCuration(db);\n}\n\n/**\n * Triggered when no activity detected for 5+ minutes.\n * Runs the full curation cycle.\n */\nexport async function onQuietPeriod(\n  db: BetterSqlite3.Database,\n): Promise<CurationReport> {\n  return runCuration(db);\n}\n","/**\n * Server-Sent Events endpoint for live updates.\n *\n * Maintains a set of connected SSE clients and provides a broadcast\n * function for pushing real-time events to all connected browsers.\n * Includes a ring buffer for event replay on reconnection via\n * Last-Event-ID header support.\n *\n * Supported event types:\n *   - connected: initial handshake\n *   - heartbeat: keepalive ping (every 30s)\n *   - new_observation: new observation stored\n *   - topic_shift: topic shift detected\n *   - entity_updated: graph entity created/modified\n *   - session_start: new session started\n *   - session_end: session ended\n *\n * @module web/routes/sse\n */\n\nimport { Hono } from 'hono';\nimport type { Context } from 'hono';\nimport { debug } from '../../shared/debug.js';\n\n// ---------------------------------------------------------------------------\n// Client management\n// ---------------------------------------------------------------------------\n\ninterface SSEClient {\n  id: string;\n  controller: ReadableStreamDefaultController;\n  heartbeatTimer: ReturnType<typeof setInterval>;\n}\n\nconst clients = new Set<SSEClient>();\n\nlet clientIdCounter = 0;\n\n// ---------------------------------------------------------------------------\n// Event ID counter and ring buffer for replay\n// ---------------------------------------------------------------------------\n\nlet lastEventId = 0;\n\ninterface BufferedEvent {\n  id: number;\n  event: string;\n  data: string;\n}\n\nconst RING_BUFFER_SIZE = 100;\nconst eventRingBuffer: BufferedEvent[] = [];\n\n/**\n * Adds an event to the ring buffer, evicting the oldest if full.\n */\nfunction pushToRingBuffer(entry: BufferedEvent): void {\n  if (eventRingBuffer.length >= RING_BUFFER_SIZE) {\n    eventRingBuffer.shift();\n  }\n  eventRingBuffer.push(entry);\n}\n\n/**\n * Returns all events with id > sinceId from the ring buffer.\n */\nfunction getEventsSince(sinceId: number): BufferedEvent[] {\n  return eventRingBuffer.filter((e) => e.id > sinceId);\n}\n\n// ---------------------------------------------------------------------------\n// SSE formatting helpers\n// ---------------------------------------------------------------------------\n\nfunction formatSSE(event: string, data: string, id?: number): string {\n  let msg = '';\n  if (id !== undefined) {\n    msg += `id: ${id}\\n`;\n  }\n  msg += `event: ${event}\\ndata: ${data}\\n\\n`;\n  return msg;\n}\n\nfunction sendToClient(client: SSEClient, event: string, data: string, id?: number): boolean {\n  try {\n    const message = formatSSE(event, data, id);\n    client.controller.enqueue(new TextEncoder().encode(message));\n    return true;\n  } catch {\n    // Client disconnected or stream errored\n    return false;\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Route group\n// ---------------------------------------------------------------------------\n\nexport const sseRoutes = new Hono();\n\n/**\n * GET /api/sse\n *\n * Server-Sent Events endpoint. Keeps the connection alive with heartbeats\n * and receives broadcast events for live UI updates.\n *\n * Supports Last-Event-ID header for replay of missed events on reconnection.\n */\nsseRoutes.get('/sse', (c: Context) => {\n  const clientId = String(++clientIdCounter);\n  const lastEventIdHeader = c.req.header('Last-Event-ID');\n  const replayFromId = lastEventIdHeader ? parseInt(lastEventIdHeader, 10) : 0;\n\n  let client: SSEClient;\n\n  const stream = new ReadableStream({\n    start(controller) {\n      // Create client with heartbeat\n      const heartbeatTimer = setInterval(() => {\n        const ok = sendToClient(client, 'heartbeat', JSON.stringify({ timestamp: Date.now() }));\n        if (!ok) {\n          clearInterval(heartbeatTimer);\n          clients.delete(client);\n          debug('db', 'SSE client heartbeat failed, removed', { clientId });\n        }\n      }, 30_000);\n\n      client = { id: clientId, controller, heartbeatTimer };\n      clients.add(client);\n\n      debug('db', 'SSE client connected', { clientId, total: clients.size });\n\n      // Send initial connected event\n      sendToClient(client, 'connected', JSON.stringify({\n        timestamp: Date.now(),\n        clientId,\n      }));\n\n      // Replay missed events if client is reconnecting with Last-Event-ID\n      if (replayFromId > 0) {\n        const missed = getEventsSince(replayFromId);\n        for (const entry of missed) {\n          sendToClient(client, entry.event, entry.data, entry.id);\n        }\n        if (missed.length > 0) {\n          debug('db', 'SSE replayed missed events', { clientId, count: missed.length, sinceId: replayFromId });\n        }\n      }\n    },\n    cancel() {\n      // Client disconnected\n      if (client) {\n        clearInterval(client.heartbeatTimer);\n        clients.delete(client);\n        debug('db', 'SSE client disconnected', { clientId, total: clients.size });\n      }\n    },\n  });\n\n  return new Response(stream, {\n    headers: {\n      'Content-Type': 'text/event-stream',\n      'Cache-Control': 'no-cache',\n      'Connection': 'keep-alive',\n      'X-Accel-Buffering': 'no', // Disable nginx buffering if proxied\n    },\n  });\n});\n\n// ---------------------------------------------------------------------------\n// Broadcast\n// ---------------------------------------------------------------------------\n\n/**\n * Broadcasts an event to all connected SSE clients.\n *\n * Each broadcast increments a monotonic event ID that is included in the\n * SSE `id:` field. Events are stored in an in-memory ring buffer (last 100)\n * so reconnecting clients can replay missed events via Last-Event-ID.\n *\n * Automatically removes disconnected clients that fail to receive\n * the message.\n *\n * @param event - Event name (e.g., 'new_observation', 'topic_shift')\n * @param data - Data object to serialize as JSON\n */\nexport function broadcast(event: string, data: object): void {\n  const eventId = ++lastEventId;\n  const json = JSON.stringify(data);\n\n  // Store in ring buffer for replay\n  pushToRingBuffer({ id: eventId, event, data: json });\n\n  if (clients.size === 0) return;\n\n  const dead: SSEClient[] = [];\n\n  for (const client of clients) {\n    const ok = sendToClient(client, event, json, eventId);\n    if (!ok) {\n      dead.push(client);\n    }\n  }\n\n  // Clean up dead clients\n  for (const client of dead) {\n    clearInterval(client.heartbeatTimer);\n    clients.delete(client);\n  }\n\n  if (dead.length > 0) {\n    debug('db', 'SSE broadcast cleaned dead clients', { dead: dead.length, remaining: clients.size });\n  }\n}\n\n/**\n * Returns the current number of connected SSE clients.\n * Useful for health/stats endpoints.\n */\nexport function getClientCount(): number {\n  return clients.size;\n}\n","/**\n * REST API routes for the Laminark visualization.\n *\n * Provides endpoints for graph data, timeline data, and individual node\n * details. All endpoints read from the better-sqlite3 database instance\n * set on the Hono context by the server middleware.\n *\n * @module web/routes/api\n */\n\nimport { Hono } from 'hono';\nimport type BetterSqlite3 from 'better-sqlite3';\n\n// ---------------------------------------------------------------------------\n// Raw row interfaces for SQL results\n// ---------------------------------------------------------------------------\n\ninterface GraphNodeRow {\n  id: string;\n  name: string;\n  type: string;\n  observation_ids: string; // JSON array\n  created_at: string;\n}\n\ninterface GraphEdgeRow {\n  id: string;\n  source_id: string;\n  target_id: string;\n  type: string;\n  name?: string; // joined from graph_nodes for label\n  weight: number;\n}\n\ninterface SessionRow {\n  id: string;\n  started_at: string;\n  ended_at: string | null;\n  summary: string | null;\n}\n\ninterface ObservationRow {\n  id: string;\n  content: string;\n  title: string | null;\n  source: string;\n  created_at: string;\n  session_id: string | null;\n}\n\ninterface ShiftDecisionRow {\n  id: string;\n  session_id: string;\n  distance: number;\n  threshold: number;\n  confidence: number | null;\n  created_at: string;\n}\n\n// ---------------------------------------------------------------------------\n// Helper: get db from Hono context\n// ---------------------------------------------------------------------------\n\nfunction getDb(c: { get: (key: string) => unknown }): BetterSqlite3.Database {\n  return c.get('db') as BetterSqlite3.Database;\n}\n\n// ---------------------------------------------------------------------------\n// Route group\n// ---------------------------------------------------------------------------\n\nexport const apiRoutes = new Hono();\n\n/**\n * GET /api/graph\n *\n * Returns the knowledge graph as JSON with nodes and edges arrays.\n * Accepts optional query params:\n *   ?type=File,Decision  - comma-separated entity types to include\n *   ?since=ISO8601       - only entities created after this timestamp\n */\napiRoutes.get('/graph', (c) => {\n  const db = getDb(c);\n  const typeFilter = c.req.query('type');\n  const sinceFilter = c.req.query('since');\n  const untilFilter = c.req.query('until');\n\n  // Build nodes query\n  let nodesSql = 'SELECT id, name, type, observation_ids, created_at FROM graph_nodes';\n  const nodeParams: unknown[] = [];\n  const nodeConditions: string[] = [];\n\n  if (typeFilter) {\n    const types = typeFilter.split(',').map(t => t.trim()).filter(Boolean);\n    if (types.length > 0) {\n      nodeConditions.push(`type IN (${types.map(() => '?').join(', ')})`);\n      nodeParams.push(...types);\n    }\n  }\n\n  if (sinceFilter) {\n    nodeConditions.push('created_at >= ?');\n    nodeParams.push(sinceFilter);\n  }\n\n  if (untilFilter) {\n    nodeConditions.push('created_at <= ?');\n    nodeParams.push(untilFilter);\n  }\n\n  if (nodeConditions.length > 0) {\n    nodesSql += ' WHERE ' + nodeConditions.join(' AND ');\n  }\n\n  nodesSql += ' ORDER BY created_at DESC';\n\n  let nodeRows: GraphNodeRow[];\n  try {\n    nodeRows = db.prepare(nodesSql).all(...nodeParams) as GraphNodeRow[];\n  } catch {\n    nodeRows = [];\n  }\n\n  const nodes = nodeRows.map(row => ({\n    id: row.id,\n    label: row.name,\n    type: row.type,\n    observationCount: safeParseJsonArray(row.observation_ids).length,\n    createdAt: row.created_at,\n  }));\n\n  // Build edges query -- only include edges where both nodes are in the result set\n  let edgeRows: GraphEdgeRow[];\n  try {\n    const edgesSql = `\n      SELECT e.id, e.source_id, e.target_id, e.type, e.weight,\n             tn.name AS name\n      FROM graph_edges e\n      LEFT JOIN graph_nodes tn ON tn.id = e.target_id\n      ORDER BY e.created_at DESC\n    `;\n    edgeRows = db.prepare(edgesSql).all() as GraphEdgeRow[];\n  } catch {\n    edgeRows = [];\n  }\n\n  // If type filtering is active, only include edges between included nodes\n  const nodeIdSet = new Set(nodes.map(n => n.id));\n  const filteredEdges = typeFilter\n    ? edgeRows.filter(e => nodeIdSet.has(e.source_id) && nodeIdSet.has(e.target_id))\n    : edgeRows;\n\n  const edges = filteredEdges.map(row => ({\n    id: row.id,\n    source: row.source_id,\n    target: row.target_id,\n    type: row.type,\n    label: row.name ?? row.type,\n  }));\n\n  return c.json({ nodes, edges });\n});\n\n/**\n * GET /api/timeline\n *\n * Returns timeline data: sessions, observations, and topic shifts.\n * Accepts optional query params:\n *   ?from=ISO8601  - start of time range\n *   ?to=ISO8601    - end of time range\n *   ?limit=N       - max observations (default 500)\n */\napiRoutes.get('/timeline', (c) => {\n  const db = getDb(c);\n  const from = c.req.query('from');\n  const to = c.req.query('to');\n  const limitStr = c.req.query('limit');\n  const limit = limitStr ? Math.min(parseInt(limitStr, 10) || 500, 2000) : 500;\n  const offsetStr = c.req.query('offset');\n  const offset = offsetStr ? Math.max(parseInt(offsetStr, 10) || 0, 0) : 0;\n\n  // Sessions\n  let sessions: Array<{ id: string; startedAt: string; endedAt: string | null; observationCount: number; summary: string | null }> = [];\n  try {\n    let sessionsSql = 'SELECT id, started_at, ended_at, summary FROM sessions';\n    const sessionParams: unknown[] = [];\n    const sessionConds: string[] = [];\n\n    if (from) {\n      sessionConds.push('started_at >= ?');\n      sessionParams.push(from);\n    }\n    if (to) {\n      sessionConds.push('(ended_at IS NULL OR ended_at <= ?)');\n      sessionParams.push(to);\n    }\n\n    if (sessionConds.length > 0) {\n      sessionsSql += ' WHERE ' + sessionConds.join(' AND ');\n    }\n    sessionsSql += ' ORDER BY started_at DESC LIMIT 50 OFFSET ?';\n    sessionParams.push(offset);\n\n    const sessionRows = db.prepare(sessionsSql).all(...sessionParams) as SessionRow[];\n\n    // Count observations per session\n    const countStmt = db.prepare(\n      'SELECT COUNT(*) AS cnt FROM observations WHERE session_id = ? AND deleted_at IS NULL'\n    );\n\n    sessions = sessionRows.map(row => {\n      let obsCount = 0;\n      try {\n        const countRow = countStmt.get(row.id) as { cnt: number } | undefined;\n        obsCount = countRow?.cnt ?? 0;\n      } catch { /* empty */ }\n\n      return {\n        id: row.id,\n        startedAt: row.started_at,\n        endedAt: row.ended_at,\n        observationCount: obsCount,\n        summary: row.summary,\n      };\n    });\n  } catch { /* tables may not exist yet */ }\n\n  // Observations\n  let observations: Array<{ id: string; text: string; createdAt: string; sessionId: string | null; type: string }> = [];\n  try {\n    let obsSql = 'SELECT id, content, title, source, created_at, session_id FROM observations WHERE deleted_at IS NULL';\n    const obsParams: unknown[] = [];\n\n    if (from) {\n      obsSql += ' AND created_at >= ?';\n      obsParams.push(from);\n    }\n    if (to) {\n      obsSql += ' AND created_at <= ?';\n      obsParams.push(to);\n    }\n\n    obsSql += ' ORDER BY created_at DESC LIMIT ? OFFSET ?';\n    obsParams.push(limit);\n    obsParams.push(offset);\n\n    const obsRows = db.prepare(obsSql).all(...obsParams) as ObservationRow[];\n\n    observations = obsRows.map(row => ({\n      id: row.id,\n      text: row.title ? `${row.title}: ${row.content}` : row.content,\n      createdAt: row.created_at,\n      sessionId: row.session_id,\n      type: row.source,\n    }));\n  } catch { /* table may not exist yet */ }\n\n  // Topic shifts\n  let topicShifts: Array<{ id: string; fromTopic: string | null; toTopic: string | null; timestamp: string; confidence: number | null }> = [];\n  try {\n    let shiftSql = 'SELECT id, session_id, distance, threshold, confidence, created_at FROM shift_decisions WHERE shifted = 1';\n    const shiftParams: unknown[] = [];\n\n    if (from) {\n      shiftSql += ' AND created_at >= ?';\n      shiftParams.push(from);\n    }\n    if (to) {\n      shiftSql += ' AND created_at <= ?';\n      shiftParams.push(to);\n    }\n\n    shiftSql += ' ORDER BY created_at DESC LIMIT 100';\n\n    const shiftRows = db.prepare(shiftSql).all(...shiftParams) as ShiftDecisionRow[];\n\n    topicShifts = shiftRows.map(row => ({\n      id: row.id,\n      fromTopic: null, // shift_decisions doesn't store topic labels directly\n      toTopic: null,\n      timestamp: row.created_at,\n      confidence: row.confidence,\n    }));\n  } catch { /* table may not exist yet */ }\n\n  return c.json({ sessions, observations, topicShifts });\n});\n\n/**\n * GET /api/node/:id\n *\n * Returns details for a single entity node including its observations\n * and relationships. Powers the detail panel.\n */\napiRoutes.get('/node/:id', (c) => {\n  const db = getDb(c);\n  const nodeId = c.req.param('id');\n\n  // Get the entity node\n  interface FullNodeRow {\n    id: string;\n    name: string;\n    type: string;\n    observation_ids: string;\n    metadata: string;\n    created_at: string;\n    updated_at: string;\n  }\n\n  let nodeRow: FullNodeRow | undefined;\n  try {\n    nodeRow = db.prepare(\n      'SELECT id, name, type, observation_ids, metadata, created_at, updated_at FROM graph_nodes WHERE id = ?'\n    ).get(nodeId) as FullNodeRow | undefined;\n  } catch { /* table may not exist */ }\n\n  if (!nodeRow) {\n    return c.json({ error: 'Node not found' }, 404);\n  }\n\n  const entity = {\n    id: nodeRow.id,\n    label: nodeRow.name,\n    type: nodeRow.type,\n    createdAt: nodeRow.created_at,\n    updatedAt: nodeRow.updated_at,\n    metadata: safeParseJson(nodeRow.metadata),\n  };\n\n  // Get observations for this entity\n  const observationIds = safeParseJsonArray(nodeRow.observation_ids);\n  let nodeObservations: Array<{ id: string; text: string; createdAt: string }> = [];\n\n  if (observationIds.length > 0) {\n    try {\n      const placeholders = observationIds.map(() => '?').join(', ');\n      const obsRows = db.prepare(\n        `SELECT id, content, title, created_at FROM observations WHERE id IN (${placeholders}) AND deleted_at IS NULL ORDER BY created_at DESC`\n      ).all(...observationIds) as Array<{ id: string; content: string; title: string | null; created_at: string }>;\n\n      nodeObservations = obsRows.map(row => ({\n        id: row.id,\n        text: row.title ? `${row.title}: ${row.content}` : row.content,\n        createdAt: row.created_at,\n      }));\n    } catch { /* table may not exist */ }\n  }\n\n  // Get relationships\n  interface RelRow {\n    id: string;\n    source_id: string;\n    target_id: string;\n    type: string;\n    weight: number;\n    target_name: string | null;\n    target_type: string | null;\n    source_name: string | null;\n    source_type: string | null;\n  }\n\n  let relationships: Array<{ id: string; targetId: string; targetLabel: string; type: string; direction: string }> = [];\n  try {\n    const relRows = db.prepare(`\n      SELECT\n        e.id, e.source_id, e.target_id, e.type, e.weight,\n        tn.name AS target_name, tn.type AS target_type,\n        sn.name AS source_name, sn.type AS source_type\n      FROM graph_edges e\n      LEFT JOIN graph_nodes tn ON tn.id = e.target_id\n      LEFT JOIN graph_nodes sn ON sn.id = e.source_id\n      WHERE e.source_id = ? OR e.target_id = ?\n      ORDER BY e.weight DESC\n    `).all(nodeId, nodeId) as RelRow[];\n\n    relationships = relRows.map(row => {\n      const isSource = row.source_id === nodeId;\n      return {\n        id: row.id,\n        targetId: isSource ? row.target_id : row.source_id,\n        targetLabel: isSource ? (row.target_name ?? row.target_id) : (row.source_name ?? row.source_id),\n        type: row.type,\n        direction: isSource ? 'outgoing' : 'incoming',\n      };\n    });\n  } catch { /* table may not exist */ }\n\n  return c.json({ entity, observations: nodeObservations, relationships });\n});\n\n// ---------------------------------------------------------------------------\n// Helpers\n// ---------------------------------------------------------------------------\n\nfunction safeParseJsonArray(json: string): string[] {\n  try {\n    const parsed = JSON.parse(json);\n    return Array.isArray(parsed) ? parsed : [];\n  } catch {\n    return [];\n  }\n}\n\nfunction safeParseJson(json: string): Record<string, unknown> {\n  try {\n    return JSON.parse(json) as Record<string, unknown>;\n  } catch {\n    return {};\n  }\n}\n","/**\n * Hono web server for the Laminark visualization UI.\n *\n * Serves static assets from the ui/ directory and registers REST API\n * and SSE route groups. Configured with CORS for localhost development\n * and a health check endpoint.\n *\n * @module web/server\n */\n\nimport { Hono } from 'hono';\nimport { cors } from 'hono/cors';\nimport { serve } from '@hono/node-server';\nimport { serveStatic } from '@hono/node-server/serve-static';\nimport type BetterSqlite3 from 'better-sqlite3';\n\nimport { debug } from '../shared/debug.js';\nimport { apiRoutes } from './routes/api.js';\nimport { sseRoutes } from './routes/sse.js';\n\n/**\n * Creates a configured Hono app with middleware, static serving,\n * and route registration.\n *\n * @param db - better-sqlite3 Database instance for API queries\n * @returns Configured Hono app\n */\nexport function createWebServer(db: BetterSqlite3.Database): Hono {\n  const app = new Hono();\n\n  // CORS middleware for localhost development\n  app.use(\n    '*',\n    cors({\n      origin: (origin) => {\n        if (!origin) return '*';\n        if (origin.startsWith('http://localhost:') || origin.startsWith('http://127.0.0.1:')) {\n          return origin;\n        }\n        return null as unknown as string;\n      },\n    }),\n  );\n\n  // Make db available to all route handlers via Hono context\n  app.use('*', async (c, next) => {\n    c.set('db', db);\n    await next();\n  });\n\n  // Health check endpoint\n  app.get('/api/health', (c) => {\n    return c.json({ status: 'ok', timestamp: Date.now() });\n  });\n\n  // Mount API and SSE routes\n  app.route('/api', apiRoutes);\n  app.route('/api', sseRoutes);\n\n  // Serve static files from ui/ directory\n  app.use(\n    '/*',\n    serveStatic({\n      root: './ui/',\n    }),\n  );\n\n  // Fallback: serve index.html for SPA routing\n  app.get('*', serveStatic({ root: './ui/', path: 'index.html' }));\n\n  return app;\n}\n\n/**\n * Starts the Hono web server on the specified port.\n *\n * @param app - Configured Hono app from createWebServer()\n * @param port - Port number (default: 37820)\n * @returns The Node.js HTTP server instance\n */\nexport function startWebServer(\n  app: Hono,\n  port: number = 37820,\n): ReturnType<typeof serve> {\n  debug('db', `Starting web server on port ${port}`);\n\n  const server = serve({\n    fetch: app.fetch,\n    port,\n  });\n\n  debug('db', `Web server listening on http://localhost:${port}`);\n\n  return server;\n}\n","#!/usr/bin/env node\n\n// Laminark MCP server entry point\n// Re-export storage API for library consumers\nexport * from './storage/index.js';\n\nimport { openDatabase } from './storage/database.js';\nimport { getDatabaseConfig, getProjectHash } from './shared/config.js';\nimport { debug } from './shared/debug.js';\nimport { createServer, startServer } from './mcp/server.js';\nimport { registerRecall } from './mcp/tools/recall.js';\nimport { registerSaveMemory } from './mcp/tools/save-memory.js';\nimport { registerTopicContext } from './mcp/tools/topic-context.js';\nimport { registerQueryGraph } from './mcp/tools/query-graph.js';\nimport { registerGraphStats } from './mcp/tools/graph-stats.js';\nimport { AnalysisWorker } from './analysis/worker-bridge.js';\nimport { EmbeddingStore } from './storage/embeddings.js';\nimport { ObservationRepository } from './storage/observations.js';\nimport { TopicShiftHandler } from './hooks/topic-shift-handler.js';\nimport { TopicShiftDetector } from './intelligence/topic-detector.js';\nimport { AdaptiveThresholdManager } from './intelligence/adaptive-threshold.js';\nimport { TopicShiftDecisionLogger } from './intelligence/decision-logger.js';\nimport { loadTopicDetectionConfig, applyConfig } from './config/topic-detection-config.js';\nimport { StashManager } from './storage/stash-manager.js';\nimport { ThresholdStore } from './storage/threshold-store.js';\nimport { NotificationStore } from './storage/notifications.js';\nimport { initGraphSchema } from './graph/schema.js';\nimport { extractAndPersist } from './graph/entity-extractor.js';\nimport { detectAndPersist } from './graph/relationship-detector.js';\nimport { CurationAgent } from './graph/curation-agent.js';\nimport { broadcast } from './web/routes/sse.js';\nimport { createWebServer, startWebServer } from './web/server.js';\n\nconst db = openDatabase(getDatabaseConfig());\ninitGraphSchema(db.db);\nconst projectHash = getProjectHash(process.cwd());\n\n// ---------------------------------------------------------------------------\n// Worker thread and embedding store (graceful degradation)\n// ---------------------------------------------------------------------------\n\nconst embeddingStore = db.hasVectorSupport\n  ? new EmbeddingStore(db.db, projectHash)\n  : null;\n\nconst worker = new AnalysisWorker();\n\n// Start worker in background -- do NOT await during server startup (DQ-04)\nconst workerReady = worker.start().catch(() => {\n  debug('mcp', 'Worker failed to start, keyword-only mode');\n});\n\n// Suppress unhandled rejection from workerReady (already handled above)\nvoid workerReady;\n\n// ---------------------------------------------------------------------------\n// Topic shift detection (runs in background embedding loop)\n// ---------------------------------------------------------------------------\n\nconst topicConfig = loadTopicDetectionConfig();\nconst detector = new TopicShiftDetector();\nconst adaptiveManager = new AdaptiveThresholdManager({\n  sensitivityMultiplier: topicConfig.sensitivityMultiplier,\n  alpha: topicConfig.ewmaAlpha,\n});\napplyConfig(topicConfig, detector, adaptiveManager);\n\n// Seed adaptive threshold from history (cold start handling)\nconst thresholdStore = new ThresholdStore(db.db);\nconst historicalSeed = thresholdStore.loadHistoricalSeed(projectHash);\nif (historicalSeed) {\n  adaptiveManager.seedFromHistory(historicalSeed.averageDistance, historicalSeed.averageVariance);\n  applyConfig(topicConfig, detector, adaptiveManager);\n}\n\nconst stashManager = new StashManager(db.db);\nconst decisionLogger = new TopicShiftDecisionLogger(db.db);\nconst notificationStore = new NotificationStore(db.db);\nconst obsRepoForTopicDetection = new ObservationRepository(db.db, projectHash);\n\nconst topicShiftHandler = new TopicShiftHandler({\n  detector,\n  stashManager,\n  observationStore: obsRepoForTopicDetection,\n  config: topicConfig,\n  decisionLogger,\n  adaptiveManager,\n});\n\n// ---------------------------------------------------------------------------\n// Background embedding loop\n// ---------------------------------------------------------------------------\n\nasync function processUnembedded(): Promise<void> {\n  if (!embeddingStore || !worker.isReady()) return;\n\n  const ids = embeddingStore.findUnembedded(10);\n  if (ids.length === 0) return;\n\n  const obsRepo = new ObservationRepository(db.db, projectHash);\n\n  for (const id of ids) {\n    const obs = obsRepo.getById(id);\n    if (!obs) continue;\n\n    const text = obs.title ? `${obs.title}\\n${obs.content}` : obs.content;\n    const embedding = await worker.embed(text);\n\n    if (embedding) {\n      embeddingStore.store(id, embedding);\n      obsRepo.update(id, {\n        embeddingModel: worker.getEngineName(),\n        embeddingVersion: '1',\n      });\n\n      // Broadcast new observation to SSE clients (minimal payload)\n      const truncatedText = obs.content.length > 120\n        ? obs.content.substring(0, 120) + '...'\n        : obs.content;\n      broadcast('new_observation', {\n        id,\n        text: truncatedText,\n        sessionId: obs.sessionId ?? null,\n        createdAt: obs.createdAt,\n      });\n\n      // Topic shift detection -- evaluate the newly embedded observation\n      if (topicConfig.enabled) {\n        try {\n          // Build the observation with its newly generated embedding\n          const obsWithEmbedding = { ...obs, embedding };\n          const result = await topicShiftHandler.handleObservation(\n            obsWithEmbedding,\n            obs.sessionId ?? 'unknown',\n            projectHash,\n          );\n          if (result.stashed && result.notification) {\n            notificationStore.add(projectHash, result.notification);\n            debug('embed', 'Topic shift detected, notification queued', { id });\n\n            // Broadcast topic shift to SSE clients\n            broadcast('topic_shift', {\n              id: result.notification.substring(0, 32),\n              fromTopic: null,\n              toTopic: null,\n              timestamp: new Date().toISOString(),\n              confidence: null,\n            });\n          }\n        } catch (topicErr) {\n          const msg = topicErr instanceof Error ? topicErr.message : String(topicErr);\n          debug('embed', 'Topic shift detection error (non-fatal)', { error: msg });\n        }\n      }\n\n      // Knowledge graph -- extract entities and detect relationships\n      try {\n        const nodes = extractAndPersist(db.db, text, String(id));\n        if (nodes.length > 0) {\n          const entityPairs = nodes.map(n => ({\n            name: n.name,\n            type: n.type,\n          }));\n          detectAndPersist(db.db, text, entityPairs);\n          debug('embed', 'Graph updated', {\n            id,\n            entities: nodes.length,\n          });\n\n          // Broadcast entity updates to SSE clients\n          for (const node of nodes) {\n            broadcast('entity_updated', {\n              id: node.name,\n              label: node.name,\n              type: node.type,\n              observationCount: 1,\n              createdAt: new Date().toISOString(),\n            });\n          }\n        }\n      } catch (graphErr) {\n        const msg = graphErr instanceof Error ? graphErr.message : String(graphErr);\n        debug('embed', 'Graph extraction error (non-fatal)', { error: msg });\n      }\n    }\n  }\n}\n\n// Process unembedded observations every 5 seconds\nconst embedTimer = setInterval(() => {\n  processUnembedded().catch((err) => {\n    const message = err instanceof Error ? err.message : String(err);\n    debug('embed', 'Background embedding error', { error: message });\n  });\n}, 5000);\n\n// ---------------------------------------------------------------------------\n// MCP server setup\n// ---------------------------------------------------------------------------\n\nconst server = createServer();\nregisterSaveMemory(server, db.db, projectHash, notificationStore);\nregisterRecall(server, db.db, projectHash, worker, embeddingStore, notificationStore);\nregisterTopicContext(server, db.db, projectHash, notificationStore);\nregisterQueryGraph(server, db.db, projectHash, notificationStore);\nregisterGraphStats(server, db.db, projectHash, notificationStore);\n\nstartServer(server).catch((err) => {\n  debug('mcp', 'Fatal: failed to start server', { error: err.message });\n  clearInterval(embedTimer);\n  db.close();\n  process.exit(1);\n});\n\n// ---------------------------------------------------------------------------\n// Web visualization server (runs alongside MCP server)\n// ---------------------------------------------------------------------------\n\nconst webPort = parseInt(process.env.LAMINARK_WEB_PORT || '37820', 10);\nconst webApp = createWebServer(db.db);\nstartWebServer(webApp, webPort);\n\n// ---------------------------------------------------------------------------\n// Background curation agent (graph maintenance)\n// ---------------------------------------------------------------------------\n\nconst curationAgent = new CurationAgent(db.db, {\n  intervalMs: 5 * 60 * 1000, // 5 minutes\n  onComplete: (report) => {\n    debug('db', 'Curation complete', {\n      merged: report.observationsMerged,\n      deduped: report.entitiesDeduplicated,\n      stale: report.stalenessFlagsAdded,\n      pruned: report.lowValuePruned,\n    });\n  },\n});\ncurationAgent.start();\n\n// ---------------------------------------------------------------------------\n// Shutdown handlers\n// ---------------------------------------------------------------------------\n\nprocess.on('SIGINT', () => {\n  clearInterval(embedTimer);\n  curationAgent.stop();\n  worker.shutdown().catch(() => {});\n  db.close();\n  process.exit(0);\n});\nprocess.on('SIGTERM', () => {\n  clearInterval(embedTimer);\n  curationAgent.stop();\n  worker.shutdown().catch(() => {});\n  db.close();\n  process.exit(0);\n});\nprocess.on('uncaughtException', (err) => {\n  debug('mcp', 'Uncaught exception', { error: err.message });\n  clearInterval(embedTimer);\n  curationAgent.stop();\n  worker.shutdown().catch(() => {});\n  db.close();\n  process.exit(1);\n});\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;AAeA,IAAa,eAAb,MAA0B;CACxB,AAAiB;CACjB,AAAiB;CAEjB,YAAY,IAA4B,aAAqB;AAC3D,OAAK,KAAK;AACV,OAAK,cAAc;;;;;;;;;;;;CAarB,cACE,OACA,SACgB;EAChB,MAAM,YAAY,KAAK,cAAc,MAAM;AAC3C,MAAI,CAAC,UACH,QAAO,EAAE;EAGX,MAAM,QAAQ,SAAS,SAAS;EAEhC,IAAI,MAAM;;;;;;;;;;;EAWV,MAAM,SAAoB,CAAC,WAAW,KAAK,YAAY;AAEvD,MAAI,SAAS,WAAW;AACtB,UAAO;AACP,UAAO,KAAK,QAAQ,UAAU;;AAGhC,SAAO;AACP,SAAO,KAAK,MAAM;EAElB,MAAM,UAAU,WAAW,UAAU,6BAA6B;AAMhE,UALa,KAAK,GAAG,QAAQ,IAAI,CAAC,IAAI,GAAG,OAAO,CAKpC,KAAK,SAAS;IACxB,aAAa,iBAAiB,IAAI;IAClC,OAAO,KAAK,IAAI,IAAI,KAAK;IACzB,WAAW;IACX,SAAS,IAAI;IACd,EAAE;IACH;AAEF,QAAM,UAAU,4BAA4B;GAAE,OAAO;GAAW,aAAa,QAAQ;GAAQ,CAAC;AAE9F,SAAO;;;;;;CAOT,eAAe,QAAgB,OAAgC;EAC7D,MAAM,QAAQ,OAAO,MAAM,CAAC,MAAM,MAAM,CAAC,OAAO,QAAQ;AACxD,MAAI,MAAM,WAAW,EACnB,QAAO,EAAE;EAIX,MAAM,iBAAiB,MACpB,KAAK,MAAM,KAAK,aAAa,EAAE,CAAC,CAChC,OAAO,QAAQ;AAClB,MAAI,eAAe,WAAW,EAC5B,QAAO,EAAE;EAGX,MAAM,WAAW,eAAe,KAAK,MAAM,GAAG,EAAE,GAAG,CAAC,KAAK,IAAI;EAC7D,MAAM,iBAAiB,SAAS;EAEhC,MAAM,MAAM;;;;;;;;;;;;;EAcZ,MAAM,UAAU,WAAW,UAAU,4BAA4B;AAQ/D,UAPa,KAAK,GACf,QAAQ,IAAI,CACZ,IAAI,UAAU,KAAK,aAAa,eAAe,CAKtC,KAAK,SAAS;IACxB,aAAa,iBAAiB,IAAI;IAClC,OAAO,KAAK,IAAI,IAAI,KAAK;IACzB,WAAW;IACX,SAAS,IAAI;IACd,EAAE;IACH;AAEF,QAAM,UAAU,2BAA2B;GAAE;GAAQ,aAAa,QAAQ;GAAQ,CAAC;AAEnF,SAAO;;;;;CAMT,eAAqB;AACnB,QAAM,UAAU,wBAAwB;AACxC,OAAK,GAAG,KACN,mEACD;;;;;;;CAQH,AAAQ,cAAc,OAA8B;EAClD,MAAM,QAAQ,MAAM,MAAM,CAAC,MAAM,MAAM,CAAC,OAAO,QAAQ;AACvD,MAAI,MAAM,WAAW,EACnB,QAAO;EAGT,MAAM,iBAAiB,MACpB,KAAK,MAAM,KAAK,aAAa,EAAE,CAAC,CAChC,OAAO,QAAQ;AAElB,MAAI,eAAe,WAAW,EAC5B,QAAO;AAIT,SAAO,eAAe,KAAK,IAAI;;;;;;CAOjC,AAAQ,aAAa,MAAsB;EAEzC,IAAI,UAAU,KAAK,QAAQ,iBAAiB,GAAG;AAG/C,MAAI,uBAAuB,KAAK,QAAQ,CACtC,QAAO;AAIT,YAAU,QAAQ,QAAQ,YAAY,GAAG;AAEzC,SAAO;;;;;;;;;;;;ACnKX,IAAa,iBAAb,MAA4B;CAC1B,AAAQ;CACR,AAAQ;CACR,AAAQ;CACR,AAAQ;CACR,AAAQ;CAER,YACE,AAAQ,IACR,AAAQ,aACR;EAFQ;EACA;AAER,OAAK,aAAa,GAAG,QACnB,yFACD;AAED,OAAK,aAAa,GAAG,QAAQ;;;;;;;;;MAS3B;AAEF,OAAK,aAAa,GAAG,QACnB,8DACD;AAED,OAAK,aAAa,GAAG,QACnB,gEACD;AAED,OAAK,qBAAqB,GAAG,QAAQ;;;;;;MAMnC;;;;;;;;CASJ,MAAM,eAAuB,WAA+B;AAC1D,MAAI;AACF,QAAK,WAAW,IAAI,eAAe,UAAU;AAC7C,SAAM,SAAS,oBAAoB;IAAE;IAAe,YAAY,UAAU;IAAQ,CAAC;WAC5E,KAAK;AACZ,SAAM,SAAS,6BAA6B;IAC1C;IACA,OAAO,OAAO,IAAI;IACnB,CAAC;;;;;;;;;;CAWN,OAAO,gBAA8B,QAAQ,IAA6B;AACxE,MAAI;GACF,MAAM,OAAO,KAAK,WAAW,IAC3B,gBACA,KAAK,aACL,MACD;AAED,SAAM,SAAS,oBAAoB;IACjC,SAAS,KAAK;IACd;IACD,CAAC;AAEF,UAAO,KAAK,KAAK,SAAS;IACxB,eAAe,IAAI;IACnB,UAAU,IAAI;IACf,EAAE;WACI,KAAK;AACZ,SAAM,SAAS,iBAAiB,EAAE,OAAO,OAAO,IAAI,EAAE,CAAC;AACvD,UAAO,EAAE;;;;;;CAOb,OAAO,eAA6B;AAClC,MAAI;AACF,QAAK,WAAW,IAAI,cAAc;AAClC,SAAM,SAAS,qBAAqB,EAAE,eAAe,CAAC;WAC/C,KAAK;AACZ,SAAM,SAAS,8BAA8B;IAC3C;IACA,OAAO,OAAO,IAAI;IACnB,CAAC;;;;;;CAON,IAAI,eAAgC;AAClC,MAAI;AAEF,UADY,KAAK,WAAW,IAAI,cAAc,KAC/B;WACR,KAAK;AACZ,SAAM,SAAS,uCAAuC;IACpD;IACA,OAAO,OAAO,IAAI;IACnB,CAAC;AACF,UAAO;;;;;;;;;CAUX,eAAe,QAAQ,IAAc;AACnC,MAAI;GACF,MAAM,OAAO,KAAK,mBAAmB,IACnC,KAAK,aACL,MACD;AAED,SAAM,SAAS,iCAAiC;IAAE,OAAO,KAAK;IAAQ;IAAO,CAAC;AAE9E,UAAO,KAAK,KAAK,QAAQ,IAAI,GAAG;WACzB,KAAK;AACZ,SAAM,SAAS,0CAA0C,EACvD,OAAO,OAAO,IAAI,EACnB,CAAC;AACF,UAAO,EAAE;;;;;;;;;;;;AC1If,SAAS,WAAW,KAA6B;AAC/C,QAAO;EACL,IAAI,IAAI;EACR,WAAW,IAAI;EACf,WAAW,IAAI;EACf,YAAY,IAAI;EAChB,SAAS,IAAI;EACb,gBAAgB,KAAK,MAAM,IAAI,gBAAgB;EAC/C,sBAAsB,KAAK,MACzB,IAAI,sBACL;EACD,WAAW,IAAI;EACf,WAAW,IAAI;EACf,QAAQ,IAAI;EACb;;;;;;;;;;;;AAaH,IAAa,eAAb,MAA0B;CACxB,AAAiB;CAGjB,AAAiB;CACjB,AAAiB;CACjB,AAAiB;CACjB,AAAiB;CAEjB,YAAY,IAA4B;AACtC,OAAK,KAAK;AAEV,OAAK,aAAa,GAAG,QAAQ;;;MAG3B;AAEF,OAAK,cAAc,GAAG,QAAQ;;MAE5B;AAEF,OAAK,aAAa,GAAG,QAAQ;;;;MAI3B;AAEF,OAAK,aAAa,GAAG,QAAQ;;MAE3B;AAEF,QAAM,MAAM,2BAA2B;;;;;;;CAQzC,YAAY,OAAuC;EACjD,MAAM,KAAK,YAAY,GAAG,CAAC,SAAS,MAAM;EAC1C,MAAM,iBAAiB,MAAM,aAAa,KAAK,MAAM,EAAE,GAAG;EAC1D,MAAM,gBAAgB,KAAK,UAAU,MAAM,aAAa;EACxD,MAAM,UAAU,KAAK,UAAU,eAAe;AAE9C,QAAM,MAAM,kBAAkB;GAC5B,YAAY,MAAM;GAClB,kBAAkB,MAAM,aAAa;GACtC,CAAC;AAEF,OAAK,WAAW,IACd,IACA,MAAM,WACN,MAAM,WACN,MAAM,YACN,MAAM,SACN,eACA,QACD;EAED,MAAM,MAAM,KAAK,YAAY,IAAI,GAAG;AACpC,MAAI,CAAC,IACH,OAAM,IAAI,MAAM,yCAAyC;AAG3D,QAAM,MAAM,iBAAiB,EAAE,IAAI,CAAC;AAEpC,SAAO,WAAW,IAAI;;;;;;CAOxB,YACE,WACA,SACgB;EAChB,MAAM,QAAQ,SAAS,SAAS;EAEhC,IAAI,MACF;EACF,MAAM,SAAoB,CAAC,UAAU;AAErC,MAAI,SAAS,WAAW;AACtB,UAAO;AACP,UAAO,KAAK,QAAQ,UAAU;;AAGhC,MAAI,SAAS,QAAQ;AACnB,UAAO;AACP,UAAO,KAAK,QAAQ,OAAO;;AAG7B,SAAO;AACP,SAAO,KAAK,MAAM;AAElB,QAAM,MAAM,mBAAmB;GAAE;GAAW,GAAG;GAAS,CAAC;AAGzD,SADa,KAAK,GAAG,QAAQ,IAAI,CAAC,IAAI,GAAG,OAAO,CACpC,IAAI,WAAW;;;;;;CAO7B,SAAS,IAAiC;EACxC,MAAM,MAAM,KAAK,YAAY,IAAI,GAAG;AACpC,SAAO,MAAM,WAAW,IAAI,GAAG;;;;;;;CAQjC,YAAY,IAA0B;AAGpC,MAFe,KAAK,WAAW,IAAI,GAAG,CAE3B,YAAY,EACrB,OAAM,IAAI,MAAM,oBAAoB,KAAK;AAG3C,QAAM,MAAM,iBAAiB,EAAE,IAAI,CAAC;EAEpC,MAAM,MAAM,KAAK,YAAY,IAAI,GAAG;AACpC,MAAI,CAAC,IACH,OAAM,IAAI,MAAM,qCAAqC,KAAK;AAG5D,SAAO,WAAW,IAAI;;;;;CAMxB,YAAY,IAAkB;AAC5B,OAAK,WAAW,IAAI,GAAG;AACvB,QAAM,MAAM,iBAAiB,EAAE,IAAI,CAAC;;;;;;CAOtC,iBAAiB,WAAmB,OAAgC;AAClE,SAAO,KAAK,YAAY,WAAW;GACjC,QAAQ;GACR,OAAO,SAAS;GACjB,CAAC;;;;;;;;;;;;;;;;;ACtLN,IAAa,iBAAb,MAA4B;CAC1B,AAAiB;CAGjB,AAAiB;CACjB,AAAiB;CAEjB,YAAY,IAA4B;AACtC,OAAK,KAAK;AAEV,OAAK,aAAa,GAAG,QAAQ;;;MAG3B;AAEF,OAAK,eAAe,GAAG,QAAQ;;;;;;;;;;;MAW7B;AAEF,QAAM,MAAM,6BAA6B;;;;;CAM3C,qBACE,WACA,WACA,OACM;AACN,OAAK,WAAW,IACd,WACA,WACA,MAAM,cACN,MAAM,cACN,MAAM,iBACP;AAED,QAAM,MAAM,mBAAmB;GAC7B;GACA;GACA,cAAc,MAAM;GACpB,cAAc,MAAM;GACrB,CAAC;;;;;;;CAQJ,mBAAmB,WAA0C;EAC3D,MAAM,MAAM,KAAK,aAAa,IAAI,UAAU;AAK5C,MAAI,IAAI,iBAAiB,QAAQ,IAAI,iBAAiB,MAAM;AAC1D,SAAM,MAAM,8BAA8B,EAAE,WAAW,CAAC;AACxD,UAAO;;AAGT,QAAM,MAAM,yBAAyB;GACnC;GACA,aAAa,IAAI;GACjB,aAAa,IAAI;GAClB,CAAC;AAEF,SAAO;GACL,iBAAiB,IAAI;GACrB,iBAAiB,IAAI;GACtB;;;;;;ACrGL,SAAgB,eAA0B;AACxC,QAAO,IAAI,UACT;EAAE,MAAM;EAAY,SAAS;EAAS,EACtC,EAAE,cAAc,EAAE,OAAO,EAAE,EAAE,EAAE,CAChC;;AAGH,eAAsB,YAAY,QAAkC;CAClE,MAAM,YAAY,IAAI,sBAAsB;AAC5C,OAAM,OAAO,QAAQ,UAAU;AAC/B,OAAM,OAAO,wCAAwC;;;;;;;;;;;;;;;;;;;;;;;;ACwCvD,SAAgB,qBACd,aACA,IAAI,IACW;CACf,MAAM,yBAAS,IAAI,KAAqB;AAExC,MAAK,MAAM,QAAQ,YACjB,MAAK,IAAI,OAAO,GAAG,OAAO,KAAK,QAAQ,QAAQ;EAC7C,MAAM,OAAO,KAAK;EAClB,MAAM,UAAU,OAAO,IAAI,KAAK,GAAG,IAAI;AACvC,SAAO,IAAI,KAAK,IAAI,UAAU,KAAK,IAAI,OAAO,GAAG;;CAIrD,MAAM,UAAyB,EAAE;AACjC,MAAK,MAAM,CAAC,IAAI,eAAe,OAC7B,SAAQ,KAAK;EAAE;EAAI;EAAY,CAAC;AAGlC,SAAQ,MAAM,GAAG,MAAM,EAAE,aAAa,EAAE,WAAW;AACnD,QAAO;;;;;;;;;;;;AAiBT,eAAsB,aACpB,QACyB;CACzB,MAAM,EAAE,cAAc,gBAAgB,QAAQ,OAAO,IAAI,aAAa,YAAY;CAClF,MAAM,QAAQ,SAAS,SAAS;AAEhC,QAAO,WAAW,UAAU,iBAAiB,YAAY;EAEvD,MAAM,iBAAiB,aAAa,cAAc,OAAO;GACvD;GACA,WAAW,SAAS;GACrB,CAAC;AAEF,QAAM,UAAU,mBAAmB,EAAE,OAAO,eAAe,QAAQ,CAAC;EAGpE,IAAI,gBAAyC,EAAE;AAE/C,MAAI,UAAU,OAAO,SAAS,EAAE;GAC9B,MAAM,iBAAiB,MAAM,OAAO,MAAM,MAAM;AAEhD,OAAI,gBAAgB;AAElB,oBAAgB,eAAe,OAAO,gBAAgB,QAAQ,EAAE;AAChE,UAAM,UAAU,kBAAkB,EAAE,OAAO,cAAc,QAAQ,CAAC;SAElE,OAAM,UAAU,uCAAuC;QAGzD,OAAM,UAAU,iCAAiC;AAInD,MAAI,cAAc,WAAW,GAAG;AAC9B,SAAM,UAAU,kCAAkC,EAAE,OAAO,eAAe,QAAQ,CAAC;AACnF,UAAO;;EAYT,MAAM,QAAQ,qBAAqB,CARC,eAAe,KAAK,OAAO,EAC7D,IAAI,EAAE,YAAY,IACnB,EAAE,EAEgC,cAAc,KAAK,OAAO,EAC3D,IAAI,EAAE,eACP,EAAE,CAE6D,CAAC;EAGjE,MAAM,6BAAa,IAAI,KAA2B;AAClD,OAAK,MAAM,KAAK,eACd,YAAW,IAAI,EAAE,YAAY,IAAI,EAAE;EAGrC,MAAM,cAAc,IAAI,IAAI,cAAc,KAAK,MAAM,EAAE,cAAc,CAAC;EAGtE,MAAM,UAAU,IAAI,sBAAsB,IAAI,YAAY;EAG1D,MAAM,SAAyB,EAAE;AAEjC,OAAK,MAAM,QAAQ,OAAO;AACxB,OAAI,OAAO,UAAU,MAAO;GAE5B,MAAM,cAAc,WAAW,IAAI,KAAK,GAAG;GAC3C,MAAM,aAAa,YAAY,IAAI,KAAK,GAAG;AAE3C,OAAI,eAAe,WAEjB,QAAO,KAAK;IACV,aAAa,YAAY;IACzB,OAAO,KAAK;IACZ,WAAW;IACX,SAAS,YAAY;IACtB,CAAC;YACO,YAET,QAAO,KAAK;IACV,aAAa,YAAY;IACzB,OAAO,KAAK;IACZ,WAAW;IACX,SAAS,YAAY;IACtB,CAAC;YACO,YAAY;IAErB,MAAM,MAAM,QAAQ,QAAQ,KAAK,GAAG;AACpC,QAAI,KAAK;KACP,MAAM,WAAW,IAAI,WAAW,IAAI,QAAQ,OAAO,IAAI,CAAC,MAAM,GAAG,IAAI;AACrE,YAAO,KAAK;MACV,aAAa;MACb,OAAO,KAAK;MACZ,WAAW;MACX;MACD,CAAC;;;;AAKR,QAAM,UAAU,0BAA0B;GACxC,SAAS,eAAe;GACxB,QAAQ,cAAc;GACtB,OAAO,OAAO;GACd,QAAQ,OAAO,QAAQ,MAAM,EAAE,cAAc,SAAS,CAAC;GACxD,CAAC;AAEF,SAAO;GACP;;;;;ACxMJ,MAAa,eAAe;AAC5B,MAAa,mBAAmB;AAEhC,SAAgB,eAAe,MAAsB;AACnD,QAAO,KAAK,KAAK,KAAK,SAAS,EAAE;;AAGnC,SAAgB,mBACd,SACA,cACA,SAAiB,cAC0C;CAE3D,MAAM,kBAAkB,SADC;CAEzB,IAAI,cAAc;CAClB,MAAM,QAAa,EAAE;AAErB,MAAK,MAAM,UAAU,SAAS;EAE5B,MAAM,SAAS,eADG,aAAa,OAAO,CACE;AACxC,MAAI,cAAc,SAAS,mBAAmB,MAAM,SAAS,EAC3D,QAAO;GAAE;GAAO,WAAW;GAAM,eAAe;GAAa;AAE/D,QAAM,KAAK,OAAO;AAClB,iBAAe;;AAGjB,QAAO;EAAE;EAAO,WAAW;EAAO,eAAe;EAAa;;;;;ACJhE,SAAS,QAAQ,IAAoB;AACnC,QAAO,GAAG,MAAM,GAAG,EAAE;;AAGvB,SAAS,QAAQ,KAAqB;AACpC,QAAO,IAAI,MAAM,GAAG,GAAG;;AAGzB,SAAS,QAAQ,KAAqB;AACpC,QAAO,IAAI,MAAM,IAAI,GAAG;;AAG1B,SAAS,YAAY,SAAiB,QAAwB;AAC5D,QAAO,QAAQ,QAAQ,OAAO,IAAI,CAAC,MAAM,GAAG,OAAO;;AAOrD,SAAS,kBACP,KACA,OACA,OACQ;AAMR,QAAO,IAAI,MAAM,IALD,QAAQ,IAAI,GAAG,CAKF,KAJf,IAAI,SAAS,WAIa,KAHvB,UAAU,SAAY,MAAM,QAAQ,EAAE,GAAG,IAGJ,KAFtC,YAAY,IAAI,SAAS,IAAI,CAEsB,KADtD,QAAQ,IAAI,UAAU;;AAIrC,SAAS,oBACP,MACA,OACQ;CACR,MAAM,QAAQ,CAAC,MAAM,OAAO;AAC5B,MAAK,MAAM,EAAE,SAAS,OAAO;EAC3B,MAAM,OAAO,QAAQ,IAAI,UAAU;EACnC,MAAM,QAAQ,IAAI,SAAS;EAC3B,MAAM,SAAS,IAAI;EACnB,MAAM,UAAU,YAAY,IAAI,SAAS,IAAI;AAC7C,QAAM,KAAK,GAAG,KAAK,KAAK,MAAM,KAAK,OAAO,KAAK,UAAU;;AAE3D,QAAO,MAAM,KAAK,KAAK;;AAGzB,SAAS,eAAe,KAA0B;AAGhD,QAAO,OAFS,QAAQ,IAAI,GAAG,CAET,KADR,IAAI,SAAS,WACM,KAAK,IAAI,UAAU,QAAQ,IAAI;;AAOlE,SAASA,uBACP,mBACA,aACA,cACQ;AACR,KAAI,CAAC,kBAAmB,QAAO;CAC/B,MAAM,UAAU,kBAAkB,eAAe,YAAY;AAC7D,KAAI,QAAQ,WAAW,EAAG,QAAO;AAEjC,QADe,QAAQ,KAAI,MAAK,cAAc,EAAE,UAAU,CAAC,KAAK,KAAK,GACrD,SAAS;;AAG3B,SAASC,eAAa,MAAc;AAClC,QAAO,EAAE,SAAS,CAAC;EAAE,MAAM;EAAiB;EAAM,CAAC,EAAE;;AAGvD,SAASC,gBAAc,MAAc;AACnC,QAAO;EAAE,SAAS,CAAC;GAAE,MAAM;GAAiB;GAAM,CAAC;EAAE,SAAS;EAAM;;AAOtE,SAAgB,eACd,QACA,IACA,aACA,SAAgC,MAChC,iBAAwC,MACxC,oBAA8C,MACxC;AACN,QAAO,aACL,UACA;EACE,OAAO;EACP,aACE;EACF,aAAa;GACX,OAAO,EACJ,QAAQ,CACR,UAAU,CACV,SAAS,4BAA4B;GACxC,IAAI,EAAE,QAAQ,CAAC,UAAU,CAAC,SAAS,kCAAkC;GACrE,OAAO,EACJ,QAAQ,CACR,UAAU,CACV,SAAS,kCAAkC;GAC9C,QAAQ,EACL,KAAK;IAAC;IAAQ;IAAS;IAAU,CAAC,CAClC,QAAQ,OAAO,CACf,SACC,2FACD;GACH,KAAK,EACF,MAAM,EAAE,QAAQ,CAAC,CACjB,UAAU,CACV,SACC,qEACD;GACH,QAAQ,EACL,KAAK;IAAC;IAAW;IAAY;IAAO,CAAC,CACrC,QAAQ,UAAU,CAClB,SACC,sGACD;GACH,OAAO,EACJ,QAAQ,CACR,KAAK,CACL,IAAI,EAAE,CACN,IAAI,GAAG,CACP,QAAQ,GAAG,CACX,SAAS,4BAA4B;GACxC,gBAAgB,EACb,SAAS,CACT,QAAQ,MAAM,CACd,SACC,6DACD;GACJ;EACF,EACD,OAAO,SAAS;EAEd,MAAM,qBAAqB,SACzBD,eAAaD,uBAAqB,mBAAmB,aAAa,KAAK,CAAC;AAE1E,MAAI;GACF,MAAM,OAAO,IAAI,sBAAsB,IAAI,YAAY;GACvD,MAAM,eAAe,IAAI,aAAa,IAAI,YAAY;GAKtD,MAAM,YAAY,KAAK,UAAU,UAAa,KAAK,OAAO,UAAa,KAAK,UAAU;AACtF,OAAI,KAAK,OAAO,UACd,QAAOE,gBACL,4DACD;AAGH,QACG,KAAK,WAAW,WAAW,KAAK,WAAW,cAC5C,CAAC,KAAK,OACN,CAAC,KAAK,GAEN,QAAOA,gBACL,wDAAwD,KAAK,OAAO,GACrE;GAMH,IAAI,eAA8B,EAAE;GACpC,IAAI,gBAAuC;AAE3C,OAAI,KAAK,KAAK;IAEZ,MAAM,WAAqB,EAAE;AAC7B,SAAK,MAAM,UAAU,KAAK,KAAK;KAC7B,MAAM,MAAM,KAAK,wBAAwB,OAAO;AAChD,SAAI,IACF,cAAa,KAAK,IAAI;SAEtB,UAAS,KAAK,OAAO;;AAGzB,QAAI,SAAS,SAAS,KAAK,aAAa,WAAW,EACjD,QAAO,kBACL,+BAA+B,SAAS,KAAK,KAAK,CAAC,8CACpD;cAEM,KAAK,IAAI;IAClB,MAAM,MAAM,KAAK,iBACb,KAAK,wBAAwB,KAAK,GAAG,GACrC,KAAK,QAAQ,KAAK,GAAG;AACzB,QAAI,CAAC,IACH,QAAO,kBACL,+BAA+B,KAAK,GAAG,8CACxC;AAEH,mBAAe,CAAC,IAAI;cACX,KAAK,OAAO;AACrB,QAAI,eACF,iBAAgB,MAAM,aAAa;KACjC;KACA;KACA;KACA,OAAO,KAAK;KACZ;KACA;KACA,SAAS,EAAE,OAAO,KAAK,OAAO;KAC/B,CAAC;QAEF,iBAAgB,aAAa,cAAc,KAAK,OAAO,EACrD,OAAO,KAAK,OACb,CAAC;AAEJ,mBAAe,cAAc,KAAK,MAAM,EAAE,YAAY;cAC7C,KAAK,MACd,gBAAe,KAAK,WAAW,KAAK,OAAO;IACzC,OAAO,KAAK;IACZ,eAAe,KAAK;IACrB,CAAC;OAGF,gBAAe,KAAK,iBAChB,KAAK,qBAAqB,EAAE,OAAO,KAAK,OAAO,CAAC,GAChD,KAAK,KAAK,EAAE,OAAO,KAAK,OAAO,CAAC;AAGtC,OAAI,aAAa,WAAW,EAE1B,QAAO,kBACL,+BAFiB,KAAK,SAAS,KAAK,SAAS,KAAK,MAAM,GAEd,8CAC3C;AAQH,OAAI,KAAK,WAAW,QAAQ;IAQ1B,MAAM,eAPe,mBACnB,cACA,eACA,KAAK,QACL,KAAK,OAAO,OACb,CAEiC,QAAQ,GAAG;AAC7C,WAAOD,eAAaD,uBAAqB,mBAAmB,aAAa,aAAa,CAAC;;AAIzF,OAAI,KAAK,WAAW,SAAS;IAC3B,MAAM,YAAY,KAAK,QAAQ,KAAK,KAAK,CAAC,KAAK,GAAG,GAAG,EAAE;IACvD,IAAI,UAAU;IACd,MAAM,WAAqB,EAAE;AAC7B,SAAK,MAAM,YAAY,UACrB,KAAI,KAAK,WAAW,SAAS,CAC3B;QAEA,UAAS,KAAK,SAAS;AAG3B,UAAM,OAAO,iBAAiB;KAAE;KAAS,OAAO,UAAU;KAAQ,CAAC;IACnE,IAAI,MAAM,UAAU,QAAQ,GAAG,UAAU,OAAO;AAChD,QAAI,SAAS,SAAS,EACpB,QAAO,iCAAiC,SAAS,KAAK,KAAK;AAE7D,WAAO,kBAAkB,IAAI;;AAI/B,OAAI,KAAK,WAAW,WAAW;IAC7B,MAAM,YAAY,KAAK,QAAQ,KAAK,KAAK,CAAC,KAAK,GAAG,GAAG,EAAE;IACvD,IAAI,UAAU;IACd,MAAM,WAAqB,EAAE;AAC7B,SAAK,MAAM,YAAY,UACrB,KAAI,KAAK,QAAQ,SAAS,CACxB;QAEA,UAAS,KAAK,SAAS;AAG3B,UAAM,OAAO,mBAAmB;KAC9B;KACA,OAAO,UAAU;KAClB,CAAC;IACF,IAAI,MAAM,YAAY,QAAQ,GAAG,UAAU,OAAO;AAClD,QAAI,SAAS,SAAS,EACpB,QAAO,eAAe,SAAS,KAAK,KAAK;AAE3C,WAAO,kBAAkB,IAAI;;AAI/B,UAAOE,gBAAc,mBAAmB,KAAK,SAAmB;WACzD,KAAK;GACZ,MAAM,UACJ,eAAe,QAAQ,IAAI,UAAU;AACvC,SAAM,OAAO,iBAAiB,EAAE,OAAO,SAAS,CAAC;AACjD,UAAOA,gBAAc,iBAAiB,UAAU;;GAGrD;;AAOH,SAAS,mBACP,cACA,eACA,QACA,kBAC+C;CAC/C,IAAI;CACJ,IAAI;CACJ,IAAI;AAEJ,KAAI,WAAW,WAAW;EACxB,MAAM,WAAW,cAAc,cAAc;EAC7C,MAAM,SAAS,mBACb,eACC,QAAQ,kBAAkB,KAAK,aAAa,QAAQ,IAAI,GAAG,GAAG,SAAS,IAAI,IAAI,GAAG,CAAC,EACpF,aACD;AACD,SAAO,OAAO,MACX,KAAK,KAAK,MAAM,kBAAkB,KAAK,IAAI,GAAG,SAAS,IAAI,IAAI,GAAG,CAAC,CAAC,CACpE,KAAK,KAAK;AACb,cAAY,OAAO;AACnB,kBAAgB,OAAO;YACd,WAAW,YAAY;EAEhC,MAAM,yBAAS,IAAI,KAAqD;EACxE,MAAM,WAAW,cAAc,cAAc;AAC7C,OAAK,MAAM,OAAO,cAAc;GAC9B,MAAM,OAAO,QAAQ,IAAI,UAAU;AACnC,OAAI,CAAC,OAAO,IAAI,KAAK,CACnB,QAAO,IAAI,MAAM,EAAE,CAAC;AAEtB,UAAO,IAAI,KAAK,CAAE,KAAK;IAAE;IAAK,OAAO,SAAS,IAAI,IAAI,GAAG;IAAE,CAAC;;EAG9D,MAAM,SAAS,mBACb,eACC,QAAQ;AAGP,UAAO,GAFM,QAAQ,IAAI,UAAU,CAEpB,KADD,IAAI,SAAS,WACD,KAAK,IAAI,OAAO,KAAK,YAAY,IAAI,SAAS,IAAI;KAE9E,aACD;EAGD,MAAM,cAAc,IAAI,IAAI,OAAO,MAAM,KAAK,MAAM,EAAE,GAAG,CAAC;EAC1D,MAAM,iCAAiB,IAAI,KAAqD;AAChF,OAAK,MAAM,CAAC,MAAM,UAAU,QAAQ;GAClC,MAAM,WAAW,MAAM,QAAQ,SAAS,YAAY,IAAI,KAAK,IAAI,GAAG,CAAC;AACrE,OAAI,SAAS,SAAS,EACpB,gBAAe,IAAI,MAAM,SAAS;;AAItC,SAAO,MAAM,KAAK,eAAe,SAAS,CAAC,CACxC,KAAK,CAAC,MAAM,WAAW,oBAAoB,MAAM,MAAM,CAAC,CACxD,KAAK,OAAO;AACf,cAAY,OAAO;AACnB,kBAAgB,OAAO;QAClB;EAEL,MAAM,SAAS,mBAAmB,mBAAmB;AAErD,MAAI,aAAa,WAAW,GAAG;GAC7B,MAAM,YAAY,eAAe,aAAa,GAAG;AACjD,mBAAgB,eAAe,UAAU;AACzC,OAAI,gBAAgB,QAAQ;IAE1B,MAAM,WAAW,SAAS;AAC1B,WACE,UAAU,MAAM,GAAG,SAAS,GAC5B,uBAAuB,OAAO;AAChC,gBAAY;AACZ,oBAAgB;UACX;AACL,WAAO;AACP,gBAAY;;SAET;GACL,MAAM,SAAS,mBACb,cACA,gBACA,OACD;AACD,UAAO,OAAO,MAAM,IAAI,eAAe,CAAC,KAAK,OAAO;AACpD,eAAY,OAAO;AACnB,mBAAgB,OAAO;;;CAK3B,IAAI,SAAS,QAAQ,aAAa,OAAO,gBAAgB,cAAc,oBAAoB;AAC3F,KAAI,UACF,WAAU;AAGZ,QAAOD,eAAa,GAAG,KAAK,IAAI,SAAS;;AAO3C,SAAS,cACP,eACqB;CACrB,MAAM,sBAAM,IAAI,KAAqB;AACrC,KAAI,cACF,MAAK,MAAM,KAAK,cACd,KAAI,IAAI,EAAE,YAAY,IAAI,EAAE,MAAM;AAGtC,QAAO;;;;;;;;;ACnbT,SAAgB,cAAc,SAAyB;CACrD,MAAM,gBAAgB,QAAQ,MAAM,mBAAmB;AACvD,KAAI,iBAAiB,cAAc,GAAG,UAAU,IAC9C,QAAO,cAAc,GAAG,MAAM;AAEhC,KAAI,QAAQ,UAAU,GAAI,QAAO,QAAQ,MAAM;AAC/C,QAAO,QAAQ,MAAM,GAAG,GAAG,CAAC,MAAM,GAAG;;;;;;;;AASvC,SAAgB,mBACd,QACA,IACA,aACA,oBAA8C,MACxC;AACN,QAAO,aACL,eACA;EACE,OAAO;EACP,aACE;EACF,aAAa;GACX,MAAM,EACH,QAAQ,CACR,IAAI,EAAE,CACN,IAAI,IAAM,CACV,SAAS,uCAAuC;GACnD,OAAO,EACJ,QAAQ,CACR,IAAI,IAAI,CACR,UAAU,CACV,SACC,sEACD;GACH,QAAQ,EACL,QAAQ,CACR,QAAQ,SAAS,CACjB,SAAS,qDAAqD;GAClE;EACF,EACD,OAAO,SAAS;AACd,MAAI;GACF,MAAM,OAAO,IAAI,sBAAsB,IAAI,YAAY;GACvD,MAAM,gBAAgB,KAAK,SAAS,cAAc,KAAK,KAAK;GAC5D,MAAM,MAAM,KAAK,OAAO;IACtB,SAAS,KAAK;IACd,OAAO;IACP,QAAQ,KAAK;IACd,CAAC;AAEF,SAAM,OAAO,sBAAsB;IACjC,IAAI,IAAI;IACR,OAAO;IACR,CAAC;GAGF,IAAI,eAAe,iBAAiB,cAAc,SAAS,IAAI,GAAG;AAClE,OAAI,mBAAmB;IACrB,MAAM,UAAU,kBAAkB,eAAe,YAAY;AAC7D,QAAI,QAAQ,SAAS,EAEnB,gBADe,QAAQ,KAAI,MAAK,cAAc,EAAE,UAAU,CAAC,KAAK,KAAK,GAC7C,SAAS;;AAIrC,UAAO,EACL,SAAS,CACP;IACE,MAAM;IACN,MAAM;IACP,CACF,EACF;WACM,KAAK;AAGZ,UAAO;IACL,SAAS,CACP;KAAE,MAAM;KAAiB,MAAM,mBAHjC,eAAe,QAAQ,IAAI,UAAU;KAG0B,CAC9D;IACD,SAAS;IACV;;GAGN;;;;;;;;;AC/EH,SAAgB,QAAQ,YAAoB,KAAoB;CAC9D,MAAM,OAAO,IAAI,KAAK,WAAW;CAEjC,MAAM,UADM,uBAAO,IAAI,MAAM,EACV,SAAS,GAAG,KAAK,SAAS;AAE7C,KAAI,SAAS,EAAG,QAAO;CAEvB,MAAM,UAAU,KAAK,MAAM,SAAS,IAAK;AACzC,KAAI,UAAU,GAAI,QAAO;CAEzB,MAAM,UAAU,KAAK,MAAM,UAAU,GAAG;AACxC,KAAI,YAAY,EAAG,QAAO;AAC1B,KAAI,UAAU,GAAI,QAAO,GAAG,QAAQ;CAEpC,MAAM,QAAQ,KAAK,MAAM,UAAU,GAAG;AACtC,KAAI,UAAU,EAAG,QAAO;AACxB,KAAI,QAAQ,GAAI,QAAO,GAAG,MAAM;CAEhC,MAAM,OAAO,KAAK,MAAM,QAAQ,GAAG;AACnC,KAAI,SAAS,EAAG,QAAO;AACvB,KAAI,OAAO,GAAI,QAAO,GAAG,KAAK;CAE9B,MAAM,SAAS,KAAK,MAAM,OAAO,GAAG;AACpC,KAAI,WAAW,EAAG,QAAO;AACzB,QAAO,GAAG,OAAO;;;;;ACjCnB,SAAS,SAAS,MAAc,QAAwB;AACtD,KAAI,KAAK,UAAU,OAAQ,QAAO;AAClC,QAAO,KAAK,MAAM,GAAG,OAAO,GAAG;;;;;AAMjC,SAAS,cAAc,SAAiC;AACtD,QAAO,QACJ,KACE,GAAG,MACF,GAAG,IAAI,EAAE,IAAI,EAAE,WAAW,IAAI,QAAQ,EAAE,UAAU,CAAC,GACtD,CACA,KAAK,KAAK;;;;;AAMf,SAAS,aAAa,SAAiC;AACrD,QAAO,QACJ,KACE,GAAG,MACF,GAAG,IAAI,EAAE,MAAM,EAAE,WAAW,MAAM,QAAQ,EAAE,UAAU,CAAC,QAAQ,SAAS,EAAE,SAAS,IAAI,GAC1F,CACA,KAAK,OAAO;;;;;AAMjB,SAAS,WAAW,SAAiC;AACnD,QAAO,QACJ,KAAK,GAAG,MAAM;EACb,MAAM,QAAQ;GACZ,GAAG,IAAI,EAAE,MAAM,EAAE,WAAW,MAAM,QAAQ,EAAE,UAAU,CAAC;GACvD,MAAM,EAAE;GACR,oBAAoB,EAAE,qBAAqB;GAC5C;EAGD,MAAM,WAAW,EAAE,qBAAqB,MAAM,GAAG,EAAE;AACnD,OAAK,MAAM,OAAO,SAChB,OAAM,KAAK,QAAQ,SAAS,IAAI,QAAQ,QAAQ,OAAO,IAAI,EAAE,GAAG,GAAG;AAErE,MAAI,EAAE,qBAAqB,SAAS,EAClC,OAAM,KACJ,cAAc,EAAE,qBAAqB,SAAS,EAAE,OACjD;AAGH,SAAO,MAAM,KAAK,KAAK;GACvB,CACD,KAAK,OAAO;;;;;;;;AASjB,SAAgB,cAAc,SAAiC;AAC7D,KAAI,QAAQ,UAAU,EAAG,QAAO,WAAW,QAAQ;AACnD,KAAI,QAAQ,UAAU,EAAG,QAAO,aAAa,QAAQ;AACrD,QAAO,cAAc,QAAQ;;AAO/B,SAASE,uBACP,mBACA,aACA,cACQ;AACR,KAAI,CAAC,kBAAmB,QAAO;CAC/B,MAAM,UAAU,kBAAkB,eAAe,YAAY;AAC7D,KAAI,QAAQ,WAAW,EAAG,QAAO;AAEjC,QADe,QAAQ,KAAI,MAAK,cAAc,EAAE,UAAU,CAAC,KAAK,KAAK,GACrD,SAAS;;AAG3B,SAASC,eAAa,MAAc;AAClC,QAAO,EAAE,SAAS,CAAC;EAAE,MAAM;EAAiB;EAAM,CAAC,EAAE;;;;;;;;AAavD,SAAgB,qBACd,QACA,IACA,aACA,oBAA8C,MACxC;CACN,MAAM,eAAe,IAAI,aAAa,GAAG;AAEzC,QAAO,aACL,iBACA;EACE,OAAO;EACP,aACE;EACF,aAAa;GACX,OAAO,EACJ,QAAQ,CACR,UAAU,CACV,SAAS,oEAAoE;GAChF,OAAO,EACJ,QAAQ,CACR,KAAK,CACL,IAAI,EAAE,CACN,IAAI,GAAG,CACP,QAAQ,EAAE,CACV,SAAS,wBAAwB;GACrC;EACF,EACD,OAAO,SAAS;EAEd,MAAM,qBAAqB,SACzBA,eAAaD,uBAAqB,mBAAmB,aAAa,KAAK,CAAC;AAE1E,MAAI;AACF,SAAM,OAAO,0BAA0B;IAAE,OAAO,KAAK;IAAO,OAAO,KAAK;IAAO,CAAC;GAEhF,IAAI,UAAU,aAAa,iBAAiB,aAAa,KAAK,MAAM;AAGpE,OAAI,KAAK,OAAO;IACd,MAAM,IAAI,KAAK,MAAM,aAAa;AAClC,cAAU,QAAQ,QACf,MACC,EAAE,WAAW,aAAa,CAAC,SAAS,EAAE,IACtC,EAAE,QAAQ,aAAa,CAAC,SAAS,EAAE,CACtC;;AAGH,OAAI,QAAQ,WAAW,EACrB,QAAO,kBACL,uEACD;GAGH,MAAM,YAAY,cAAc,QAAQ;GACxC,MAAM,SAAS,UAAU,QAAQ,OAAO;AAExC,SAAM,OAAO,4BAA4B,EAAE,OAAO,QAAQ,QAAQ,CAAC;AAEnE,UAAO,kBAAkB,YAAY,OAAO;WACrC,KAAK;GACZ,MAAM,UAAU,eAAe,QAAQ,IAAI,UAAU;AACrD,SAAM,OAAO,wBAAwB,EAAE,OAAO,SAAS,CAAC;AACxD,UAAOC,eAAa,qCAAqC,UAAU;;GAGxE;;;;;;;;;;;;ACvKH,MAAa,eAAe;CAC1B;CACA;CACA;CACA;CACA;CACA;CACA;CACD;AAQD,MAAa,qBAAqB;CAChC;CACA;CACA;CACA;CACA;CACA;CACA;CACD;;;;;AA4DD,SAAgB,aAAa,GAA4B;AACvD,QAAQ,aAAmC,SAAS,EAAE;;;;;;AAOxD,SAAgB,mBAAmB,GAAkC;AACnE,QAAQ,mBAAyC,SAAS,EAAE;;;;;;;AAY9D,MAAa,kBAAkB;;;;;;;;;;;;;;;;;;;;ACpG/B,MAAa,KAAK;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACsDlB,SAAS,UAAU,KAAyB;AAC1C,QAAO;EACL,IAAI,IAAI;EACR,MAAM,IAAI;EACV,MAAM,IAAI;EACV,UAAU,KAAK,MAAM,IAAI,SAAS;EAClC,iBAAiB,KAAK,MAAM,IAAI,gBAAgB;EAChD,YAAY,IAAI;EAChB,YAAY,IAAI;EACjB;;AAGH,SAAS,UAAU,KAAyB;AAC1C,QAAO;EACL,IAAI,IAAI;EACR,WAAW,IAAI;EACf,WAAW,IAAI;EACf,MAAM,IAAI;EACV,QAAQ,IAAI;EACZ,UAAU,KAAK,MAAM,IAAI,SAAS;EAClC,YAAY,IAAI;EACjB;;;;;;AAqBH,SAAgB,gBAAgB,IAAkC;AAChE,IAAG,KAAKC,GAAe;;;;;;;;;;;;;;;;;;AAuBzB,SAAgB,aACd,IACA,QACA,OAII,EAAE,EACa;CACnB,MAAM,WAAW,KAAK,SAAS;CAC/B,MAAM,YAAY,KAAK,aAAa;CAGpC,IAAI,iBAAiB;AAGrB,KAAI,KAAK,aAAa,KAAK,UAAU,SAAS,EAE5C,kBAAiB,kBADI,KAAK,UAAU,UAAU,IAAI,CAAC,KAAK,KAAK,CACb;CAKlD,IAAI;AAEJ,KAAI,cAAc,WAChB,iBAAgB;;;;;QAKZ,eAAe;;UAEV,cAAc,WACvB,iBAAgB;;;;;QAKZ,eAAe;;KAInB,iBAAgB;;;;;QAKZ,eAAe;;;;;;QAMf,eAAe;;CAIrB,MAAM,MAAM;;;;QAIN,cAAc;;;;;;;;;;;;;;;CAiBpB,MAAM,cAAyB,CAAC,OAAO;AAEvC,KAAI,cAAc,QAAQ;AAExB,cAAY,KAAK,SAAS;AAC1B,MAAI,KAAK,UAAW,aAAY,KAAK,GAAG,KAAK,UAAU;AACvD,cAAY,KAAK,SAAS;AAC1B,MAAI,KAAK,UAAW,aAAY,KAAK,GAAG,KAAK,UAAU;QAClD;AACL,cAAY,KAAK,SAAS;AAC1B,MAAI,KAAK,UAAW,aAAY,KAAK,GAAG,KAAK,UAAU;;AAKzD,QAFa,GAAG,QAAQ,IAAI,CAAC,IAAI,GAAG,YAAY,CAEpC,KAAK,SAAS;EACxB,MAAM;GACJ,IAAI,IAAI;GACR,MAAM,IAAI;GACV,MAAM,IAAI;GACV,UAAU,KAAK,MAAM,IAAI,WAAW;GACpC,iBAAiB,KAAK,MAAM,IAAI,kBAAkB;GAClD,YAAY,IAAI;GAChB,YAAY,IAAI;GACjB;EACD,MAAM,IAAI,OACN;GACE,IAAI,IAAI;GACR,WAAW,IAAI;GACf,WAAW,IAAI;GACf,MAAM,IAAI;GACV,QAAQ,IAAI;GACZ,UAAU,KAAK,MAAM,IAAI,WAAY;GACrC,YAAY,IAAI;GACjB,GACD;EACJ,OAAO,IAAI;EACZ,EAAE;;;;;AAUL,SAAgB,eACd,IACA,MACa;AAIb,QAHa,GACV,QAAQ,2CAA2C,CACnD,IAAI,KAAK,CACA,IAAI,UAAU;;;;;;AAO5B,SAAgB,qBACd,IACA,MACA,MACkB;CAClB,MAAM,MAAM,GACT,QAAQ,wDAAwD,CAChE,IAAI,MAAM,KAAK;AAClB,QAAO,MAAM,UAAU,IAAI,GAAG;;;;;;;AAYhC,SAAgB,gBACd,IACA,QACA,MACa;CACb,MAAM,YAAY,MAAM,aAAa;CAErC,IAAI;CACJ,IAAI;AAEJ,KAAI,cAAc,YAAY;AAC5B,QAAM;AACN,WAAS,CAAC,OAAO;YACR,cAAc,YAAY;AACnC,QAAM;AACN,WAAS,CAAC,OAAO;QACZ;AACL,QAAM;AACN,WAAS,CAAC,QAAQ,OAAO;;AAI3B,QADa,GAAG,QAAQ,IAAI,CAAC,IAAI,GAAG,OAAO,CAC/B,IAAI,UAAU;;;;;;AAO5B,SAAgB,kBACd,IACA,QACQ;AAMR,QALe,GACZ,QACC,+EACD,CACA,IAAI,QAAQ,OAAO,CACR;;;;;;;;;;AAehB,SAAgB,WACd,IACA,MACW;CACX,MAAM,WAAW,qBAAqB,IAAI,KAAK,MAAM,KAAK,KAAK;AAE/D,KAAI,UAAU;EAEZ,MAAM,eAAe,CACnB,GAAG,IAAI,IAAI,CAAC,GAAG,SAAS,iBAAiB,GAAG,KAAK,gBAAgB,CAAC,CACnE;EAGD,MAAM,iBAAiB;GAAE,GAAG,SAAS;GAAU,GAAG,KAAK;GAAU;AAEjE,KAAG,QACD;;qBAGD,CAAC,IACA,KAAK,UAAU,eAAe,EAC9B,KAAK,UAAU,aAAa,EAC5B,SAAS,GACV;AAKD,SAAO,UAHS,GACb,QAAQ,yCAAyC,CACjD,IAAI,SAAS,GAAG,CACM;;CAI3B,MAAM,KAAK,KAAK,MAAM,YAAY,GAAG,CAAC,SAAS,MAAM;AACrD,IAAG,QACD;6BAED,CAAC,IACA,IACA,KAAK,MACL,KAAK,MACL,KAAK,UAAU,KAAK,SAAS,EAC7B,KAAK,UAAU,KAAK,gBAAgB,CACrC;AAKD,QAAO,UAHU,GACd,QAAQ,yCAAyC,CACjD,IAAI,GAAG,CACgB;;;;;;;;AAS5B,SAAgB,WACd,IACA,MACW;CACX,MAAM,KAAK,KAAK,MAAM,YAAY,GAAG,CAAC,SAAS,MAAM;AAErD,IAAG,QACD;;;;qCAKD,CAAC,IACA,IACA,KAAK,WACL,KAAK,WACL,KAAK,MACL,KAAK,QACL,KAAK,UAAU,KAAK,SAAS,CAC9B;AAQD,QAAO,UALU,GACd,QACC,+EACD,CACA,IAAI,KAAK,WAAW,KAAK,WAAW,KAAK,KAAK,CACvB;;;;;AC5V5B,SAAS,aAAa,MAAc,QAAwB;AAC1D,KAAI,KAAK,UAAU,OAAQ,QAAO;AAClC,QAAO,KAAK,MAAM,GAAG,OAAO,CAAC,SAAS,GAAG;;AAG3C,SAAS,iBAAiB,MAA0B;AAClD,QAAO,IAAI,KAAK;;;;;;AAOlB,SAAS,cACP,WACA,kBACA,cACA,OACQ;CACR,MAAM,QAAkB,EAAE;AAG1B,OAAM,KAAK,oBAAoB;AAC/B,OAAM,KAAK,GAAG;AAEd,MAAK,MAAM,QAAQ,WAAW;EAC5B,MAAM,aAAa,iBAAiB,IAAI,KAAK,GAAG,IAAI,EAAE;EACtD,MAAM,kBAAkB,WAAW;AAEnC,QAAM,KACJ,KAAK,iBAAiB,KAAK,KAAK,CAAC,GAAG,KAAK,KAAK,IAAI,gBAAgB,aAAa,oBAAoB,IAAI,MAAM,GAAG,GACjH;AAGD,OAAK,MAAM,KAAK,YAAY;AAC1B,OAAI,CAAC,EAAE,KAAM;GACb,MAAM,YAAY,EAAE,KAAK,cAAc,KAAK,KAAK,OAAO;AACxD,SAAM,KACJ,KAAK,UAAU,GAAG,EAAE,KAAK,KAAK,GAAG,iBAAiB,EAAE,KAAK,KAAK,CAAC,GAAG,EAAE,KAAK,OAC1E;;AAGH,QAAM,KAAK,GAAG;;AAIhB,KAAI,aAAa,SAAS,GAAG;AAC3B,QAAM,KAAK,0BAA0B;AACrC,QAAM,KAAK,GAAG;AAEd,OAAK,MAAM,OAAO,cAAc;GAC9B,MAAM,MAAM,UAAU,IAAI,UAAU;GACpC,MAAM,UAAU,aAAa,IAAI,KAAK,QAAQ,OAAO,IAAI,EAAE,IAAI;AAC/D,SAAM,KAAK,MAAM,QAAQ,KAAK,IAAI,GAAG;;;AAIzC,QAAO,MAAM,KAAK,KAAK,CAAC,MAAM;;;;;AAMhC,SAAS,UAAU,SAAyB;CAG1C,MAAM,SAFM,KAAK,KAAK,GACT,IAAI,KAAK,QAAQ,CAAC,SAAS;CAGxC,MAAM,QAAQ,KAAK,MAAM,UAAU,MAAO,KAAK,IAAI;AACnD,KAAI,QAAQ,EAAG,QAAO;AACtB,KAAI,QAAQ,GAAI,QAAO,GAAG,MAAM,OAAO,UAAU,IAAI,MAAM,GAAG;CAE9D,MAAM,OAAO,KAAK,MAAM,QAAQ,GAAG;AACnC,KAAI,OAAO,GAAI,QAAO,GAAG,KAAK,MAAM,SAAS,IAAI,MAAM,GAAG;CAE1D,MAAM,SAAS,KAAK,MAAM,OAAO,GAAG;AACpC,QAAO,GAAG,OAAO,QAAQ,WAAW,IAAI,MAAM,GAAG;;AAOnD,SAASC,uBACP,mBACA,aACA,cACQ;AACR,KAAI,CAAC,kBAAmB,QAAO;CAC/B,MAAM,UAAU,kBAAkB,eAAe,YAAY;AAC7D,KAAI,QAAQ,WAAW,EAAG,QAAO;AAEjC,QADe,QAAQ,KAAK,MAAM,cAAc,EAAE,UAAU,CAAC,KAAK,KAAK,GACvD,SAAS;;AAG3B,SAASC,eAAa,MAAc;AAClC,QAAO,EAAE,SAAS,CAAC;EAAE,MAAM;EAAiB;EAAM,CAAC,EAAE;;AAGvD,SAAS,cAAc,MAAc;AACnC,QAAO;EAAE,SAAS,CAAC;GAAE,MAAM;GAAiB;GAAM,CAAC;EAAE,SAAS;EAAM;;;;;;;;AAatE,SAAgB,mBACd,QACA,IACA,aACA,oBAA8C,MACxC;AAEN,iBAAgB,GAAG;AAEnB,QAAO,aACL,eACA;EACE,OAAO;EACP,aACE;EACF,aAAa;GACX,OAAO,EACJ,QAAQ,CACR,IAAI,EAAE,CACN,SAAS,yCAAyC;GACrD,aAAa,EACV,QAAQ,CACR,UAAU,CACV,SACC,0BAA0B,aAAa,KAAK,KAAK,GAClD;GACH,OAAO,EACJ,QAAQ,CACR,KAAK,CACL,IAAI,EAAE,CACN,IAAI,EAAE,CACN,QAAQ,EAAE,CACV,SAAS,uCAAuC;GACnD,oBAAoB,EACjB,MAAM,EAAE,QAAQ,CAAC,CACjB,UAAU,CACV,SACC,iCAAiC,mBAAmB,KAAK,KAAK,GAC/D;GACH,OAAO,EACJ,QAAQ,CACR,KAAK,CACL,IAAI,EAAE,CACN,IAAI,GAAG,CACP,QAAQ,GAAG,CACX,SAAS,qDAAqD;GAClE;EACF,EACD,OAAO,SAAS;EACd,MAAM,qBAAqB,SACzBA,eACED,uBAAqB,mBAAmB,aAAa,KAAK,CAC3D;AAEH,MAAI;AACF,SAAM,OAAO,wBAAwB;IACnC,OAAO,KAAK;IACZ,aAAa,KAAK;IAClB,OAAO,KAAK;IACb,CAAC;AAGF,OAAI,KAAK,gBAAgB,UAAa,CAAC,aAAa,KAAK,YAAY,CACnE,QAAO,cACL,wBAAwB,KAAK,YAAY,kBAAkB,aAAa,KAAK,KAAK,GACnF;GAEH,MAAM,aAAa,KAAK;AAGxB,OAAI,KAAK,oBACP;SAAK,MAAM,MAAM,KAAK,mBACpB,KAAI,CAAC,mBAAmB,GAAG,CACzB,QAAO,cACL,8BAA8B,GAAG,kBAAkB,mBAAmB,KAAK,KAAK,GACjF;;GAIP,MAAM,oBAAoB,KAAK;GAK/B,MAAM,YAAyB,EAAE;AAGjC,OAAI,YAAY;IACd,MAAM,QAAQ,qBAAqB,IAAI,KAAK,OAAO,WAAW;AAC9D,QAAI,MAAO,WAAU,KAAK,MAAM;SAGhC,MAAK,MAAM,KAAK,cAAc;IAC5B,MAAM,QAAQ,qBAAqB,IAAI,KAAK,OAAO,EAAE;AACrD,QAAI,OAAO;AACT,eAAU,KAAK,MAAM;AACrB;;;AAMN,OAAI,UAAU,WAAW,GAAG;IAC1B,MAAM,cAAc,IAAI,KAAK,MAAM;IACnC,IAAI;IACJ,MAAM,SAAoB,CAAC,YAAY;AAEvC,QAAI,YAAY;AACd,WACE;AACF,YAAO,KAAK,YAAY,KAAK,MAAM;WAC9B;AACL,WACE;AACF,YAAO,KAAK,KAAK,MAAM;;IAGzB,MAAM,OAAO,GAAG,QAAQ,IAAI,CAAC,IAAI,GAAG,OAAO;AAC3C,SAAK,MAAM,OAAO,KAChB,WAAU,KAAK;KACb,IAAI,IAAI;KACR,MAAM,IAAI;KACV,MAAM,IAAI;KACV,UAAU,KAAK,MAAM,IAAI,SAAS;KAClC,iBAAiB,KAAK,MAAM,IAAI,gBAAgB;KAChD,YAAY,IAAI;KAChB,YAAY,IAAI;KACjB,CAAC;;AAKN,OAAI,UAAU,WAAW,GAAG;IAC1B,MAAM,cAAc,aAChB,2EACA,qBAAqB,aAAa,KAAK,KAAK;AAChD,WAAO,kBACL,yBAAyB,KAAK,MAAM,WAAW,cAChD;;GAIH,MAAM,mCAAmB,IAAI,KAAgC;AAE7D,QAAK,MAAM,QAAQ,WAAW;IAC5B,MAAM,UAAU,aAAa,IAAI,KAAK,IAAI;KACxC,OAAO,KAAK;KACZ,WAAW;KACX,WAAW;KACZ,CAAC;AACF,qBAAiB,IAAI,KAAK,IAAI,QAAQ;;GAIxC,MAAM,4BAAY,IAAI,KAAa;AACnC,QAAK,MAAM,QAAQ,UACjB,MAAK,MAAM,SAAS,KAAK,gBACvB,WAAU,IAAI,MAAM;GAIxB,MAAM,eAA2D,EAAE;AACnE,OAAI,UAAU,OAAO,GAAG;IACtB,MAAM,YAAY,CAAC,GAAG,UAAU;IAChC,MAAM,eAAe,UAAU,UAAU,IAAI,CAAC,KAAK,KAAK;IACxD,MAAM,UAAU,GACb,QACC,6DAA6D,aAAa,4DAC3E,CACA,IAAI,GAAG,UAAU;AAEpB,SAAK,MAAM,OAAO,QAChB,cAAa,KAAK;KAChB,MAAM,IAAI;KACV,WAAW,IAAI;KAChB,CAAC;;GAKN,MAAM,YAAY,cAChB,WACA,kBACA,cACA,KAAK,MACN;AAED,SAAM,OAAO,0BAA0B;IACrC,WAAW,UAAU;IACrB,iBAAiB,CAAC,GAAG,iBAAiB,QAAQ,CAAC,CAAC,QAC7C,KAAK,QAAQ,MAAM,IAAI,QACxB,EACD;IACD,cAAc,aAAa;IAC5B,CAAC;AAEF,UAAO,kBAAkB,UAAU;WAC5B,KAAK;GACZ,MAAM,UACJ,eAAe,QAAQ,IAAI,UAAU;AACvC,SAAM,OAAO,sBAAsB,EAAE,OAAO,SAAS,CAAC;AACtD,UAAO,cAAc,sBAAsB,UAAU;;GAG1D;;;;;;;;;;AC3VH,MAAM,oBAAoB;CACxB;CACA;CACA;CACA;CACA;CACA;CACA;CACA;CACA;CACA;CACD;;;;AAKD,MAAM,uBAAuB;CAC3B;CACA;CACA;CACA;CACA;CACA;CACA;CACD;;;;AAKD,MAAM,yBAAyB;CAC7B;CACA;CACA;CACA;CACA;CACA;CACA;CACA;CACA;CACD;;;;;;AAWD,SAAgB,oBAAoB,IAAkC;AACpE,IAAG,KAAK;;;;;;;;IAQN;;;;;;;;;;;;;;;;;AAsBJ,SAAgB,gBACd,IACA,UACmB;CAEnB,MAAM,OAAO,GACV,QAAQ,uEAAuE,CAC/E,IAAI,SAAS;AAIhB,KAAI,CAAC,KAAM,QAAO,EAAE;CAEpB,MAAM,SAAS,KAAK,MAAM,KAAK,gBAAgB;AAC/C,KAAI,OAAO,SAAS,EAAG,QAAO,EAAE;CAGhC,MAAM,eAAe,OAAO,UAAU,IAAI,CAAC,KAAK,KAAK;CAOrD,MAAM,eANO,GACV,QACC,2CAA2C,aAAa,kDACzD,CACA,IAAI,GAAG,OAAO,CAES,IAAI,iBAAiB;AAC/C,KAAI,aAAa,SAAS,EAAG,QAAO,EAAE;CAEtC,MAAM,UAA6B,EAAE;CACrC,MAAM,uBAAM,IAAI,MAAM,EAAC,aAAa;AAGpC,MAAK,IAAI,IAAI,GAAG,IAAI,aAAa,SAAS,GAAG,KAAK;EAChD,MAAM,QAAQ,aAAa;EAC3B,MAAM,QAAQ,aAAa,IAAI;EAE/B,MAAM,SAAS,oBAAoB,MAAM,SAAS,MAAM,QAAQ;AAChE,MAAI,OACF,SAAQ,KAAK;GACX,UAAU,KAAK;GACf,YAAY,KAAK;GACjB,YAAY,KAAK;GACjB,kBAAkB;IAChB,IAAI,MAAM;IACV,MAAM,MAAM;IACZ,YAAY,MAAM;IACnB;GACD,kBAAkB;IAChB,IAAI,MAAM;IACV,MAAM,MAAM;IACZ,YAAY,MAAM;IACnB;GACD;GACA,YAAY;GACb,CAAC;;AAIN,QAAO;;;;;;AAOT,SAAS,oBACP,WACA,WACe;CACf,MAAM,aAAa,UAAU,aAAa;CAC1C,MAAM,aAAa,UAAU,aAAa;CAG1C,MAAM,iBAAiB,eAAe,YAAY,WAAW;AAC7D,KAAI,eAAgB,QAAO;CAG3B,MAAM,oBAAoB,kBAAkB,WAAW;AACvD,KAAI,kBAAmB,QAAO;CAG9B,MAAM,eAAe,mBAAmB,YAAY,WAAW;AAC/D,KAAI,aAAc,QAAO;AAEzB,QAAO;;;;;;AAOT,SAAS,eACP,YACA,YACe;AACf,MAAK,MAAM,WAAW,kBACpB,KAAI,WAAW,SAAS,QAAQ,IAAI,CAAC,WAAW,SAAS,QAAQ,CAC/D,QAAO,yCAAyC,QAAQ;AAG5D,QAAO;;;;;AAMT,SAAS,kBAAkB,YAAmC;AAC5D,MAAK,MAAM,WAAW,sBAAsB;EAC1C,MAAM,QAAQ,WAAW,MAAM,QAAQ;AACvC,MAAI,MACF,QAAO,6CAA6C,MAAM,GAAG,MAAM,CAAC;;AAGxE,QAAO;;;;;;AAOT,SAAS,mBACP,YACA,YACe;AACf,MAAK,MAAM,WAAW,uBACpB,KAAI,WAAW,SAAS,QAAQ,IAAI,CAAC,WAAW,SAAS,QAAQ,CAC/D,QAAO,+CAA+C,QAAQ;AAGlE,QAAO;;;;;;;;;;;;;;AAmBT,SAAgB,qBACd,IACA,eACA,QACM;AACN,qBAAoB,GAAG;AAEvB,IAAG,QACD;uBAED,CAAC,IAAI,eAAe,OAAO;;;;;;;;;ACpO9B,SAAS,kBAAkB,IAA8C;CAEvE,MAAM,aACH,GAAG,QAAQ,0CAA0C,CAAC,KAAK,CACzD;CACL,MAAM,aACH,GAAG,QAAQ,0CAA0C,CAAC,KAAK,CACzD;CAGL,MAAM,eAAe,GAClB,QAAQ,8DAA8D,CACtE,KAAK;CACR,MAAM,eAAuC,EAAE;AAC/C,MAAK,MAAM,KAAK,aACd,cAAa,KAAK;AAEpB,MAAK,MAAM,OAAO,aAChB,cAAa,IAAI,QAAQ,IAAI;CAI/B,MAAM,YAAY,GACf,QAAQ,8DAA8D,CACtE,KAAK;CACR,MAAM,YAAoC,EAAE;AAC5C,MAAK,MAAM,KAAK,mBACd,WAAU,KAAK;AAEjB,MAAK,MAAM,OAAO,UAChB,WAAU,IAAI,QAAQ,IAAI;CAI5B,MAAM,YACJ,aAAa,IAAK,aAAa,IAAK,aAAa;CAGnD,MAAM,aAAa,GAChB,QACC;;;;iBAKD,CACA,KAAK;CAER,IAAI,iBACF;CACF,MAAM,WAAsE,EAAE;CAC9E,MAAM,mBAAmB,KAAK,MAAM,kBAAkB,GAAI;AAE1D,MAAK,MAAM,OAAO,YAAY;AAC5B,MAAI,CAAC,kBAAkB,IAAI,SAAS,eAAe,OACjD,kBAAiB;GACf,WAAW,IAAI;GACf,WAAW,IAAI;GACf,QAAQ,IAAI;GACb;AAEH,MAAI,IAAI,UAAU,iBAChB,UAAS,KAAK;GACZ,MAAM,IAAI;GACV,MAAM,IAAI;GACV,QAAQ,IAAI;GACb,CAAC;;CAKN,MAAM,WAEF,GACG,QACC;;aAGD,CACA,KAAK,CACR;CAGJ,IAAI,iBAAiB;AACrB,KAAI;AACF,sBAAoB,GAAG;AACvB,mBAEI,GACG,QACC,iEACD,CACA,KAAK,CACR;SACE;AAEN,mBAAiB;;AAGnB,QAAO;EACL,aAAa;EACb,aAAa;EACb,gBAAgB;EAChB,sBAAsB;EACtB,YAAY,KAAK,MAAM,YAAY,GAAG,GAAG;EACzC,YAAY;EACZ;EACA,sBAAsB;EACtB,iBAAiB;EAClB;;;;;AAUH,SAAS,YAAY,OAAiC;CACpD,MAAM,QAAkB,EAAE;AAG1B,OAAM,KAAK,2BAA2B;AACtC,OAAM,KACJ,UAAU,MAAM,YAAY,YAAY,MAAM,YAAY,iBAAiB,MAAM,aAClF;AACD,OAAM,KAAK,GAAG;AAGd,OAAM,KAAK,0BAA0B;CACrC,MAAM,cAAwB,EAAE;AAChC,MAAK,MAAM,KAAK,cAAc;EAC5B,MAAM,QAAQ,MAAM,eAAe,MAAM;AACzC,MAAI,QAAQ,EACV,aAAY,KAAK,GAAG,EAAE,IAAI,QAAQ;;AAGtC,OAAM,KAAK,YAAY,SAAS,IAAI,YAAY,KAAK,MAAM,GAAG,kBAAkB;AAChF,OAAM,KAAK,GAAG;AAGd,OAAM,KAAK,gCAAgC;CAC3C,MAAM,WAAqB,EAAE;AAC7B,MAAK,MAAM,KAAK,oBAAoB;EAClC,MAAM,QAAQ,MAAM,qBAAqB,MAAM;AAC/C,MAAI,QAAQ,EACV,UAAS,KAAK,GAAG,EAAE,IAAI,QAAQ;;AAGnC,OAAM,KAAK,SAAS,SAAS,IAAI,SAAS,KAAK,MAAM,GAAG,uBAAuB;AAC/E,OAAM,KAAK,GAAG;AAGd,OAAM,KAAK,aAAa;AACxB,KAAI,MAAM,SAAS,SAAS,GAAG;EAC7B,MAAM,aAAa,MAAM,SACtB,KAAK,MAAM,GAAG,EAAE,KAAK,IAAI,EAAE,OAAO,SAAS,CAC3C,KAAK,KAAK;AACb,QAAM,KAAK,kBAAkB,gBAAgB,gBAAgB,aAAa;OAE1E,OAAM,KAAK,qDAAqD;AAElE,OAAM,KAAK,yBAAyB,MAAM,qBAAqB,OAAO,MAAM,yBAAyB,IAAI,MAAM,KAAK;AACpH,OAAM,KAAK,uBAAuB,MAAM,kBAAkB;AAE1D,KAAI,MAAM,YAAY;AACpB,QAAM,KAAK,GAAG;AACd,QAAM,KACJ,mBAAmB,MAAM,WAAW,UAAU,IAAI,MAAM,WAAW,UAAU,IAAI,MAAM,WAAW,OAAO,SAC1G;;AAGH,QAAO,MAAM,KAAK,KAAK;;AAOzB,SAAS,qBACP,mBACA,aACA,cACQ;AACR,KAAI,CAAC,kBAAmB,QAAO;CAC/B,MAAM,UAAU,kBAAkB,eAAe,YAAY;AAC7D,KAAI,QAAQ,WAAW,EAAG,QAAO;AAEjC,QADe,QAAQ,KAAK,MAAM,cAAc,EAAE,UAAU,CAAC,KAAK,KAAK,GACvD,SAAS;;AAG3B,SAAS,aAAa,MAAc;AAClC,QAAO,EAAE,SAAS,CAAC;EAAE,MAAM;EAAiB;EAAM,CAAC,EAAE;;;;;;;;;AAcvD,SAAgB,mBACd,QACA,IACA,aACA,oBAA8C,MACxC;AAEN,iBAAgB,GAAG;AAEnB,QAAO,aACL,eACA;EACE,OAAO;EACP,aACE;EACF,aAAa,EAAE;EAChB,EACD,YAAY;AACV,MAAI;AACF,SAAM,OAAO,uBAAuB;GAEpC,MAAM,QAAQ,kBAAkB,GAAG;GACnC,MAAM,YAAY,YAAY,MAAM;AAEpC,SAAM,OAAO,0BAA0B;IACrC,OAAO,MAAM;IACb,OAAO,MAAM;IACd,CAAC;AAEF,UAAO,aACL,qBAAqB,mBAAmB,aAAa,UAAU,CAChE;WACM,KAAK;GACZ,MAAM,UACJ,eAAe,QAAQ,IAAI,UAAU;AACvC,SAAM,OAAO,sBAAsB,EAAE,OAAO,SAAS,CAAC;AACtD,UAAO,aAAa,sBAAsB,UAAU;;GAGzD;;;;;;;;;;;;;ACxSH,MAAM,qBAAqB;;AAG3B,MAAM,qBAAqB;;;;;;;;;;;;AAsC3B,IAAa,iBAAb,MAA4B;CAC1B,AAAQ,SAAwB;CAChC,AAAQ,0BAAU,IAAI,KAAsC;CAC5D,AAAQ,SAAS;CACjB,AAAQ,QAAQ;CAChB,AAAQ,aAAa;CACrB,AAAQ,aAAa;CACrB,AAAQ;CAER,YAAY,YAAqB;AAC/B,MAAI,WACF,MAAK,aAAa;MAIlB,MAAK,aAAa,KADF,QAAQ,cAAc,OAAO,KAAK,IAAI,CAAC,EACvB,YAAY;;;;;;;;CAUhD,MAAM,QAAuB;AAC3B,SAAO,IAAI,SAAe,SAAS,WAAW;GAC5C,MAAM,QAAQ,iBAAiB;AAC7B,UAAM,SAAS,2BAA2B;AAC1C,SAAK,QAAQ;AACb,2BAAO,IAAI,MAAM,2BAA2B,CAAC;MAC5C,mBAAmB;AAEtB,OAAI;AACF,SAAK,SAAS,IAAI,OAAO,KAAK,WAAW;YAClC,KAAK;AACZ,iBAAa,MAAM;AACnB,UAAM,SAAS,2BAA2B,EAAE,OAAO,OAAO,IAAI,EAAE,CAAC;AACjE,WAAO,IAAI;AACX;;GAIF,MAAM,WAAW,QAAwB;AACvC,QAAI,IAAI,SAAS,SAAS;AACxB,kBAAa,MAAM;AACnB,UAAK,QAAQ;AACb,UAAK,aAAa,IAAI;AACtB,UAAK,aAAa,IAAI;AACtB,WAAM,SAAS,gBAAgB;MAAE,YAAY,IAAI;MAAY,YAAY,IAAI;MAAY,CAAC;AAG1F,UAAK,OAAQ,IAAI,WAAW,QAAQ;AACpC,UAAK,OAAQ,GAAG,YAAY,MAAsB,KAAK,cAAc,EAAE,CAAC;AACxE,cAAS;;;AAIb,QAAK,OAAO,GAAG,WAAW,QAAQ;AAElC,QAAK,OAAO,GAAG,UAAU,QAAQ;AAC/B,iBAAa,MAAM;AACnB,UAAM,SAAS,gBAAgB,EAAE,OAAO,OAAO,IAAI,EAAE,CAAC;AACtD,SAAK,mBAAmB;AACxB,SAAK,QAAQ;KACb;AAEF,QAAK,OAAO,GAAG,SAAS,SAAS;AAC/B,UAAM,SAAS,iBAAiB,EAAE,MAAM,CAAC;AACzC,SAAK,mBAAmB;AACxB,SAAK,QAAQ;AACb,SAAK,SAAS;KACd;IACF;;;;;;;;CASJ,MAAM,MAAM,MAA4C;AACtD,MAAI,CAAC,KAAK,UAAU,CAAC,KAAK,MACxB,QAAO;EAGT,MAAM,KAAK,OAAO,KAAK,SAAS;AAEhC,SAAO,IAAI,SAA8B,YAAY;GACnD,MAAM,QAAQ,iBAAiB;AAC7B,UAAM,SAAS,2BAA2B,EAAE,IAAI,CAAC;AACjD,SAAK,QAAQ,OAAO,GAAG;AACvB,YAAQ,KAAK;MACZ,mBAAmB;AAEtB,QAAK,QAAQ,IAAI,IAAI;IAAW;IAAqC;IAAO,CAAC;AAC7E,QAAK,OAAQ,YAAY;IAAE,MAAM;IAAS;IAAI;IAAM,CAAC;IACrD;;;;;;;CAQJ,MAAM,WAAW,OAAmD;AAClE,MAAI,CAAC,KAAK,UAAU,CAAC,KAAK,MACxB,QAAO,MAAM,UAAU,KAAK;EAG9B,MAAM,KAAK,OAAO,KAAK,SAAS;AAEhC,SAAO,IAAI,SAAkC,YAAY;GACvD,MAAM,QAAQ,iBAAiB;AAC7B,UAAM,SAAS,iCAAiC,EAAE,IAAI,CAAC;AACvD,SAAK,QAAQ,OAAO,GAAG;AACvB,YAAQ,MAAM,UAAU,KAAK,CAAC;MAC7B,mBAAmB;AAEtB,QAAK,QAAQ,IAAI,IAAI;IAAW;IAAqC;IAAO,CAAC;AAC7E,QAAK,OAAQ,YAAY;IAAE,MAAM;IAAe;IAAI;IAAO,CAAC;IAC5D;;;;;CAMJ,MAAM,WAA0B;AAC9B,MAAI,CAAC,KAAK,OACR;AAGF,SAAO,IAAI,SAAe,YAAY;GACpC,MAAM,IAAI,KAAK;AACf,KAAE,KAAK,cAAc;AACnB,SAAK,SAAS;AACd,SAAK,QAAQ;AACb,SAAK,mBAAmB;AACxB,aAAS;KACT;AAEF,KAAE,YAAY,EAAE,MAAM,YAAY,CAAC;AAGnC,oBAAiB;AACf,QAAI,KAAK,QAAQ;AACf,WAAM,SAAS,yCAAyC;AACxD,UAAK,OAAO,WAAW;;MAExB,IAAM;IACT;;;CAIJ,UAAmB;AACjB,SAAO,KAAK;;;CAId,gBAAwB;AACtB,SAAO,KAAK;;;CAId,gBAAwB;AACtB,SAAO,KAAK;;;;;CAMd,AAAQ,cAAc,KAA2B;AAC/C,MAAI,IAAI,SAAS,kBAAkB,IAAI,SAAS,sBAAsB;GACpE,MAAM,KAAK,IAAI;GACf,MAAM,MAAM,KAAK,QAAQ,IAAI,GAAG;AAEhC,OAAI,KAAK;AACP,iBAAa,IAAI,MAAM;AACvB,SAAK,QAAQ,OAAO,GAAG;AAEvB,QAAI,IAAI,SAAS,eACf,KAAI,QAAQ,IAAI,UAAU;QAE1B,KAAI,QAAQ,IAAI,WAAW;;;;;;;;;CAWnC,AAAQ,oBAA0B;AAChC,OAAK,MAAM,CAAC,IAAI,QAAQ,KAAK,SAAS;AACpC,gBAAa,IAAI,MAAM;AACvB,OAAI,QAAQ,KAAK;AACjB,QAAK,QAAQ,OAAO,GAAG;;;;;;;;;;;;;;;;;;;;AC3L7B,IAAa,oBAAb,MAA+B;CAC7B,AAAiB;CACjB,AAAiB;CACjB,AAAiB;CACjB,AAAiB;CACjB,AAAiB;CACjB,AAAiB;CAEjB,YAAY,MAA6B;AACvC,OAAK,WAAW,KAAK;AACrB,OAAK,eAAe,KAAK;AACzB,OAAK,mBAAmB,KAAK;AAC7B,OAAK,SAAS,KAAK;AACnB,OAAK,iBAAiB,KAAK;AAC3B,OAAK,kBAAkB,KAAK;AAE5B,QAAM,QAAQ,iCAAiC;GAC7C,WAAW,CAAC,CAAC,KAAK;GAClB,mBAAmB,CAAC,CAAC,KAAK;GAC1B,oBAAoB,CAAC,CAAC,KAAK;GAC5B,CAAC;;;;;;;;;CAUJ,MAAM,kBACJ,aACA,WACA,WACkC;AAElC,MAAI,KAAK,UAAU,CAAC,KAAK,OAAO,SAAS;AACvC,SAAM,QAAQ,kDAAkD;AAChE,UAAO;IAAE,SAAS;IAAO,cAAc;IAAM;;AAI/C,MAAI,CAAC,YAAY,WAAW;AAC1B,SAAM,QAAQ,6CAA6C,EACzD,IAAI,YAAY,IACjB,CAAC;AACF,UAAO;IAAE,SAAS;IAAO,cAAc;IAAM;;AAI/C,MAAI,KAAK,QAAQ,oBAAoB,UAAa,KAAK,OAAO,oBAAoB,KAChF,MAAK,SAAS,aAAa,KAAK,OAAO,gBAAgB;EAIzD,MAAM,iBAAiB,MAAM,KAAK,YAAY,UAAU;EACxD,MAAM,SAAS,KAAK,SAAS,OAAO,eAAe;AAEnD,QAAM,QAAQ,uCAAuC;GACnD,SAAS,OAAO;GAChB,UAAU,OAAO;GACjB,WAAW,OAAO;GACnB,CAAC;AAGF,MAAI,KAAK,mBAAmB,EAAE,KAAK,QAAQ,oBAAoB,UAAa,KAAK,OAAO,oBAAoB,OAAO;GACjH,MAAM,eAAe,KAAK,gBAAgB,OAAO,OAAO,SAAS;AACjE,QAAK,SAAS,aAAa,aAAa;AACxC,SAAM,QAAQ,iDAAiD,EAC7D,cACD,CAAC;;EAIJ,IAAI,UAAyB;AAG7B,MAAI,OAAO,SAAS;GAQlB,MAAM,uBANqB,KAAK,iBAAiB,KAAK;IACpD;IACA,OAAO;IACR,CAAC,CAG8C,QAC7C,QAAQ,IAAI,YAAY,YAAY,UACtC;GAGD,MAAM,aAAa,KAAK,mBAAmB,qBAAqB;GAGhE,MAAM,UAAU,KAAK,gBAAgB,qBAAqB;GAG1D,MAAM,YAAgC,qBAAqB,KAAK,SAAS;IACvE,IAAI,IAAI;IACR,SAAS,IAAI;IACb,MAAM,IAAI;IACV,WAAW,IAAI;IACf,WAAW,IAAI,YAAY,MAAM,KAAK,IAAI,UAAU,GAAG;IACxD,EAAE;AAWH,aARc,KAAK,aAAa,YAAY;IAC1C;IACA;IACA;IACA;IACA,cAAc;IACf,CAAC,CAEc;AAEhB,SAAM,QAAQ,oCAAoC;IAAE;IAAY;IAAS,CAAC;AAG1E,OAAI,KAAK,gBAAgB;IACvB,MAAM,WAA0B;KAC9B;KACA;KACA,eAAe,YAAY;KAC3B,UAAU,OAAO;KACjB,WAAW,OAAO;KAClB,cAAc,KAAK,iBAAiB,UAAU,CAAC,gBAAgB;KAC/D,cAAc,KAAK,iBAAiB,UAAU,CAAC,gBAAgB;KAC/D,uBAAuB,KAAK,QAAQ,yBAAyB;KAC7D,SAAS;KACT,YAAY,OAAO;KACnB;KACD;AACD,SAAK,eAAe,IAAI,SAAS;;AAKnC,UAAO;IAAE,SAAS;IAAM,cADH,oDAAoD,WAAW;IAC9C;;AAIxC,MAAI,KAAK,gBAAgB;GACvB,MAAM,WAA0B;IAC9B;IACA;IACA,eAAe,YAAY;IAC3B,UAAU,OAAO;IACjB,WAAW,OAAO;IAClB,cAAc,KAAK,iBAAiB,UAAU,CAAC,gBAAgB;IAC/D,cAAc,KAAK,iBAAiB,UAAU,CAAC,gBAAgB;IAC/D,uBAAuB,KAAK,QAAQ,yBAAyB;IAC7D,SAAS;IACT,YAAY,OAAO;IACnB,SAAS;IACV;AACD,QAAK,eAAe,IAAI,SAAS;;AAGnC,SAAO;GAAE,SAAS;GAAO,cAAc;GAAM;;;;;;CAO/C,AAAQ,mBAAmB,cAAqC;AAC9D,MAAI,aAAa,WAAW,EAC1B,QAAO;AAKT,SAFc,aAAa,aAAa,SAAS,GAC/B,QAAQ,QAAQ,OAAO,IAAI,CAAC,MAAM,CACzC,MAAM,GAAG,GAAG,IAAI;;;;;;CAO7B,AAAQ,gBAAgB,cAAqC;AAC3D,MAAI,aAAa,WAAW,EAC1B,QAAO;AAST,SALe,aAAa,MAAM,GAAG,CAAC,SAAS,CAE5C,KAAK,QAAQ,IAAI,QAAQ,QAAQ,OAAO,IAAI,CAAC,MAAM,CAAC,CACpD,KAAK,MAAM,CAEA,MAAM,GAAG,IAAI;;;;;;;;;;;;;AChO/B,SAAgB,eAAe,GAAa,GAAqB;CAC/D,IAAI,MAAM;CACV,IAAI,OAAO;CACX,IAAI,OAAO;AAEX,MAAK,IAAI,IAAI,GAAG,IAAI,EAAE,QAAQ,KAAK;AACjC,SAAO,EAAE,KAAK,EAAE;AAChB,UAAQ,EAAE,KAAK,EAAE;AACjB,UAAQ,EAAE,KAAK,EAAE;;CAGnB,MAAM,mBAAmB,KAAK,KAAK,KAAK,GAAG,KAAK,KAAK,KAAK;AAG1D,KAAI,qBAAqB,EACvB,QAAO;CAGT,MAAM,aAAa,MAAM;AAKzB,QAAO,IAFmB,KAAK,IAAI,IAAI,KAAK,IAAI,GAAG,WAAW,CAAC;;;;;;AASjE,IAAa,qBAAb,MAAgC;CAC9B,AAAQ,gBAAiC;CACzC,AAAQ;CAER,YAAY,SAAkC;AAC5C,OAAK,YAAY,SAAS,aAAa;;;;;;CAOzC,OAAO,WAAuC;EAC5C,MAAM,WAAW,KAAK;AACtB,OAAK,gBAAgB;AAGrB,MAAI,aAAa,KACf,QAAO;GACL,SAAS;GACT,UAAU;GACV,WAAW,KAAK;GAChB,YAAY;GACZ,mBAAmB;GACnB,kBAAkB;GACnB;EAGH,MAAM,WAAW,eAAe,UAAU,UAAU;EACpD,MAAM,UAAU,WAAW,KAAK;EAChC,MAAM,aAAa,UACf,KAAK,KAAK,WAAW,KAAK,aAAa,KAAK,WAAW,EAAI,GAC3D;AAEJ,SAAO;GACL;GACA;GACA,WAAW,KAAK;GAChB;GACA,mBAAmB;GACnB,kBAAkB;GACnB;;;CAIH,QAAc;AACZ,OAAK,gBAAgB;;;CAIvB,eAAuB;AACrB,SAAO,KAAK;;;CAId,aAAa,OAAqB;AAChC,OAAK,YAAY,KAAK,IAAI,KAAM,KAAK,IAAI,KAAM,MAAM,CAAC;;;;;;;AC5F1D,MAAM,wBAAwB;;AAG9B,MAAM,wBAAwB;;AAG9B,MAAM,gBAAgB;;AAGtB,MAAM,iCAAiC;;AAGvC,MAAM,gBAAgB;;AAGtB,MAAM,gBAAgB;;;;;;;;;;;;;AActB,IAAa,2BAAb,MAAsC;CACpC,AAAQ;CACR,AAAQ;CACR,AAAQ;CACR,AAAQ;CACR,AAAQ;CAER,YAAY,SAGT;AACD,OAAK,QAAQ,SAAS,SAAS;AAC/B,OAAK,wBACH,SAAS,yBAAyB;AACpC,OAAK,eAAe;AACpB,OAAK,eAAe;AACpB,OAAK,mBAAmB;;;;;;;;;;;;;;CAe1B,OAAO,UAA0B;AAE/B,OAAK,eACH,KAAK,QAAQ,YAAY,IAAI,KAAK,SAAS,KAAK;EAGlD,MAAM,OAAO,WAAW,KAAK;AAG7B,OAAK,eACH,KAAK,SAAS,OAAO,SAAS,IAAI,KAAK,SAAS,KAAK;AAGvD,OAAK;AAGL,SAAO,KAAK,cAAc;;;;;;CAO5B,gBAAgB,iBAAyB,iBAA+B;AACtE,OAAK,eAAe;AACpB,OAAK,eAAe;;;;;;;;CAStB,eAAuB;EACrB,MAAM,MACJ,KAAK,eACL,KAAK,wBAAwB,KAAK,KAAK,KAAK,aAAa;AAC3D,SAAO,KAAK,IAAI,eAAe,KAAK,IAAI,eAAe,IAAI,CAAC;;;;;CAM9D,WAA2B;AACzB,SAAO;GACL,cAAc,KAAK;GACnB,cAAc,KAAK;GACnB,OAAO,KAAK;GACZ,uBAAuB,KAAK;GAC5B,kBAAkB,KAAK;GACxB;;;;;CAMH,QAAc;AACZ,OAAK,eAAe;AACpB,OAAK,eAAe;AACpB,OAAK,mBAAmB;;;;;;;;;;;;;;;;AC7F5B,IAAa,2BAAb,MAAsC;CACpC,AAAiB;CAGjB,AAAiB;CACjB,AAAiB;CACjB,AAAiB;CAEjB,YAAY,IAA4B;AACtC,OAAK,KAAK;AAEV,OAAK,aAAa,GAAG,QAAQ;;;;;;MAM3B;AAEF,OAAK,0BAA0B,GAAG,QAAQ;;;;;MAKxC;AAEF,OAAK,mBAAmB,GAAG,QAAQ;;;;;;;;;;;MAWjC;AAEF,QAAM,MAAM,uCAAuC;;;;;;;CAQrD,IAAI,UAA+B;EACjC,MAAM,KAAK,YAAY,GAAG,CAAC,SAAS,MAAM;AAE1C,OAAK,WAAW,IACd,IACA,SAAS,WACT,SAAS,WACT,SAAS,eACT,SAAS,UACT,SAAS,WACT,SAAS,cACT,SAAS,cACT,SAAS,uBACT,SAAS,UAAU,IAAI,GACvB,SAAS,YACT,SAAS,QACV;AAED,QAAM,MAAM,yBAAyB;GACnC,SAAS,SAAS;GAClB,UAAU,SAAS;GACnB,WAAW,SAAS;GACrB,CAAC;;;;;;;CAQJ,oBACE,WACA,WACA,QAAgB,IACC;AAOjB,SANa,KAAK,wBAAwB,IACxC,WACA,WACA,MACD,CAEW,IAAI,cAAc;;;;;;;;;CAUhC,aACE,WACA,QAAgB,KACkC;EAClD,MAAM,MAAM,KAAK,iBAAiB,IAAI,WAAW,MAAM;EAKvD,MAAM,QAAQ,IAAI;EAClB,MAAM,UAAU,IAAI,iBAAiB;AAGrC,SAAO;GAAE;GAAO;GAAS,MAFZ,QAAQ,IAAI,UAAU,QAAQ;GAEZ;;;AAwBnC,SAAS,cAAc,KAAiC;AACtD,QAAO;EACL,WAAW,IAAI;EACf,WAAW,IAAI;EACf,eAAe,IAAI;EACnB,UAAU,IAAI;EACd,WAAW,IAAI;EACf,cAAc,IAAI;EAClB,cAAc,IAAI;EAClB,uBAAuB,IAAI;EAC3B,SAAS,IAAI,YAAY;EACzB,YAAY,IAAI;EAChB,SAAS,IAAI;EACd;;;;;;;;;;;;AC7IH,SAAgB,8BAA8B,QAAmC;AAC/E,SAAQ,QAAR;EACE,KAAK,YACH,QAAO;EACT,KAAK,WACH,QAAO;EACT,KAAK,UACH,QAAO;;;;AAKb,MAAM,WAAiC;CACrC,mBAAmB;CACnB,uBAAuB;CACvB,iBAAiB;CACjB,WAAW;CACX,iBAAiB;EAAE,KAAK;EAAM,KAAK;EAAK;CACxC,SAAS;CACV;;;;;;;;AASD,SAAgB,2BAAiD;CAC/D,MAAM,aAAa,KAAK,cAAc,EAAE,uBAAuB;CAE/D,IAAI,MAAqB,EAAE;AAE3B,KAAI;EACF,MAAM,UAAU,aAAa,YAAY,QAAQ;AACjD,QAAM,KAAK,MAAM,QAAQ;AACzB,QAAM,UAAU,iCAAiC,EAAE,MAAM,YAAY,CAAC;SAChE;AAEN,QAAM,UAAU,kDAAkD;AAClE,SAAO,EAAE,GAAG,UAAU;;CAKxB,MAAM,SADoC;EAAC;EAAa;EAAY;EAAU,CAC/B,SAAS,IAAI,kBAAuC,GAC9F,IAAI,oBACL,SAAS;CAGb,MAAM,aACJ,OAAO,IAAI,0BAA0B,YAAY,IAAI,wBAAwB,IACzE,IAAI,wBACJ,8BAA8B,OAAO;CAG3C,MAAM,kBACJ,OAAO,IAAI,oBAAoB,WAAW,IAAI,kBAAkB;CAGlE,MAAM,YACJ,OAAO,IAAI,cAAc,YAAY,IAAI,YAAY,KAAK,IAAI,aAAa,IACvE,IAAI,YACJ,SAAS;CAGf,IAAI,YACF,OAAO,IAAI,iBAAiB,QAAQ,WAChC,IAAI,gBAAgB,MACpB,SAAS,gBAAgB;CAC/B,IAAI,YACF,OAAO,IAAI,iBAAiB,QAAQ,WAChC,IAAI,gBAAgB,MACpB,SAAS,gBAAgB;AAG/B,KAAI,YAAY,IAAM,aAAY;AAClC,KAAI,YAAY,IAAM,aAAY;AAClC,KAAI,aAAa,WAAW;AAC1B,cAAY,SAAS,gBAAgB;AACrC,cAAY,SAAS,gBAAgB;;CAIvC,MAAM,UAAU,OAAO,IAAI,YAAY,YAAY,IAAI,UAAU,SAAS;AAE1E,QAAO;EACL,mBAAmB;EACnB,uBAAuB;EACvB;EACA;EACA,iBAAiB;GAAE,KAAK;GAAW,KAAK;GAAW;EACnD;EACD;;;;;;;;;AAUH,SAAgB,YACd,QACA,UACA,iBACM;AACN,KAAI,CAAC,OAAO,SAAS;AAEnB,WAAS,aAAa,IAAI;AAC1B,QAAM,UAAU,mDAAmD;AACnE;;AAGF,KAAI,OAAO,oBAAoB,MAAM;AAEnC,WAAS,aAAa,OAAO,gBAAgB;AAC7C,QAAM,UAAU,qCAAqC,EACnD,WAAW,OAAO,iBACnB,CAAC;AACF;;CAOF,MAAM,oBAAoB,gBAAgB,cAAc;AACxD,UAAS,aAAa,kBAAkB;AAExC,OAAM,UAAU,2BAA2B;EACzC,QAAQ,OAAO;EACf,YAAY,OAAO;EACnB,WAAW;EACZ,CAAC;;;;;AC1LJ,IAAa,oBAAb,MAA+B;CAC7B,AAAiB;CACjB,AAAiB;CACjB,AAAiB;CAEjB,YAAY,IAA4B;AAEtC,KAAG,KAAK;;;;;;;MAON;AAEF,OAAK,aAAa,GAAG,QACnB,+EACD;AACD,OAAK,aAAa,GAAG,QACnB,4FACD;AACD,OAAK,cAAc,GAAG,QACpB,yDACD;AACD,QAAM,MAAM,gCAAgC;;CAG9C,IAAI,WAAmB,SAAuB;EAC5C,MAAM,KAAK,YAAY,GAAG,CAAC,SAAS,MAAM;AAC1C,OAAK,WAAW,IAAI,IAAI,WAAW,QAAQ;AAC3C,QAAM,MAAM,sBAAsB,EAAE,WAAW,CAAC;;;CAIlD,eAAe,WAA0C;EACvD,MAAM,OAAO,KAAK,WAAW,IAAI,UAAU;AAG3C,MAAI,KAAK,SAAS,EAChB,MAAK,YAAY,IAAI,UAAU;AAEjC,SAAO,KAAK,KAAI,OAAM;GACpB,IAAI,EAAE;GACN,WAAW,EAAE;GACb,SAAS,EAAE;GACX,WAAW,EAAE;GACd,EAAE;;;;;;;;;;;;ACjBP,MAAa,gBAAgC,SAAoC;CAC/E,MAAM,UAA6B,EAAE;CAGrC,MAAM,QAAQ;CAEd,MAAM,kBAAkB;CAExB,IAAI;AAEJ,SAAQ,QAAQ,MAAM,KAAK,KAAK,MAAM,MAAM;EAC1C,IAAI,OAAO,MAAM;AAEjB,MAAI,KAAK,WAAW,KAAK,CAAE,QAAO,KAAK,MAAM,EAAE;AAE/C,SAAO,KAAK,QAAQ,SAAS,IAAI;AAEjC,UAAQ,KAAK;GACX;GACA,MAAM;GACN,YAAY;GACZ,MAAM,CAAC,MAAM,OAAO,MAAM,QAAQ,MAAM,GAAG,OAAO;GACnD,CAAC;;AAIJ,SAAQ,QAAQ,gBAAgB,KAAK,KAAK,MAAM,MAAM;EACpD,MAAM,OAAO,MAAM;AAKnB,MAAI,CAHa,QAAQ,MACtB,MAAM,MAAO,SAAS,EAAE,KAAK,MAAM,MAAO,QAAQ,EAAE,KAAK,GAC3D,CAEC,SAAQ,KAAK;GACX;GACA,MAAM;GACN,YAAY;GACZ,MAAM,CAAC,MAAM,OAAO,MAAM,QAAQ,MAAM,GAAG,OAAO;GACnD,CAAC;;AAIN,QAAO;;;;;;;;;AAcT,MAAa,gBAAgC,SAAoC;CAC/E,MAAM,UAA6B,EAAE;AAYrC,MAAK,MAAM,WAXQ;EACjB;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACD,EAEiC;EAChC,IAAI;AACJ,UAAQ,QAAQ,QAAQ,KAAK,KAAK,MAAM,MAAM;GAC5C,MAAM,cAAc,MAAM,QAAQ,MAAM,GAAG;GAE3C,MAAM,YAAY,KAAK,MAAM,YAAY;GACzC,MAAM,YAAY,UAAU,OAAO,gDAAgD;GACnF,IAAI,SAAS,aAAa,IAAI,UAAU,MAAM,GAAG,UAAU,GAAG;AAC9D,YAAS,OAAO,MAAM;AAGtB,OAAI,OAAO,SAAS,IAAK,UAAS,OAAO,MAAM,GAAG,IAAI,CAAC,MAAM;AAE7D,OAAI,OAAO,SAAS,EAAG;AAEvB,WAAQ,KAAK;IACX,MAAM;IACN,MAAM;IACN,YAAY;IACZ,MAAM,CAAC,MAAM,OAAO,eAAe,aAAa,IAAI,YAAY,UAAU,QAAQ;IACnF,CAAC;;;AAIN,QAAO;;AAgDT,MAAM,cAAc,IAAI,OACtB,OArCkB;CAElB;CAAU;CAAY;CAAS;CAAa;CAE5C;CAAc;CAAc;CAAU;CAAQ;CAC9C;CAAQ;CAAQ;CAEhB;CAAO;CAAQ;CAAQ;CAAS;CAEhC;CAAW;CAAQ;CAAU;CAAW;CAAQ;CAAU;CAAa;CAAU;CAEjF;CAAQ;CAAU;CAAS;CAAW;CAAc;CAEpD;CAAS;CAAO;CAAU;CAAW;CAAS;CAAS;CAAQ;CAAQ;CAAS;CAEhF;CAAY;CAAe;CAAa;CAExC;CAAU;CAAY;CAAc;CAAS;CAAW;CAAS;CAAY;CAE7E;CAAU;CAAW;CAAW;CAAa;CAAQ;CAErD;CAAU;CAAc;CAAa;CAAS;CAE9C;CAAO;CAAU;CAAU;CAAY;CAEvC;CAAO;CAAS;CAAU;CAAU;CAEpC;CAAW;CAAQ;CAAQ;CAAW;CAAW;CAAQ;CAEzD;CAAU;CAAa;CAAa;CAAe;CAEnD;CAAO;CAAO;CAAW;CAAQ;CAAa;CAC9C;CAAa;CAAa;CAC3B,CAIoB,KAAK,MAAM,EAAE,QAAQ,uBAAuB,OAAO,CAAC,CAAC,KAAK,IAAI,CAAC,OAClF,KACD;AAED,MAAa,YAA4B,SAAoC;CAC3E,MAAM,UAA6B,EAAE;CACrC,MAAM,uBAAO,IAAI,KAAa;CAE9B,IAAI;AAEJ,aAAY,YAAY;AACxB,SAAQ,QAAQ,YAAY,KAAK,KAAK,MAAM,MAAM;EAChD,MAAM,OAAO,MAAM,GAAG,aAAa;AACnC,MAAI,KAAK,IAAI,KAAK,CAAE;AACpB,OAAK,IAAI,KAAK;AAEd,UAAQ,KAAK;GACX;GACA,MAAM;GACN,YAAY;GACZ,MAAM,CAAC,MAAM,OAAO,MAAM,QAAQ,MAAM,GAAG,OAAO;GACnD,CAAC;;AAGJ,QAAO;;;;;;AAWT,MAAa,cAA8B,SAAoC;CAC7E,MAAM,UAA6B,EAAE;CACrC,MAAM,uBAAO,IAAI,KAAa;CAG9B,MAAM,eAAe;CACrB,IAAI;AACJ,SAAQ,QAAQ,aAAa,KAAK,KAAK,MAAM,MAAM;EACjD,MAAM,OAAO,MAAM;EACnB,MAAM,QAAQ,KAAK,aAAa;AAChC,MAAI,KAAK,IAAI,MAAM,CAAE;AACrB,OAAK,IAAI,MAAM;AAEf,UAAQ,KAAK;GACX,MAAM,IAAI;GACV,MAAM;GACN,YAAY;GACZ,MAAM,CAAC,MAAM,OAAO,MAAM,QAAQ,MAAM,GAAG,OAAO;GACnD,CAAC;;CAIJ,MAAM,UAAU;AAChB,SAAQ,QAAQ,QAAQ,KAAK,KAAK,MAAM,MAAM;EAC5C,MAAM,OAAO,MAAM;EACnB,MAAM,QAAQ,KAAK,aAAa;AAChC,MAAI,KAAK,IAAI,MAAM,CAAE;AACrB,OAAK,IAAI,MAAM;AAEf,UAAQ,KAAK;GACX;GACA,MAAM;GACN,YAAY;GACZ,MAAM,CAAC,MAAM,OAAO,MAAM,QAAQ,MAAM,GAAG,OAAO;GACnD,CAAC;;CAKJ,MAAM,gBAAgB;AACtB,SAAQ,QAAQ,cAAc,KAAK,KAAK,MAAM,MAAM;EAGlD,MAAM,YADY,KAAK,MAAM,MAAM,QAAQ,MAAM,GAAG,OAAO,CAC/B,MAAM,0CAA0C;AAC5E,MAAI,CAAC,UAAW;EAEhB,MAAM,OAAO,UAAU;EACvB,MAAM,QAAQ,KAAK,aAAa;AAChC,MAAI,KAAK,IAAI,MAAM,CAAE;AACrB,OAAK,IAAI,MAAM;EAEf,MAAM,UAAU,MAAM,QAAQ,MAAM,GAAG,SAAS,UAAU,GAAG;AAC7D,UAAQ,KAAK;GACX;GACA,MAAM;GACN,YAAY;GACZ,MAAM,CAAC,MAAM,OAAO,QAAQ;GAC7B,CAAC;;AAGJ,QAAO;;;;;;;AAYT,MAAa,eAA+B,SAAoC;CAC9E,MAAM,UAA6B,EAAE;AAcrC,MAAK,MAAM,WAbQ;EACjB;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACD,EAEiC;EAChC,IAAI;AACJ,UAAQ,QAAQ,QAAQ,KAAK,KAAK,MAAM,MAAM;GAC5C,MAAM,cAAc,MAAM,QAAQ,MAAM,GAAG;GAC3C,MAAM,YAAY,KAAK,MAAM,YAAY;GACzC,MAAM,YAAY,UAAU,OAAO,gDAAgD;GACnF,IAAI,SAAS,aAAa,IAAI,UAAU,MAAM,GAAG,UAAU,GAAG;AAC9D,YAAS,OAAO,MAAM;AAEtB,OAAI,OAAO,SAAS,IAAK,UAAS,OAAO,MAAM,GAAG,IAAI,CAAC,MAAM;AAC7D,OAAI,OAAO,SAAS,EAAG;AAEvB,WAAQ,KAAK;IACX,MAAM;IACN,MAAM;IACN,YAAY;IACZ,MAAM,CAAC,MAAM,OAAO,eAAe,aAAa,IAAI,YAAY,UAAU,QAAQ;IACnF,CAAC;;;AAIN,QAAO;;;;;;;AAYT,MAAa,gBAAgC,SAAoC;CAC/E,MAAM,UAA6B,EAAE;AAUrC,MAAK,MAAM,WATQ;EACjB;EACA;EACA;EACA;EACA;EACA;EACD,EAEiC;EAChC,IAAI;AACJ,UAAQ,QAAQ,QAAQ,KAAK,KAAK,MAAM,MAAM;GAC5C,MAAM,cAAc,MAAM,QAAQ,MAAM,GAAG;GAC3C,MAAM,YAAY,KAAK,MAAM,YAAY;GACzC,MAAM,YAAY,UAAU,OAAO,gDAAgD;GACnF,IAAI,SAAS,aAAa,IAAI,UAAU,MAAM,GAAG,UAAU,GAAG;AAC9D,YAAS,OAAO,MAAM;AAEtB,OAAI,OAAO,SAAS,IAAK,UAAS,OAAO,MAAM,GAAG,IAAI,CAAC,MAAM;AAC7D,OAAI,OAAO,SAAS,EAAG;AAEvB,WAAQ,KAAK;IACX,MAAM;IACN,MAAM;IACN,YAAY;IACZ,MAAM,CAAC,MAAM,OAAO,eAAe,aAAa,IAAI,YAAY,UAAU,QAAQ;IACnF,CAAC;;;AAIN,QAAO;;;;;;;AAYT,MAAa,eAA+B,SAAoC;CAC9E,MAAM,UAA6B,EAAE;CACrC,MAAM,uBAAO,IAAI,KAAa;CAK9B,MAAM,eAAe;CACrB,IAAI;AACJ,SAAQ,QAAQ,aAAa,KAAK,KAAK,MAAM,MAAM;EACjD,MAAM,YAAY,MAAM;AAExB,MAAI,mBAAmB,KAAK,UAAU,IAAI,CAAC,QAAQ,KAAK,UAAU,CAAE;AAEpE,MAAI,yDAAyD,KAAK,UAAU,CAAE;EAE9E,MAAM,QAAQ,UAAU,aAAa;AACrC,MAAI,KAAK,IAAI,MAAM,CAAE;AACrB,OAAK,IAAI,MAAM;AAEf,UAAQ,KAAK;GACX,MAAM;GACN,MAAM;GACN,YAAY;GACZ,MAAM,CAAC,MAAM,OAAO,MAAM,QAAQ,MAAM,GAAG,OAAO;GACnD,CAAC;;CAIJ,MAAM,mBAAmB;AACzB,SAAQ,QAAQ,iBAAiB,KAAK,KAAK,MAAM,MAAM;EACrD,MAAM,OAAO,MAAM,GAAG,MAAM;EAC5B,MAAM,QAAQ,KAAK,aAAa;AAChC,MAAI,KAAK,IAAI,MAAM,CAAE;AACrB,OAAK,IAAI,MAAM;AAEf,UAAQ,KAAK;GACX;GACA,MAAM;GACN,YAAY;GACZ,MAAM,CAAC,MAAM,OAAO,MAAM,QAAQ,MAAM,GAAG,OAAO;GACnD,CAAC;;CAIJ,MAAM,cAAc;AACpB,SAAQ,QAAQ,YAAY,KAAK,KAAK,MAAM,MAAM;EAChD,MAAM,OAAO,IAAI,MAAM;EACvB,MAAM,QAAQ,KAAK,aAAa;AAChC,MAAI,KAAK,IAAI,MAAM,CAAE;AACrB,OAAK,IAAI,MAAM;AAEf,UAAQ,KAAK;GACX;GACA,MAAM;GACN,YAAY;GACZ,MAAM,CAAC,MAAM,OAAO,MAAM,QAAQ,MAAM,GAAG,OAAO;GACnD,CAAC;;AAGJ,QAAO;;;;;;AAWT,MAAa,YAA8B;CACzC;CACA;CACA;CACA;CACA;CACA;CACA;CACD;;;;AC1aD,MAAM,yBAAyB;;;;;;;;;;AAe/B,SAAgB,gBACd,MACA,eACA,MACwB;CACxB,MAAM,gBAAgB,MAAM,iBAAiB;CAG7C,MAAM,aAAgC,EAAE;AACxC,MAAK,MAAM,QAAQ,UACjB,KAAI;EACF,MAAM,UAAU,KAAK,KAAK;AAC1B,aAAW,KAAK,GAAG,QAAQ;SACrB;AAEN;;CAWJ,MAAM,WAHU,kBAHO,gBAAgB,WAAW,CAGD,CAGxB,QAAQ,MAAM,EAAE,cAAc,cAAc;AAGrE,UAAS,MAAM,GAAG,MAAM,EAAE,aAAa,EAAE,WAAW;AAEpD,QAAO;EACL,UAAU,SAAS,KAAK,OAAO;GAC7B,MAAM,EAAE;GACR,MAAM,EAAE;GACR,YAAY,EAAE;GACf,EAAE;EACH;EACA,8BAAa,IAAI,MAAM,EAAC,aAAa;EACtC;;;;;;;;;;;;;;AAmBH,SAAgB,kBACd,IACA,MACA,eACA,MACa;CACb,MAAM,SAAS,gBAAgB,MAAM,eAAe,KAAK;CACzD,MAAM,YAAyB,EAAE;AAoBjC,CAlBgB,GAAG,kBAAkB;AACnC,OAAK,MAAM,UAAU,OAAO,SAC1B,KAAI;GACF,MAAM,OAAO,WAAW,IAAI;IAC1B,MAAM,OAAO;IACb,MAAM,OAAO;IACb,UAAU,EAAE,YAAY,OAAO,YAAY;IAC3C,iBAAiB,CAAC,cAAc;IACjC,CAAC;AACF,aAAU,KAAK,KAAK;UACd;AAGN;;GAGJ,EAEO;AACT,QAAO;;;;;;;;;;;;AAiBT,SAAS,gBAAgB,SAA+C;AACtE,KAAI,QAAQ,UAAU,EAAG,QAAO,CAAC,GAAG,QAAQ;CAG5C,MAAM,SAAS,CAAC,GAAG,QAAQ,CAAC,MAAM,GAAG,MAAM,EAAE,aAAa,EAAE,WAAW;CAEvE,MAAM,SAA4B,EAAE;AAEpC,MAAK,MAAM,SAAS,OASlB,KAPwB,OAAO,WAC5B,SACC,KAAK,SAAS,MAAM,QACpB,MAAM,KAAK,KAAK,KAAK,KAAK,MAC1B,MAAM,KAAK,KAAK,KAAK,KAAK,GAC7B,KAEuB,GAEtB,QAAO,KAAK,MAAM;AAKtB,QAAO;;;;;;;AAQT,SAAS,kBAAkB,SAA+C;CACxE,MAAM,wBAAQ,IAAI,KAA8B;AAEhD,MAAK,MAAM,SAAS,SAAS;EAC3B,MAAM,MAAM,GAAG,MAAM,KAAK,GAAG,MAAM,KAAK,aAAa;EACrD,MAAM,WAAW,MAAM,IAAI,IAAI;AAC/B,MAAI,CAAC,YAAY,MAAM,aAAa,SAAS,WAC3C,OAAM,IAAI,KAAK,MAAM;;AAIzB,QAAO,CAAC,GAAG,MAAM,QAAQ,CAAC;;;;;;;;;;;;;;;;;;;;;AClI5B,SAAgB,iBACd,IACA,QACA,YAAoB,iBACmB;AA6BvC,QA5BgB,GAAG,kBAAkB;EACnC,MAAM,eAAe,kBAAkB,IAAI,OAAO;AAElD,MAAI,gBAAgB,UAClB,QAAO;GAAE,QAAQ;GAAG,WAAW;GAAc;EAI/C,MAAM,QAAQ,gBAAgB,IAAI,OAAO;AACzC,QAAM,MAAM,GAAG,MAAM,EAAE,SAAS,EAAE,OAAO;EAEzC,MAAM,UAAU,eAAe;EAC/B,MAAM,gBAAgB,MAAM,MAAM,GAAG,QAAQ;EAE7C,MAAM,aAAa,GAAG,QAAQ,uCAAuC;AACrE,OAAK,MAAM,QAAQ,cACjB,YAAW,IAAI,KAAK,GAAG;EAGzB,MAAM,YAAY,eAAe;AAEjC,UAAQ,OAAO,MACb,2BAA2B,QAAQ,iCAAiC,OAAO,IAAI,UAAU,eAC1F;AAED,SAAO;GAAE,QAAQ;GAAS;GAAW;GACrC,EAEc;;;;;;;;;;;;;;;;;AAsBlB,SAAgB,cACd,IACA,QACA,SACM;AAmFN,CAlFc,GAAG,kBAAkB;EAEjC,MAAM,UAAU,GACb,QAAQ,yCAAyC,CACjD,IAAI,OAAO;EACd,MAAM,WAAW,GACd,QAAQ,yCAAyC,CACjD,IAAI,QAAQ;AAEf,MAAI,CAAC,WAAW,CAAC,SACf,OAAM,IAAI,MAAM,mDAAmD,OAAO,UAAU,QAAQ,GAAG;EAGjG,MAAM,aAAa,KAAK,MAAM,QAAQ,gBAAgB;EACtD,MAAM,cAAc,KAAK,MAAM,SAAS,gBAAgB;EACxD,MAAM,eAAe,CAAC,GAAG,IAAI,IAAI,CAAC,GAAG,YAAY,GAAG,YAAY,CAAC,CAAC;EAGlE,MAAM,WAAW,KAAK,MAAM,QAAQ,SAAS;EAE7C,MAAM,aAAa;GAAE,GADH,KAAK,MAAM,SAAS,SAAS;GACZ,GAAG;GAAU;AAEhD,KAAG,QACD,sGACD,CAAC,IAAI,KAAK,UAAU,aAAa,EAAE,KAAK,UAAU,WAAW,EAAE,OAAO;EAGvE,MAAM,aAAa,gBAAgB,IAAI,QAAQ;AAG/C,OAAK,MAAM,QAAQ,YAAY;GAC7B,IAAI,cAAc,KAAK;GACvB,IAAI,cAAc,KAAK;AAEvB,OAAI,KAAK,cAAc,QACrB,eAAc;AAEhB,OAAI,KAAK,cAAc,QACrB,eAAc;AAIhB,OAAI,gBAAgB,aAAa;AAC/B,OAAG,QAAQ,uCAAuC,CAAC,IAAI,KAAK,GAAG;AAC/D;;GAIF,MAAM,WAAW,GACd,QACC,wFACD,CACA,IAAI,aAAa,aAAa,KAAK,KAAK;AAI3C,OAAI,YAAY,SAAS,OAAO,KAAK,IAAI;AAEvC,QAAI,KAAK,SAAS,SAAS,OACzB,IAAG,QAAQ,iDAAiD,CAAC,IAC3D,KAAK,QACL,SAAS,GACV;AAEH,OAAG,QAAQ,uCAAuC,CAAC,IAAI,KAAK,GAAG;cACtD,CAAC,SAEV,IAAG,QACD,mEACD,CAAC,IAAI,aAAa,aAAa,KAAK,GAAG;;AAM5C,KAAG,QAAQ,uCAAuC,CAAC,IAAI,QAAQ;AAE/D,UAAQ,OAAO,MACb,kCAAkC,QAAQ,QAAQ,OAAO,IAAI,WAAW,OAAO,oBAChF;GACD,EAEK;;;;;;AAWT,MAAM,mBAA2C;CAC/C,IAAI;CACJ,IAAI;CACJ,IAAI;CACJ,IAAI;CACJ,IAAI;CACJ,IAAI;CACJ,UAAU;CACV,OAAO;CACP,KAAK;CACL,IAAI;CACJ,IAAI;CACJ,IAAI;CACJ,IAAI;CACJ,KAAK;CACL,IAAI;CACJ,aAAa;CACb,IAAI;CACJ,IAAI;CACJ,IAAI;CACJ,IAAI;CACJ,IAAI;CACL;;;;;;;;;;;;;;;;AAiBD,SAAgB,sBACd,IACA,MACkD;CAElD,IAAI;AACJ,KAAI,MAAM,KACR,SAAQ,eAAe,IAAI,KAAK,KAAK;MAChC;EACL,MAAM,WAAW;AACjB,UAAQ,EAAE;AACV,OAAK,MAAM,QAAQ,SACjB,OAAM,KAAK,GAAG,eAAe,IAAI,KAAK,CAAC;;CAI3C,MAAM,aAA+D,EAAE;CACvE,MAAM,uBAAO,IAAI,KAAa;CAG9B,MAAM,qCAAqB,IAAI,KAA0B;AACzD,MAAK,MAAM,QAAQ,OAAO;EACxB,MAAM,MAAM,GAAG,KAAK,KAAK,GAAG,KAAK,KAAK,aAAa;EACnD,MAAM,QAAQ,mBAAmB,IAAI,IAAI,IAAI,EAAE;AAC/C,QAAM,KAAK,KAAK;AAChB,qBAAmB,IAAI,KAAK,MAAM;;AAGpC,MAAK,MAAM,GAAG,UAAU,mBACtB,KAAI,MAAM,SAAS,GAAG;EACpB,MAAM,MAAM,MAAM,KAAK,MAAM,EAAE,GAAG,CAAC,MAAM,CAAC,KAAK,IAAI;AACnD,MAAI,CAAC,KAAK,IAAI,IAAI,EAAE;AAClB,QAAK,IAAI,IAAI;AACb,cAAW,KAAK;IACd,UAAU;IACV,QAAQ,iCAAiC,MAAM,GAAG,KAAK,SAAS,MAAM,GAAG,KAAK;IAC/E,CAAC;;;CAMR,MAAM,qCAAqB,IAAI,KAA0B;AACzD,MAAK,MAAM,QAAQ,OAAO;EACxB,MAAM,QAAQ,KAAK,KAAK,aAAa;EACrC,MAAM,YAAY,iBAAiB,UAAU;EAC7C,MAAM,MAAM,GAAG,KAAK,KAAK,GAAG;EAC5B,MAAM,QAAQ,mBAAmB,IAAI,IAAI,IAAI,EAAE;AAC/C,QAAM,KAAK,KAAK;AAChB,qBAAmB,IAAI,KAAK,MAAM;;AAGpC,MAAK,MAAM,GAAG,UAAU,mBACtB,KAAI,MAAM,SAAS,GAGjB;MADc,IAAI,IAAI,MAAM,KAAK,MAAM,EAAE,KAAK,aAAa,CAAC,CAAC,CACnD,OAAO,GAAG;GAClB,MAAM,MAAM,MAAM,KAAK,MAAM,EAAE,GAAG,CAAC,MAAM,CAAC,KAAK,IAAI;AACnD,OAAI,CAAC,KAAK,IAAI,IAAI,EAAE;AAClB,SAAK,IAAI,IAAI;AACb,eAAW,KAAK;KACd,UAAU;KACV,QAAQ,+BAA+B,MAAM,GAAG,KAAK,SAAS,MAAM,GAAG,KAAK;KAC7E,CAAC;;;;AAOV,KAAI,CAAC,MAAM,QAAQ,KAAK,SAAS,QAAQ;EACvC,MAAM,YAAY,MAAM,QAAQ,MAAM,EAAE,SAAS,OAAO;EACxD,MAAM,mCAAmB,IAAI,KAA0B;AAEvD,OAAK,MAAM,QAAQ,WAAW;GAC5B,IAAI,aAAa,KAAK;AAEtB,OAAI,WAAW,WAAW,KAAK,CAAE,cAAa,WAAW,MAAM,EAAE;AAEjE,gBAAa,WAAW,QAAQ,OAAO,IAAI;AAE3C,gBAAa,WAAW,QAAQ,SAAS,IAAI;AAE7C,gBAAa,WAAW,aAAa;GAErC,MAAM,MAAM,QAAQ;GACpB,MAAM,QAAQ,iBAAiB,IAAI,IAAI,IAAI,EAAE;AAC7C,SAAM,KAAK,KAAK;AAChB,oBAAiB,IAAI,KAAK,MAAM;;AAGlC,OAAK,MAAM,GAAG,UAAU,iBACtB,KAAI,MAAM,SAAS,GAAG;GACpB,MAAM,MAAM,MAAM,KAAK,MAAM,EAAE,GAAG,CAAC,MAAM,CAAC,KAAK,IAAI;AACnD,OAAI,CAAC,KAAK,IAAI,IAAI,EAAE;AAClB,SAAK,IAAI,IAAI;AACb,eAAW,KAAK;KACd,UAAU;KACV,QAAQ,8BAA8B,MAAM,GAAG,KAAK,SAAS,MAAM,GAAG,KAAK;KAC5E,CAAC;;;;AAMV,QAAO;;;;;;;;ACpUT,MAAM,kBAGD;CAEH;EAAE,SAAS;EAAmC,MAAM;EAAc;CAClE;EAAE,SAAS;EAAiD,MAAM;EAAa;CAC/E;EAAE,SAAS;EAA8C,MAAM;EAAa;CAC5E;EAAE,SAAS;EAA6C,MAAM;EAAc;CAC5E;EAAE,SAAS;EAA2C,MAAM;EAAW;CAEvE;EAAE,SAAS;EAAwB,MAAM;EAAQ;CAClD;;;;;AAUD,MAAM,qBAAuD;CAC3D,cAAc;CACd,cAAc;CACd,cAAc;CACd,kBAAkB;CAClB,kBAAkB;CAClB,oBAAoB;CACpB,oBAAoB;CACpB,iBAAiB;CACjB,iBAAiB;CACjB,qBAAqB;CACrB,qBAAqB;CACrB,kBAAkB;CAClB,kBAAkB;CAClB,iBAAiB;CACjB,iBAAiB;CAClB;;;;;;;;;;;;;;;AAoBD,SAAgB,oBACd,MACA,UACyB;AACzB,KAAI,SAAS,SAAS,EAAG,QAAO,EAAE;CAElC,MAAM,aAAsC,EAAE;AAG9C,MAAK,IAAI,IAAI,GAAG,IAAI,SAAS,QAAQ,IACnC,MAAK,IAAI,IAAI,IAAI,GAAG,IAAI,SAAS,QAAQ,KAAK;EAC5C,MAAM,SAAS,SAAS;EACxB,MAAM,SAAS,SAAS;AAGxB,MAAI,OAAO,SAAS,OAAO,QAAQ,OAAO,SAAS,OAAO,KACxD;EAIF,MAAM,YAAY,KAAK,aAAa,CAAC,QAAQ,OAAO,KAAK,aAAa,CAAC;EACvE,MAAM,YAAY,KAAK,aAAa,CAAC,QAAQ,OAAO,KAAK,aAAa,CAAC;AAGvE,MAAI,cAAc,MAAM,cAAc,GAAI;EAM1C,MAAM,SAAS,KAAK,IAAI,WAAW,UAAU;EAC7C,MAAM,SAAS,KAAK,IAClB,YAAY,OAAO,KAAK,QACxB,YAAY,OAAO,KAAK,OACzB;EACD,MAAM,eAAe,KAAK,IAAI,GAAG,SAAS,GAAG;EAC7C,MAAM,aAAa,KAAK,IAAI,KAAK,QAAQ,SAAS,GAAG;EACrD,MAAM,cAAc,KAAK,MAAM,cAAc,WAAW;EAGxD,MAAM,UAAU,GAAG,OAAO,KAAK,IAAI,OAAO;EAC1C,IAAI,mBAAmB,mBAAmB,YAAY;AAGtD,OAAK,MAAM,UAAU,gBACnB,KAAI,OAAO,QAAQ,KAAK,YAAY,EAAE;AACpC,sBAAmB,OAAO;AAC1B;;AAKJ,MAAI,OAAO,SAAS,UAAU,OAAO,SAAS,QAC5C;OAAI,mCAAmC,KAAK,YAAY,CACtD,oBAAmB;;EAKvB,IAAI;AACJ,MAAI,qBAAqB,gBAAgB,CAAC,mBAAmB,SAE3D,cAAa;MAEb,cAAa;AAKf,MADiB,KAAK,IAAI,YAAY,UAAU,IAChC,GACd,eAAc;AAIhB,MAAI,kBAAkB,MAAM,WAAW,UAAU,CAC/C,eAAc;AAIhB,eAAa,KAAK,IAAI,YAAY,EAAI;AAEtC,aAAW,KAAK;GACd,cAAc;IAAE,MAAM,OAAO;IAAM,MAAM,OAAO;IAAM;GACtD,cAAc;IAAE,MAAM,OAAO;IAAM,MAAM,OAAO;IAAM;GACtD;GACA;GACA,UAAU,YAAY,MAAM,GAAG,IAAI;GACpC,CAAC;;AAIN,QAAO;;;;;;;;;;;;AAiBT,SAAgB,iBACd,IACA,MACA,UACa;CACb,MAAM,aAAa,oBAAoB,MAAM,SAAS;CACtD,MAAM,YAAyB,EAAE;CACjC,MAAM,kCAAkB,IAAI,KAAa;AA6CzC,CA3CgB,GAAG,kBAAkB;AACnC,OAAK,MAAM,aAAa,YAAY;AAElC,OAAI,UAAU,cAAc,GAAK;GAGjC,MAAM,aAAa,qBACjB,IACA,UAAU,aAAa,MACvB,UAAU,aAAa,KACxB;GACD,MAAM,aAAa,qBACjB,IACA,UAAU,aAAa,MACvB,UAAU,aAAa,KACxB;AAGD,OAAI,CAAC,cAAc,CAAC,WAAY;AAEhC,OAAI;IACF,MAAM,OAAO,WAAW,IAAI;KAC1B,WAAW,WAAW;KACtB,WAAW,WAAW;KACtB,MAAM,UAAU;KAChB,QAAQ,UAAU;KAClB,UAAU,EAAE,UAAU,UAAU,UAAU;KAC3C,CAAC;AACF,cAAU,KAAK,KAAK;AACpB,oBAAgB,IAAI,WAAW,GAAG;AAClC,oBAAgB,IAAI,WAAW,GAAG;WAC5B;AAEN;;;AAKJ,OAAK,MAAM,UAAU,gBACnB,kBAAiB,IAAI,OAAO;GAE9B,EAEO;AACT,QAAO;;;;;;;AAYT,SAAS,kBACP,MACA,MACA,MACS;CACT,MAAM,QAAQ,KAAK,IAAI,MAAM,KAAK;CAClC,MAAM,MAAM,KAAK,IAAI,MAAM,KAAK;CAChC,MAAM,UAAU,KAAK,MAAM,OAAO,IAAI;AAGtC,QAAO,CAAC,UAAU,KAAK,QAAQ;;;;;;;;;AC9NjC,SAAS,iBAAiB,GAAa,GAAqB;AAC1D,KAAI,EAAE,WAAW,EAAE,UAAU,EAAE,WAAW,EAAG,QAAO;CAEpD,IAAI,MAAM;CACV,IAAI,QAAQ;CACZ,IAAI,QAAQ;AAEZ,MAAK,IAAI,IAAI,GAAG,IAAI,EAAE,QAAQ,KAAK;AACjC,SAAO,EAAE,KAAK,EAAE;AAChB,WAAS,EAAE,KAAK,EAAE;AAClB,WAAS,EAAE,KAAK,EAAE;;CAGpB,MAAM,QAAQ,KAAK,KAAK,MAAM,GAAG,KAAK,KAAK,MAAM;AACjD,KAAI,UAAU,EAAG,QAAO;AAExB,QAAO,MAAM;;;;;;AAOf,SAAS,kBAAkB,OAAe,OAAuB;CAC/D,MAAM,YAAY,MAChB,IAAI,IACF,EACG,aAAa,CACb,MAAM,0CAA0C,CAChD,QAAQ,MAAM,EAAE,SAAS,EAAE,CAC/B;CAEH,MAAM,OAAO,SAAS,MAAM;CAC5B,MAAM,OAAO,SAAS,MAAM;AAE5B,KAAI,KAAK,SAAS,KAAK,KAAK,SAAS,EAAG,QAAO;AAC/C,KAAI,KAAK,SAAS,KAAK,KAAK,SAAS,EAAG,QAAO;CAE/C,IAAI,eAAe;AACnB,MAAK,MAAM,KAAK,KACd,KAAI,KAAK,IAAI,EAAE,CAAE;CAGnB,MAAM,QAAQ,KAAK,OAAO,KAAK,OAAO;AACtC,QAAO,UAAU,IAAI,IAAI,eAAe;;;;;AAM1C,SAAS,gBAAgB,KAAuB;CAC9C,MAAM,SAAS,IAAI,aACjB,IAAI,QACJ,IAAI,YACJ,IAAI,aAAa,EAClB;AACD,QAAO,MAAM,KAAK,OAAO;;;;;;;;;;;AAgB3B,SAAS,gBACP,cACQ;AACR,KAAI,aAAa,WAAW,EAAG,QAAO;AACtC,KAAI,aAAa,WAAW,EAAG,QAAO,aAAa,GAAG;CAGtD,MAAM,SAAS,CAAC,GAAG,aAAa,CAAC,MAC9B,GAAG,MAAM,EAAE,KAAK,SAAS,EAAE,KAAK,OAClC;CACD,MAAM,OAAO,OAAO;CACpB,MAAM,YAAY,IAAI,IACpB,KAAK,KACF,aAAa,CACb,MAAM,MAAM,CACZ,QAAQ,MAAM,EAAE,SAAS,EAAE,CAC/B;CAGD,MAAM,iBAA2B,EAAE;AACnC,MAAK,IAAI,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK;EACtC,MAAM,QAAQ,OAAO,GAAG,KACrB,MAAM,MAAM,CACZ,QAAQ,MAAM,EAAE,SAAS,EAAE;AAC9B,OAAK,MAAM,QAAQ,MACjB,KACE,CAAC,UAAU,IAAI,KAAK,aAAa,CAAC,IAClC,CAAC,eAAe,SAAS,KAAK,aAAa,CAAC,CAE5C,gBAAe,KAAK,KAAK,aAAa,CAAC;;CAK7C,IAAI,UAAU,KAAK;AACnB,KAAI,eAAe,SAAS,GAAG;EAC7B,MAAM,SAAS,eAAe,MAAM,GAAG,GAAG,CAAC,KAAK,KAAK;AACrD,aAAW,WAAW,OAAO;;AAG/B,QAAO,sBAAsB,aAAa,OAAO,iBAAiB;;;;;;;;;;;;;;;;AAiBpE,SAAgB,sBACd,IACA,MACgB;CAChB,MAAM,qBAAqB,MAAM,aAAa;CAC9C,MAAM,gBAAgB;CAGtB,IAAI;AACJ,KAAI,MAAM,UAAU;EAClB,MAAM,MAAM,GACT,QAAQ,2DAA2D,CACnE,IAAI,KAAK,SAAS;AACrB,UAAQ,MAAM,CAAC,IAAI,GAAG,EAAE;OAExB,SAAQ,GACL,QAAQ,8CAA8C,CACtD,KAAK;CAGV,MAAM,WAA2B,EAAE;AAEnC,MAAK,MAAM,QAAQ,OAAO;EACxB,MAAM,SAAS,KAAK,MAAM,KAAK,gBAAgB;AAC/C,MAAI,OAAO,SAAS,EAAG;EAGvB,MAAM,eAAe,OAAO,UAAU,IAAI,CAAC,KAAK,KAAK;EACrD,MAAM,OAAO,GACV,QACC;;wBAEgB,aAAa,0BAC9B,CACA,IAAI,GAAG,OAAO;AAEjB,MAAI,KAAK,SAAS,EAAG;EAGrB,MAAM,eAAe,KAAK,KAAK,OAAO;GACpC,IAAI,EAAE;GACN,MAAM,EAAE;GACR,WAAW,EAAE,YAAY,gBAAgB,EAAE,UAAU,GAAG;GACxD,YAAY,EAAE;GACf,EAAE;EAGH,MAAM,uBAAO,IAAI,KAAa;AAE9B,OAAK,IAAI,IAAI,GAAG,IAAI,aAAa,QAAQ,KAAK;AAC5C,OAAI,KAAK,IAAI,aAAa,GAAG,GAAG,CAAE;GAElC,MAAM,UAAU,CAAC,aAAa,GAAG;GACjC,IAAI,WAAW;GACf,IAAI,YAAY;AAEhB,QAAK,IAAI,IAAI,IAAI,GAAG,IAAI,aAAa,QAAQ,KAAK;AAChD,QAAI,KAAK,IAAI,aAAa,GAAG,GAAG,CAAE;IAGlC,IAAI,aAAa;IACjB,IAAI,eAAe;IACnB,IAAI,iBAAiB;AAErB,SAAK,MAAM,UAAU,SAAS;KAC5B,MAAM,MAAM,kBACV,QACA,aAAa,IACb,oBACA,cACD;AAED,SAAI,QAAQ,MAAM;AAChB,mBAAa;AACb;;AAGF,qBAAgB;AAChB;;AAGF,QAAI,cAAc,iBAAiB,GAAG;AACpC,aAAQ,KAAK,aAAa,GAAG;AAC7B,iBAAY;AACZ,kBAAa;;;AAIjB,OAAI,QAAQ,UAAU,GAAG;AAEvB,SAAK,MAAM,OAAO,QAChB,MAAK,IAAI,IAAI,GAAG;IAGlB,MAAM,SAAS,YAAY,IAAI,WAAW,YAAY;AAEtD,aAAS,KAAK;KACZ,UAAU,KAAK;KACf,cAAc;KACd,YAAY;KACZ,kBAAkB,gBAAgB,QAAQ;KAC3C,CAAC;;;;AAMR,UAAS,MAAM,GAAG,MAAM,EAAE,aAAa,SAAS,EAAE,aAAa,OAAO;AAEtE,QAAO;;;;;;AAOT,SAAS,kBACP,GACA,GACA,oBACA,eACe;AAEf,KAAI,EAAE,aAAa,EAAE,WAAW;EAC9B,MAAM,MAAM,iBAAiB,EAAE,WAAW,EAAE,UAAU;AACtD,SAAO,OAAO,qBAAqB,MAAM;;CAI3C,MAAM,MAAM,kBAAkB,EAAE,MAAM,EAAE,KAAK;AAC7C,QAAO,OAAO,gBAAgB,MAAM;;;;;;;;;;;;;;;;;;AAuBtC,SAAgB,wBACd,IACA,SAC4C;AAwF5C,QAvFc,GAAG,kBAAkB;EACjC,MAAM,WAAW,YAAY,GAAG,CAAC,SAAS,MAAM;EAChD,MAAM,uBAAM,IAAI,MAAM,EAAC,aAAa;EACpC,MAAM,aAAa,QAAQ,aAAa,KAAK,MAAM,EAAE,GAAG;EAGxD,MAAM,WAAW,KAAK,UAAU;GAC9B,aAAa;GACb,WAAW;GACX,gBAAgB,QAAQ,aAAa;GACtC,CAAC;EAGF,IAAI,gBAA+B;EACnC,MAAM,uBAAuB,QAAQ,aAAa,QAC/C,MAAM,EAAE,cAAc,KACxB;AAED,MAAI,qBAAqB,SAAS,GAAG;GACnC,MAAM,MAAM,qBAAqB,GAAG,UAAW;GAC/C,MAAM,OAAO,IAAI,aAAa,IAAI;AAElC,QAAK,MAAM,OAAO,sBAAsB;IACtC,MAAM,MAAM,IAAI;AAChB,SAAK,IAAI,IAAI,GAAG,IAAI,KAAK,IACvB,MAAK,MAAM,IAAI;;AAInB,QAAK,IAAI,IAAI,GAAG,IAAI,KAAK,IACvB,MAAK,MAAM,qBAAqB;AAGlC,mBAAgB,OAAO,KAAK,KAAK,OAAO;;EAU1C,MAAM,cANW,GACd,QAAQ,6DAA6D,CACrE,IAAI,QAAQ,aAAa,GAAG,GAAG,EAIJ,gBAAgB;AAE9C,KAAG,QACD;2CAED,CAAC,IACA,UACA,aACA,QAAQ,kBACR,YAAY,YACZ,kBACA,MACA,eACA,KACA,IACD;EAGD,MAAM,UAAU,GACb,QAAQ,uDAAuD,CAC/D,IAAI,QAAQ,SAAS;AAExB,MAAI,SAAS;GACX,MAAM,aAAa,KAAK,MAAM,QAAQ,gBAAgB;GACtD,MAAM,aAAa,IAAI,IAAI,WAAW;GACtC,MAAM,aAAa,WAAW,QAAQ,OAAO,CAAC,WAAW,IAAI,GAAG,CAAC;AACjE,cAAW,KAAK,SAAS;AAEzB,MAAG,QACD,wFACD,CAAC,IAAI,KAAK,UAAU,WAAW,EAAE,QAAQ,SAAS;;EAIrD,MAAM,iBAAiB,GAAG,QACxB,sDACD;AACD,OAAK,MAAM,SAAS,WAClB,gBAAe,IAAI,KAAK,MAAM;AAGhC,SAAO;GAAE;GAAU;GAAY;GAC/B,EAEY;;;;;;;;;;;;;;;;;;AAuBhB,SAAgB,cACd,IACA,MACoB;CACpB,MAAM,gBAAgB,MAAM,iBAAiB;CAC7C,MAAM,aAAa,MAAM,UAAU;CAEnC,MAAM,sBAAM,IAAI,MAAM;CAItB,MAAM,6BAHa,IAAI,KACrB,IAAI,SAAS,GAAG,aAAa,KAAK,KAAK,KAAK,IAC7C,EAC4B,aAAa;CAG1C,MAAM,aAAa,GAChB,QACC;;;;;kEAMD,CACA,IAAI,eAAe,UAAU;AAOhC,KAAI,WAAW,WAAW,EAAG,QAAO,EAAE,QAAQ,GAAG;CAIjD,MAAM,gCAAgB,IAAI,KAAa;CACvC,MAAM,QAAQ,GACX,QAAQ,0CAA0C,CAClD,KAAK;AAER,MAAK,MAAM,QAAQ,OAAO;EACxB,MAAM,MAAM,KAAK,MAAM,KAAK,gBAAgB;AAC5C,OAAK,MAAM,MAAM,IACf,eAAc,IAAI,GAAG;;CAKzB,MAAM,UAAU,WAAW,QAAQ,MAAM,CAAC,cAAc,IAAI,EAAE,GAAG,CAAC;AAElE,KAAI,QAAQ,WAAW,EAAG,QAAO,EAAE,QAAQ,GAAG;CAG9C,MAAM,SAAS,IAAI,aAAa;CAChC,MAAM,iBAAiB,GAAG,QACxB,sDACD;AAWD,QAAO,EAAE,QATK,GAAG,kBAAkB;AACjC,OAAK,MAAM,OAAO,QAChB,gBAAe,IAAI,QAAQ,IAAI,GAAG;AAEpC,SAAO,QAAQ;GACf,EAEoB,EAEL;;;;;;;;;;;;;;;;;;;;;;;;;AC3bnB,eAAsB,YACpB,IACyB;CACzB,MAAM,6BAAY,IAAI,MAAM,EAAC,aAAa;CAC1C,MAAM,SAAmB,EAAE;CAC3B,IAAI,qBAAqB;CACzB,IAAI,uBAAuB;CAC3B,IAAI,sBAAsB;CAC1B,IAAI,iBAAiB;AAGrB,KAAI;AACF,sBAAoB,GAAG;UAChB,KAAK;AACZ,SAAO,KAAK,gBAAgB,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,GAAG;;AAMjF,KAAI;EACF,MAAM,WAAW,sBAAsB,GAAG;AAC1C,OAAK,MAAM,WAAW,SACpB,KAAI;GACF,MAAM,SAAS,wBAAwB,IAAI,QAAQ;AACnD,yBAAsB,OAAO,WAAW;WACjC,KAAK;AACZ,UAAO,KACL,yBAAyB,QAAQ,SAAS,KAAK,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,GAChG;;UAGE,KAAK;AACZ,SAAO,KACL,mBAAmB,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,GACpE;;AAMH,KAAI;EACF,MAAM,aAAa,sBAAsB,GAAG;AAC5C,OAAK,MAAM,SAAS,YAAY;AAC9B,OAAI,MAAM,SAAS,SAAS,EAAG;GAG/B,MAAM,SAAS,CAAC,GAAG,MAAM,SAAS,CAAC,MAChC,GAAG,MAAM,EAAE,gBAAgB,SAAS,EAAE,gBAAgB,OACxD;GACD,MAAM,SAAS,OAAO,GAAG;AAEzB,QAAK,IAAI,IAAI,GAAG,IAAI,OAAO,QAAQ,IACjC,KAAI;AACF,kBAAc,IAAI,QAAQ,OAAO,GAAG,GAAG;AACvC;YACO,KAAK;AACZ,WAAO,KACL,UAAU,OAAO,GAAG,KAAK,MAAM,OAAO,GAAG,KAAK,KAAK,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,GACpG;;;UAIA,KAAK;AACZ,SAAO,KACL,mBAAmB,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,GACpE;;AAMH,KAAI;EACF,MAAM,YAAY,KAAK,MAAM,kBAAkB,GAAI;EACnD,MAAM,WAAW,GACd,QAAQ,6BAA6B,CACrC,KAAK;AAER,OAAK,MAAM,OAAO,SAChB,KAAI;AAEF,OADe,kBAAkB,IAAI,IAAI,GAAG,GAC/B,UACX,kBAAiB,IAAI,IAAI,GAAG;WAEvB,KAAK;AACZ,UAAO,KACL,oBAAoB,IAAI,GAAG,KAAK,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,GACjF;;UAGE,KAAK;AACZ,SAAO,KACL,yBAAyB,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,GAC1E;;AAMH,KAAI;EAEF,MAAM,cAAc,GACjB,QACC,8EACD,CACA,KAAK;EAGR,MAAM,gCAAgB,IAAI,KAAa;AACvC,MAAI;GACF,MAAM,WAAW,GACd,QAAQ,gEAAgE,CACxE,KAAK;AACR,QAAK,MAAM,OAAO,SAChB,eAAc,IAAI,IAAI,eAAe;UAEjC;AAIR,OAAK,MAAM,QAAQ,YACjB,KAAI;GACF,MAAM,UAAU,gBAAgB,IAAI,KAAK,GAAG;AAC5C,QAAK,MAAM,UAAU,QAEnB,KAAI,CAAC,cAAc,IAAI,OAAO,iBAAiB,GAAG,EAAE;AAClD,yBAAqB,IAAI,OAAO,iBAAiB,IAAI,OAAO,OAAO;AACnE,kBAAc,IAAI,OAAO,iBAAiB,GAAG;AAC7C;;WAGG,KAAK;AACZ,UAAO,KACL,mBAAmB,KAAK,GAAG,KAAK,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,GACjF;;UAGE,KAAK;AACZ,SAAO,KACL,uBAAuB,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,GACxE;;AAMH,KAAI;AAEF,mBADe,cAAc,GAAG,CACR;UACjB,KAAK;AACZ,SAAO,KACL,mBAAmB,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,GACpE;;CAKH,MAAM,SAAyB;EAC7B;EACA,8BAJkB,IAAI,MAAM,EAAC,aAAa;EAK1C;EACA;EACA;EACA;EACA;EACD;AAED,SAAQ,OAAO,MACb,uCAAuC,mBAAmB,WAAW,qBAAqB,YAAY,oBAAoB,kBAAkB,eAAe,WAC5J;AAED,QAAO;;;;;;;;AAaT,IAAa,gBAAb,MAA2B;CACzB,AAAQ;CACR,AAAQ;CACR,AAAQ;CACR,AAAQ,UAAmB;CAC3B,AAAQ,UAAyB;CACjC,AAAQ,QAA+C;CAEvD,YACE,IACA,MAIA;AACA,OAAK,KAAK;AACV,OAAK,aAAa,MAAM,cAAc;AACtC,OAAK,aAAa,MAAM;;;;;CAM1B,QAAc;AACZ,MAAI,KAAK,QAAS;AAElB,OAAK,UAAU;AACf,OAAK,QAAQ,kBAAkB;AAC7B,GAAK,KAAK,SAAS;KAClB,KAAK,WAAW;AAEnB,UAAQ,OAAO,MACb,gDAAgD,KAAK,WAAW,MACjE;;;;;CAMH,OAAa;AACX,MAAI,KAAK,OAAO;AACd,iBAAc,KAAK,MAAM;AACzB,QAAK,QAAQ;;AAEf,OAAK,UAAU;AAEf,UAAQ,OAAO,MAAM,sCAAsC;;;;;CAM7D,MAAM,UAAmC;EACvC,MAAM,SAAS,MAAM,YAAY,KAAK,GAAG;AACzC,OAAK,UAAU,OAAO;AAEtB,MAAI,KAAK,WACP,MAAK,WAAW,OAAO;AAGzB,SAAO;;;;;CAMT,YAAqB;AACnB,SAAO,KAAK;;;;;CAMd,aAA4B;AAC1B,SAAO,KAAK;;;;;;;;;;;;;;;;;;;;;;;;;AC7ShB,MAAM,0BAAU,IAAI,KAAgB;AAEpC,IAAI,kBAAkB;AAMtB,IAAI,cAAc;AAQlB,MAAM,mBAAmB;AACzB,MAAM,kBAAmC,EAAE;;;;AAK3C,SAAS,iBAAiB,OAA4B;AACpD,KAAI,gBAAgB,UAAU,iBAC5B,iBAAgB,OAAO;AAEzB,iBAAgB,KAAK,MAAM;;;;;AAM7B,SAAS,eAAe,SAAkC;AACxD,QAAO,gBAAgB,QAAQ,MAAM,EAAE,KAAK,QAAQ;;AAOtD,SAAS,UAAU,OAAe,MAAc,IAAqB;CACnE,IAAI,MAAM;AACV,KAAI,OAAO,OACT,QAAO,OAAO,GAAG;AAEnB,QAAO,UAAU,MAAM,UAAU,KAAK;AACtC,QAAO;;AAGT,SAAS,aAAa,QAAmB,OAAe,MAAc,IAAsB;AAC1F,KAAI;EACF,MAAM,UAAU,UAAU,OAAO,MAAM,GAAG;AAC1C,SAAO,WAAW,QAAQ,IAAI,aAAa,CAAC,OAAO,QAAQ,CAAC;AAC5D,SAAO;SACD;AAEN,SAAO;;;AAQX,MAAa,YAAY,IAAI,MAAM;;;;;;;;;AAUnC,UAAU,IAAI,SAAS,MAAe;CACpC,MAAM,WAAW,OAAO,EAAE,gBAAgB;CAC1C,MAAM,oBAAoB,EAAE,IAAI,OAAO,gBAAgB;CACvD,MAAM,eAAe,oBAAoB,SAAS,mBAAmB,GAAG,GAAG;CAE3E,IAAI;CAEJ,MAAM,SAAS,IAAI,eAAe;EAChC,MAAM,YAAY;GAEhB,MAAM,iBAAiB,kBAAkB;AAEvC,QAAI,CADO,aAAa,QAAQ,aAAa,KAAK,UAAU,EAAE,WAAW,KAAK,KAAK,EAAE,CAAC,CAAC,EAC9E;AACP,mBAAc,eAAe;AAC7B,aAAQ,OAAO,OAAO;AACtB,WAAM,MAAM,wCAAwC,EAAE,UAAU,CAAC;;MAElE,IAAO;AAEV,YAAS;IAAE,IAAI;IAAU;IAAY;IAAgB;AACrD,WAAQ,IAAI,OAAO;AAEnB,SAAM,MAAM,wBAAwB;IAAE;IAAU,OAAO,QAAQ;IAAM,CAAC;AAGtE,gBAAa,QAAQ,aAAa,KAAK,UAAU;IAC/C,WAAW,KAAK,KAAK;IACrB;IACD,CAAC,CAAC;AAGH,OAAI,eAAe,GAAG;IACpB,MAAM,SAAS,eAAe,aAAa;AAC3C,SAAK,MAAM,SAAS,OAClB,cAAa,QAAQ,MAAM,OAAO,MAAM,MAAM,MAAM,GAAG;AAEzD,QAAI,OAAO,SAAS,EAClB,OAAM,MAAM,8BAA8B;KAAE;KAAU,OAAO,OAAO;KAAQ,SAAS;KAAc,CAAC;;;EAI1G,SAAS;AAEP,OAAI,QAAQ;AACV,kBAAc,OAAO,eAAe;AACpC,YAAQ,OAAO,OAAO;AACtB,UAAM,MAAM,2BAA2B;KAAE;KAAU,OAAO,QAAQ;KAAM,CAAC;;;EAG9E,CAAC;AAEF,QAAO,IAAI,SAAS,QAAQ,EAC1B,SAAS;EACP,gBAAgB;EAChB,iBAAiB;EACjB,cAAc;EACd,qBAAqB;EACtB,EACF,CAAC;EACF;;;;;;;;;;;;;;AAmBF,SAAgB,UAAU,OAAe,MAAoB;CAC3D,MAAM,UAAU,EAAE;CAClB,MAAM,OAAO,KAAK,UAAU,KAAK;AAGjC,kBAAiB;EAAE,IAAI;EAAS;EAAO,MAAM;EAAM,CAAC;AAEpD,KAAI,QAAQ,SAAS,EAAG;CAExB,MAAM,OAAoB,EAAE;AAE5B,MAAK,MAAM,UAAU,QAEnB,KAAI,CADO,aAAa,QAAQ,OAAO,MAAM,QAAQ,CAEnD,MAAK,KAAK,OAAO;AAKrB,MAAK,MAAM,UAAU,MAAM;AACzB,gBAAc,OAAO,eAAe;AACpC,UAAQ,OAAO,OAAO;;AAGxB,KAAI,KAAK,SAAS,EAChB,OAAM,MAAM,sCAAsC;EAAE,MAAM,KAAK;EAAQ,WAAW,QAAQ;EAAM,CAAC;;;;;;;;;;;;;;ACpJrG,SAAS,MAAM,GAA8D;AAC3E,QAAO,EAAE,IAAI,KAAK;;AAOpB,MAAa,YAAY,IAAI,MAAM;;;;;;;;;AAUnC,UAAU,IAAI,WAAW,MAAM;CAC7B,MAAM,KAAK,MAAM,EAAE;CACnB,MAAM,aAAa,EAAE,IAAI,MAAM,OAAO;CACtC,MAAM,cAAc,EAAE,IAAI,MAAM,QAAQ;CACxC,MAAM,cAAc,EAAE,IAAI,MAAM,QAAQ;CAGxC,IAAI,WAAW;CACf,MAAM,aAAwB,EAAE;CAChC,MAAM,iBAA2B,EAAE;AAEnC,KAAI,YAAY;EACd,MAAM,QAAQ,WAAW,MAAM,IAAI,CAAC,KAAI,MAAK,EAAE,MAAM,CAAC,CAAC,OAAO,QAAQ;AACtE,MAAI,MAAM,SAAS,GAAG;AACpB,kBAAe,KAAK,YAAY,MAAM,UAAU,IAAI,CAAC,KAAK,KAAK,CAAC,GAAG;AACnE,cAAW,KAAK,GAAG,MAAM;;;AAI7B,KAAI,aAAa;AACf,iBAAe,KAAK,kBAAkB;AACtC,aAAW,KAAK,YAAY;;AAG9B,KAAI,aAAa;AACf,iBAAe,KAAK,kBAAkB;AACtC,aAAW,KAAK,YAAY;;AAG9B,KAAI,eAAe,SAAS,EAC1B,aAAY,YAAY,eAAe,KAAK,QAAQ;AAGtD,aAAY;CAEZ,IAAI;AACJ,KAAI;AACF,aAAW,GAAG,QAAQ,SAAS,CAAC,IAAI,GAAG,WAAW;SAC5C;AACN,aAAW,EAAE;;CAGf,MAAM,QAAQ,SAAS,KAAI,SAAQ;EACjC,IAAI,IAAI;EACR,OAAO,IAAI;EACX,MAAM,IAAI;EACV,kBAAkB,mBAAmB,IAAI,gBAAgB,CAAC;EAC1D,WAAW,IAAI;EAChB,EAAE;CAGH,IAAI;AACJ,KAAI;AAQF,aAAW,GAAG,QAPG;;;;;;MAOc,CAAC,KAAK;SAC/B;AACN,aAAW,EAAE;;CAIf,MAAM,YAAY,IAAI,IAAI,MAAM,KAAI,MAAK,EAAE,GAAG,CAAC;CAK/C,MAAM,SAJgB,aAClB,SAAS,QAAO,MAAK,UAAU,IAAI,EAAE,UAAU,IAAI,UAAU,IAAI,EAAE,UAAU,CAAC,GAC9E,UAEwB,KAAI,SAAQ;EACtC,IAAI,IAAI;EACR,QAAQ,IAAI;EACZ,QAAQ,IAAI;EACZ,MAAM,IAAI;EACV,OAAO,IAAI,QAAQ,IAAI;EACxB,EAAE;AAEH,QAAO,EAAE,KAAK;EAAE;EAAO;EAAO,CAAC;EAC/B;;;;;;;;;;AAWF,UAAU,IAAI,cAAc,MAAM;CAChC,MAAM,KAAK,MAAM,EAAE;CACnB,MAAM,OAAO,EAAE,IAAI,MAAM,OAAO;CAChC,MAAM,KAAK,EAAE,IAAI,MAAM,KAAK;CAC5B,MAAM,WAAW,EAAE,IAAI,MAAM,QAAQ;CACrC,MAAM,QAAQ,WAAW,KAAK,IAAI,SAAS,UAAU,GAAG,IAAI,KAAK,IAAK,GAAG;CACzE,MAAM,YAAY,EAAE,IAAI,MAAM,SAAS;CACvC,MAAM,SAAS,YAAY,KAAK,IAAI,SAAS,WAAW,GAAG,IAAI,GAAG,EAAE,GAAG;CAGvE,IAAI,WAA+H,EAAE;AACrI,KAAI;EACF,IAAI,cAAc;EAClB,MAAM,gBAA2B,EAAE;EACnC,MAAM,eAAyB,EAAE;AAEjC,MAAI,MAAM;AACR,gBAAa,KAAK,kBAAkB;AACpC,iBAAc,KAAK,KAAK;;AAE1B,MAAI,IAAI;AACN,gBAAa,KAAK,sCAAsC;AACxD,iBAAc,KAAK,GAAG;;AAGxB,MAAI,aAAa,SAAS,EACxB,gBAAe,YAAY,aAAa,KAAK,QAAQ;AAEvD,iBAAe;AACf,gBAAc,KAAK,OAAO;EAE1B,MAAM,cAAc,GAAG,QAAQ,YAAY,CAAC,IAAI,GAAG,cAAc;EAGjE,MAAM,YAAY,GAAG,QACnB,uFACD;AAED,aAAW,YAAY,KAAI,QAAO;GAChC,IAAI,WAAW;AACf,OAAI;AAEF,eADiB,UAAU,IAAI,IAAI,GAAG,EACjB,OAAO;WACtB;AAER,UAAO;IACL,IAAI,IAAI;IACR,WAAW,IAAI;IACf,SAAS,IAAI;IACb,kBAAkB;IAClB,SAAS,IAAI;IACd;IACD;SACI;CAGR,IAAI,eAA+G,EAAE;AACrH,KAAI;EACF,IAAI,SAAS;EACb,MAAM,YAAuB,EAAE;AAE/B,MAAI,MAAM;AACR,aAAU;AACV,aAAU,KAAK,KAAK;;AAEtB,MAAI,IAAI;AACN,aAAU;AACV,aAAU,KAAK,GAAG;;AAGpB,YAAU;AACV,YAAU,KAAK,MAAM;AACrB,YAAU,KAAK,OAAO;AAItB,iBAFgB,GAAG,QAAQ,OAAO,CAAC,IAAI,GAAG,UAAU,CAE7B,KAAI,SAAQ;GACjC,IAAI,IAAI;GACR,MAAM,IAAI,QAAQ,GAAG,IAAI,MAAM,IAAI,IAAI,YAAY,IAAI;GACvD,WAAW,IAAI;GACf,WAAW,IAAI;GACf,MAAM,IAAI;GACX,EAAE;SACG;CAGR,IAAI,cAAqI,EAAE;AAC3I,KAAI;EACF,IAAI,WAAW;EACf,MAAM,cAAyB,EAAE;AAEjC,MAAI,MAAM;AACR,eAAY;AACZ,eAAY,KAAK,KAAK;;AAExB,MAAI,IAAI;AACN,eAAY;AACZ,eAAY,KAAK,GAAG;;AAGtB,cAAY;AAIZ,gBAFkB,GAAG,QAAQ,SAAS,CAAC,IAAI,GAAG,YAAY,CAElC,KAAI,SAAQ;GAClC,IAAI,IAAI;GACR,WAAW;GACX,SAAS;GACT,WAAW,IAAI;GACf,YAAY,IAAI;GACjB,EAAE;SACG;AAER,QAAO,EAAE,KAAK;EAAE;EAAU;EAAc;EAAa,CAAC;EACtD;;;;;;;AAQF,UAAU,IAAI,cAAc,MAAM;CAChC,MAAM,KAAK,MAAM,EAAE;CACnB,MAAM,SAAS,EAAE,IAAI,MAAM,KAAK;CAahC,IAAI;AACJ,KAAI;AACF,YAAU,GAAG,QACX,yGACD,CAAC,IAAI,OAAO;SACP;AAER,KAAI,CAAC,QACH,QAAO,EAAE,KAAK,EAAE,OAAO,kBAAkB,EAAE,IAAI;CAGjD,MAAM,SAAS;EACb,IAAI,QAAQ;EACZ,OAAO,QAAQ;EACf,MAAM,QAAQ;EACd,WAAW,QAAQ;EACnB,WAAW,QAAQ;EACnB,UAAU,cAAc,QAAQ,SAAS;EAC1C;CAGD,MAAM,iBAAiB,mBAAmB,QAAQ,gBAAgB;CAClE,IAAI,mBAA2E,EAAE;AAEjF,KAAI,eAAe,SAAS,EAC1B,KAAI;EACF,MAAM,eAAe,eAAe,UAAU,IAAI,CAAC,KAAK,KAAK;AAK7D,qBAJgB,GAAG,QACjB,wEAAwE,aAAa,mDACtF,CAAC,IAAI,GAAG,eAAe,CAEG,KAAI,SAAQ;GACrC,IAAI,IAAI;GACR,MAAM,IAAI,QAAQ,GAAG,IAAI,MAAM,IAAI,IAAI,YAAY,IAAI;GACvD,WAAW,IAAI;GAChB,EAAE;SACG;CAgBV,IAAI,gBAA+G,EAAE;AACrH,KAAI;AAaF,kBAZgB,GAAG,QAAQ;;;;;;;;;;MAUzB,CAAC,IAAI,QAAQ,OAAO,CAEE,KAAI,QAAO;GACjC,MAAM,WAAW,IAAI,cAAc;AACnC,UAAO;IACL,IAAI,IAAI;IACR,UAAU,WAAW,IAAI,YAAY,IAAI;IACzC,aAAa,WAAY,IAAI,eAAe,IAAI,YAAc,IAAI,eAAe,IAAI;IACrF,MAAM,IAAI;IACV,WAAW,WAAW,aAAa;IACpC;IACD;SACI;AAER,QAAO,EAAE,KAAK;EAAE;EAAQ,cAAc;EAAkB;EAAe,CAAC;EACxE;AAMF,SAAS,mBAAmB,MAAwB;AAClD,KAAI;EACF,MAAM,SAAS,KAAK,MAAM,KAAK;AAC/B,SAAO,MAAM,QAAQ,OAAO,GAAG,SAAS,EAAE;SACpC;AACN,SAAO,EAAE;;;AAIb,SAAS,cAAc,MAAuC;AAC5D,KAAI;AACF,SAAO,KAAK,MAAM,KAAK;SACjB;AACN,SAAO,EAAE;;;;;;;;;;;;;;;;;;;;;;AC5Xb,SAAgB,gBAAgB,IAAkC;CAChE,MAAM,MAAM,IAAI,MAAM;AAGtB,KAAI,IACF,KACA,KAAK,EACH,SAAS,WAAW;AAClB,MAAI,CAAC,OAAQ,QAAO;AACpB,MAAI,OAAO,WAAW,oBAAoB,IAAI,OAAO,WAAW,oBAAoB,CAClF,QAAO;AAET,SAAO;IAEV,CAAC,CACH;AAGD,KAAI,IAAI,KAAK,OAAO,GAAG,SAAS;AAC9B,IAAE,IAAI,MAAM,GAAG;AACf,QAAM,MAAM;GACZ;AAGF,KAAI,IAAI,gBAAgB,MAAM;AAC5B,SAAO,EAAE,KAAK;GAAE,QAAQ;GAAM,WAAW,KAAK,KAAK;GAAE,CAAC;GACtD;AAGF,KAAI,MAAM,QAAQ,UAAU;AAC5B,KAAI,MAAM,QAAQ,UAAU;AAG5B,KAAI,IACF,MACA,YAAY,EACV,MAAM,SACP,CAAC,CACH;AAGD,KAAI,IAAI,KAAK,YAAY;EAAE,MAAM;EAAS,MAAM;EAAc,CAAC,CAAC;AAEhE,QAAO;;;;;;;;;AAUT,SAAgB,eACd,KACA,OAAe,OACW;AAC1B,OAAM,MAAM,+BAA+B,OAAO;CAElD,MAAM,SAAS,MAAM;EACnB,OAAO,IAAI;EACX;EACD,CAAC;AAEF,OAAM,MAAM,4CAA4C,OAAO;AAE/D,QAAO;;;;;AC5DT,MAAM,KAAK,aAAa,mBAAmB,CAAC;AAC5C,gBAAgB,GAAG,GAAG;AACtB,MAAM,cAAc,eAAe,QAAQ,KAAK,CAAC;AAMjD,MAAM,iBAAiB,GAAG,mBACtB,IAAI,eAAe,GAAG,IAAI,YAAY,GACtC;AAEJ,MAAM,SAAS,IAAI,gBAAgB;AAGf,OAAO,OAAO,CAAC,YAAY;AAC7C,OAAM,OAAO,4CAA4C;EACzD;AASF,MAAM,cAAc,0BAA0B;AAC9C,MAAM,WAAW,IAAI,oBAAoB;AACzC,MAAM,kBAAkB,IAAI,yBAAyB;CACnD,uBAAuB,YAAY;CACnC,OAAO,YAAY;CACpB,CAAC;AACF,YAAY,aAAa,UAAU,gBAAgB;AAInD,MAAM,iBADiB,IAAI,eAAe,GAAG,GAAG,CACV,mBAAmB,YAAY;AACrE,IAAI,gBAAgB;AAClB,iBAAgB,gBAAgB,eAAe,iBAAiB,eAAe,gBAAgB;AAC/F,aAAY,aAAa,UAAU,gBAAgB;;AAGrD,MAAM,eAAe,IAAI,aAAa,GAAG,GAAG;AAC5C,MAAM,iBAAiB,IAAI,yBAAyB,GAAG,GAAG;AAC1D,MAAM,oBAAoB,IAAI,kBAAkB,GAAG,GAAG;AAGtD,MAAM,oBAAoB,IAAI,kBAAkB;CAC9C;CACA;CACA,kBAL+B,IAAI,sBAAsB,GAAG,IAAI,YAAY;CAM5E,QAAQ;CACR;CACA;CACD,CAAC;AAMF,eAAe,oBAAmC;AAChD,KAAI,CAAC,kBAAkB,CAAC,OAAO,SAAS,CAAE;CAE1C,MAAM,MAAM,eAAe,eAAe,GAAG;AAC7C,KAAI,IAAI,WAAW,EAAG;CAEtB,MAAM,UAAU,IAAI,sBAAsB,GAAG,IAAI,YAAY;AAE7D,MAAK,MAAM,MAAM,KAAK;EACpB,MAAM,MAAM,QAAQ,QAAQ,GAAG;AAC/B,MAAI,CAAC,IAAK;EAEV,MAAM,OAAO,IAAI,QAAQ,GAAG,IAAI,MAAM,IAAI,IAAI,YAAY,IAAI;EAC9D,MAAM,YAAY,MAAM,OAAO,MAAM,KAAK;AAE1C,MAAI,WAAW;AACb,kBAAe,MAAM,IAAI,UAAU;AACnC,WAAQ,OAAO,IAAI;IACjB,gBAAgB,OAAO,eAAe;IACtC,kBAAkB;IACnB,CAAC;AAMF,aAAU,mBAAmB;IAC3B;IACA,MALoB,IAAI,QAAQ,SAAS,MACvC,IAAI,QAAQ,UAAU,GAAG,IAAI,GAAG,QAChC,IAAI;IAIN,WAAW,IAAI,aAAa;IAC5B,WAAW,IAAI;IAChB,CAAC;AAGF,OAAI,YAAY,QACd,KAAI;IAEF,MAAM,mBAAmB;KAAE,GAAG;KAAK;KAAW;IAC9C,MAAM,SAAS,MAAM,kBAAkB,kBACrC,kBACA,IAAI,aAAa,WACjB,YACD;AACD,QAAI,OAAO,WAAW,OAAO,cAAc;AACzC,uBAAkB,IAAI,aAAa,OAAO,aAAa;AACvD,WAAM,SAAS,6CAA6C,EAAE,IAAI,CAAC;AAGnE,eAAU,eAAe;MACvB,IAAI,OAAO,aAAa,UAAU,GAAG,GAAG;MACxC,WAAW;MACX,SAAS;MACT,4BAAW,IAAI,MAAM,EAAC,aAAa;MACnC,YAAY;MACb,CAAC;;YAEG,UAAU;AAEjB,UAAM,SAAS,2CAA2C,EAAE,OADhD,oBAAoB,QAAQ,SAAS,UAAU,OAAO,SAAS,EACH,CAAC;;AAK7E,OAAI;IACF,MAAM,QAAQ,kBAAkB,GAAG,IAAI,MAAM,OAAO,GAAG,CAAC;AACxD,QAAI,MAAM,SAAS,GAAG;KACpB,MAAM,cAAc,MAAM,KAAI,OAAM;MAClC,MAAM,EAAE;MACR,MAAM,EAAE;MACT,EAAE;AACH,sBAAiB,GAAG,IAAI,MAAM,YAAY;AAC1C,WAAM,SAAS,iBAAiB;MAC9B;MACA,UAAU,MAAM;MACjB,CAAC;AAGF,UAAK,MAAM,QAAQ,MACjB,WAAU,kBAAkB;MAC1B,IAAI,KAAK;MACT,OAAO,KAAK;MACZ,MAAM,KAAK;MACX,kBAAkB;MAClB,4BAAW,IAAI,MAAM,EAAC,aAAa;MACpC,CAAC;;YAGC,UAAU;AAEjB,UAAM,SAAS,sCAAsC,EAAE,OAD3C,oBAAoB,QAAQ,SAAS,UAAU,OAAO,SAAS,EACR,CAAC;;;;;AAO5E,MAAM,aAAa,kBAAkB;AACnC,oBAAmB,CAAC,OAAO,QAAQ;AAEjC,QAAM,SAAS,8BAA8B,EAAE,OAD/B,eAAe,QAAQ,IAAI,UAAU,OAAO,IAAI,EACD,CAAC;GAChE;GACD,IAAK;AAMR,MAAM,SAAS,cAAc;AAC7B,mBAAmB,QAAQ,GAAG,IAAI,aAAa,kBAAkB;AACjE,eAAe,QAAQ,GAAG,IAAI,aAAa,QAAQ,gBAAgB,kBAAkB;AACrF,qBAAqB,QAAQ,GAAG,IAAI,aAAa,kBAAkB;AACnE,mBAAmB,QAAQ,GAAG,IAAI,aAAa,kBAAkB;AACjE,mBAAmB,QAAQ,GAAG,IAAI,aAAa,kBAAkB;AAEjE,YAAY,OAAO,CAAC,OAAO,QAAQ;AACjC,OAAM,OAAO,iCAAiC,EAAE,OAAO,IAAI,SAAS,CAAC;AACrE,eAAc,WAAW;AACzB,IAAG,OAAO;AACV,SAAQ,KAAK,EAAE;EACf;AAMF,MAAM,UAAU,SAAS,QAAQ,IAAI,qBAAqB,SAAS,GAAG;AAEtE,eADe,gBAAgB,GAAG,GAAG,EACd,QAAQ;AAM/B,MAAM,gBAAgB,IAAI,cAAc,GAAG,IAAI;CAC7C,YAAY,MAAS;CACrB,aAAa,WAAW;AACtB,QAAM,MAAM,qBAAqB;GAC/B,QAAQ,OAAO;GACf,SAAS,OAAO;GAChB,OAAO,OAAO;GACd,QAAQ,OAAO;GAChB,CAAC;;CAEL,CAAC;AACF,cAAc,OAAO;AAMrB,QAAQ,GAAG,gBAAgB;AACzB,eAAc,WAAW;AACzB,eAAc,MAAM;AACpB,QAAO,UAAU,CAAC,YAAY,GAAG;AACjC,IAAG,OAAO;AACV,SAAQ,KAAK,EAAE;EACf;AACF,QAAQ,GAAG,iBAAiB;AAC1B,eAAc,WAAW;AACzB,eAAc,MAAM;AACpB,QAAO,UAAU,CAAC,YAAY,GAAG;AACjC,IAAG,OAAO;AACV,SAAQ,KAAK,EAAE;EACf;AACF,QAAQ,GAAG,sBAAsB,QAAQ;AACvC,OAAM,OAAO,sBAAsB,EAAE,OAAO,IAAI,SAAS,CAAC;AAC1D,eAAc,WAAW;AACzB,eAAc,MAAM;AACpB,QAAO,UAAU,CAAC,YAAY,GAAG;AACjC,IAAG,OAAO;AACV,SAAQ,KAAK,EAAE;EACf"}