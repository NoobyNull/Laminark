---
phase: 04-embedding-engine-and-semantic-search
plan: 03
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/analysis/worker.ts
  - src/analysis/worker-bridge.ts
autonomous: true

must_haves:
  truths:
    - "Embedding generation runs in a separate worker thread and never blocks the main thread"
    - "Main thread sends observations to worker via postMessage and receives embeddings back"
    - "Worker initializes embedding engine lazily on first observation, not at worker start"
    - "Worker crash or initialization failure does not crash the main process"
    - "Plugin startup creates the worker but does NOT trigger model loading"
  artifacts:
    - path: "src/analysis/worker.ts"
      provides: "Worker thread entry point that runs embedding engine"
      exports: []
    - path: "src/analysis/worker-bridge.ts"
      provides: "Main-thread API for communicating with the analysis worker"
      exports: ["AnalysisWorker"]
  key_links:
    - from: "src/analysis/worker-bridge.ts"
      to: "src/analysis/worker.ts"
      via: "new Worker() with worker_threads"
      pattern: "new Worker.*worker\\.ts"
    - from: "src/analysis/worker.ts"
      to: "src/analysis/embedder.ts"
      via: "createEmbeddingEngine() called on first observation"
      pattern: "createEmbeddingEngine"
    - from: "src/analysis/worker-bridge.ts"
      to: "main thread consumers"
      via: "Promise-based embed() returning Float32Array or null"
      pattern: "embed.*Promise.*Float32Array"
---

<objective>
Set up the worker thread that runs embedding generation off the main thread, with a bridge class that provides a clean async API for main-thread consumers.

Purpose: This is the non-blocking guarantee (INT-04). MCP tool responses and Claude's output are never delayed by embedding computation. The worker also handles lazy model loading (DQ-04) -- the ONNX model loads on first use, not at startup.

Output: A worker thread entry point and a main-thread bridge class with Promise-based embed/embedBatch methods.
</objective>

<execution_context>
@/home/matthew/.claude/get-shit-done/workflows/execute-plan.md
@/home/matthew/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/ARCHITECTURE.md
@.planning/phases/04-embedding-engine-and-semantic-search/04-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create worker thread entry point with lazy engine initialization</name>
  <files>src/analysis/worker.ts</files>
  <action>
Create `src/analysis/worker.ts` as a worker_threads worker entry point using `parentPort` from `node:worker_threads`.

**Message protocol (worker receives):**
```typescript
type WorkerMessage =
  | { type: "embed"; id: string; text: string }
  | { type: "embedBatch"; id: string; texts: string[] }
  | { type: "shutdown" };
```

**Message protocol (worker sends):**
```typescript
type WorkerResponse =
  | { type: "embed_result"; id: string; embedding: Float32Array | null; engineName: string }
  | { type: "embedBatch_result"; id: string; embeddings: (Float32Array | null)[]; engineName: string }
  | { type: "ready"; engineName: string; dimensions: number }
  | { type: "error"; id: string; error: string };
```

**Initialization flow:**
1. Worker starts with NO engine loaded. Sets `engine = null`.
2. On first "embed" or "embedBatch" message, calls `createEmbeddingEngine()` from `src/analysis/embedder.ts`.
3. After engine initializes, sends a "ready" message with engine name and dimensions.
4. Then processes the queued embed request.
5. All subsequent embed requests use the cached engine instance.

**Key behaviors:**
- If `createEmbeddingEngine()` falls back to KeywordOnlyEngine, the worker still functions (returning null embeddings). This is expected for DQ-03.
- Wrap all processing in try/catch. On error, send an "error" response with the message ID so the main thread can resolve the pending Promise.
- On "shutdown" message, call `process.exit(0)`.
- Use `parentPort.on("message", handler)` for receiving.
- Use `parentPort.postMessage(response)` for sending.

**Important:** The worker file must be compilable as a standalone entry point. It imports from `../analysis/embedder.ts` which imports the engine files. Ensure the import paths work when the worker is instantiated with `new Worker("path/to/worker.ts")` or the compiled JS equivalent. Consider using `__filename` or `import.meta.url` for the worker path resolution.
  </action>
  <verify>
Run `npx tsc --noEmit`. Verify the worker file can be loaded as a module without errors: `node -e "require('./src/analysis/worker.ts')"` (this will fail at parentPort being null, but should not fail on imports).
  </verify>
  <done>Worker thread entry point handles embed/embedBatch/shutdown messages. Engine initializes lazily on first embed request. All errors caught and returned as error messages.</done>
</task>

<task type="auto">
  <name>Task 2: Create main-thread worker bridge with Promise-based API</name>
  <files>src/analysis/worker-bridge.ts</files>
  <action>
Create `src/analysis/worker-bridge.ts` with an `AnalysisWorker` class that wraps the worker thread with a clean async API.

**Constructor:**
- Does NOT start the worker immediately. Provides a `start()` method.

**start():**
- Creates `new Worker(workerPath)` where `workerPath` points to the compiled worker.ts.
- Resolve the worker path relative to the current file using `new URL("./worker.ts", import.meta.url)` or `path.join(__dirname, "worker.js")` depending on the module system.
- Set up message handler on the worker.
- Set up error handler: log the error, set `this.available = false`.
- Set up exit handler: if unexpected exit, log warning, set `this.available = false`.
- Returns a Promise that resolves when the worker is alive (not when the engine is ready -- engine initializes lazily).

**embed(text: string): Promise<Float32Array | null>**
- If worker not available, return `null` immediately.
- Generate a unique request ID (incrementing counter).
- Send `{ type: "embed", id, text }` to worker.
- Store a Promise resolver in a `Map<string, { resolve, reject }>` keyed by ID.
- When the worker responds with matching ID, resolve the Promise with the embedding.
- Add a timeout (10 seconds for first call which includes model loading, 5 seconds for subsequent). On timeout, resolve with `null` (not reject -- graceful degradation).

**embedBatch(texts: string[]): Promise<(Float32Array | null)[]>**
- Same pattern as embed() but for batch requests.

**shutdown(): Promise<void>**
- Send "shutdown" message.
- Wait for worker exit event.
- Clean up resources.

**isAvailable(): boolean**
- Returns whether the worker is alive and has not errored.

**getEngineInfo(): { name: string; dimensions: number } | null**
- Returns engine info from the "ready" message, or null if not yet initialized.

**Key design decisions:**
- The pending-requests Map ensures multiple concurrent embed calls don't interfere.
- Timeout ensures a hung worker never blocks callers indefinitely.
- `null` returns on failure (not exceptions) match the EmbeddingEngine contract.
- The bridge is the ONLY way main-thread code interacts with the worker. No direct worker_threads API elsewhere.
  </action>
  <verify>
Write an integration test:
1. Create AnalysisWorker, call start()
2. Call embed("test sentence")
3. Verify it returns either a Float32Array(384) or null (depending on ONNX availability)
4. Call shutdown()
5. Verify embed() returns null after shutdown

Run `npx tsc --noEmit`. Run the integration test.
  </verify>
  <done>AnalysisWorker provides Promise-based embed/embedBatch from main thread. Worker crash or timeout returns null, never throws. Multiple concurrent embed calls resolved correctly via request ID mapping. Shutdown is clean.</done>
</task>

<task type="auto">
  <name>Task 3: Wire worker into observation ingest pipeline</name>
  <files>src/analysis/worker-bridge.ts, src/storage/embeddings.ts</files>
  <action>
Add a `processObservation()` method to AnalysisWorker that orchestrates the full embedding pipeline for a single observation:

**processObservation(observation: { id: string; content: string }): Promise<{ observationId: string; embedding: Float32Array | null; engineName: string | null }>**

1. Call `this.embed(observation.content)`
2. Return the result with the observation ID and engine name
3. The CALLER (main thread) is responsible for writing the embedding to the database via EmbeddingStore. This maintains the single-writer pattern from the architecture (main thread writes, worker only computes).

Also add a `processObservationBatch()` method for batch processing:

**processObservationBatch(observations: Array<{ id: string; content: string }>): Promise<Array<{ observationId: string; embedding: Float32Array | null; engineName: string | null }>>**

1. Extract texts, call embedBatch()
2. Zip results back with observation IDs

**Main thread integration pattern** (document in code comments, actual wiring happens when the ingest pipeline from Phase 3 is updated):

```typescript
// In main thread (e.g., ingest handler):
const result = await analysisWorker.processObservation(observation);
if (result.embedding) {
  embeddingStore.storeEmbedding(result.observationId, result.embedding);
  // Update observation record with model_version = result.engineName
}
```

This pattern ensures:
- Embedding computation happens off main thread (INT-04)
- Database writes happen on main thread (single-writer pattern)
- Null embeddings are simply not stored (graceful degradation)
  </action>
  <verify>
Extend the integration test from Task 2:
1. Call processObservation({ id: "obs-1", content: "authentication decisions for the API" })
2. Verify result has observationId "obs-1"
3. If ONNX available: verify embedding is Float32Array(384) and engineName contains "onnx"
4. If ONNX unavailable: verify embedding is null and engineName contains "keyword"

Run `npx tsc --noEmit`.
  </verify>
  <done>processObservation and processObservationBatch provide the complete embedding pipeline. Caller receives observation ID + embedding + engine name. Null embeddings signal keyword-only fallback. Main thread remains responsible for database writes.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes
2. Worker thread starts without loading ONNX model
3. First embed() call triggers lazy model loading in the worker
4. embed() returns Float32Array(384) when ONNX available, null when not
5. Worker crash does not crash main process
6. Multiple concurrent embed() calls all resolve correctly
7. shutdown() cleanly terminates the worker
8. processObservation() returns structured results for database writing
</verification>

<success_criteria>
- Worker thread runs embedding computation completely off the main thread (INT-04)
- ONNX model loads lazily on first embed request, not at startup (DQ-04)
- Worker failure returns null embeddings, never crashes main process (DQ-03)
- Promise-based API with request-ID tracking handles concurrent calls
- processObservation provides the integration point for the ingest pipeline
</success_criteria>

<output>
After completion, create `.planning/phases/04-embedding-engine-and-semantic-search/04-03-SUMMARY.md`
</output>
