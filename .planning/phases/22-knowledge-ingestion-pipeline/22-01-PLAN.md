---
phase: 22-knowledge-ingestion-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ingestion/markdown-parser.ts
  - src/ingestion/knowledge-ingester.ts
  - src/ingestion/__tests__/markdown-parser.test.ts
  - src/ingestion/__tests__/knowledge-ingester.test.ts
autonomous: true
requirements:
  - FR-2.1
  - FR-2.2
  - FR-2.3
  - FR-2.4
  - FR-2.5

must_haves:
  truths:
    - "Markdown files with ## headings are split into discrete sections"
    - "Each section becomes a kind=reference observation with title and source tag"
    - "Re-running ingestion replaces stale sections without creating duplicates"
    - "Sections removed from source docs are cleaned up on re-ingestion"
    - "All ingested observations are scoped to the correct project"
  artifacts:
    - path: "src/ingestion/markdown-parser.ts"
      provides: "parseMarkdownSections function"
      exports: ["parseMarkdownSections", "ParsedSection"]
    - path: "src/ingestion/knowledge-ingester.ts"
      provides: "KnowledgeIngester class with ingestDirectory"
      exports: ["KnowledgeIngester", "IngestionStats"]
  key_links:
    - from: "src/ingestion/knowledge-ingester.ts"
      to: "src/storage/observations.ts"
      via: "ObservationRepository.createClassified and softDelete"
      pattern: "repo\\.createClassified|repo\\.softDelete"
    - from: "src/ingestion/knowledge-ingester.ts"
      to: "src/ingestion/markdown-parser.ts"
      via: "parseMarkdownSections import"
      pattern: "import.*parseMarkdownSections"
---

<objective>
Build the markdown section parser and knowledge ingester that transforms structured markdown documents into discrete, queryable per-project reference memories.

Purpose: This is the core ingestion pipeline -- FR-2.1 through FR-2.5. It takes markdown files (from .planning/codebase/ or .laminark/codebase/) and creates one observation per ## section, with idempotent re-ingestion that cleans up stale sections.

Output: Two source files (markdown-parser.ts, knowledge-ingester.ts) and their test files.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-knowledge-ingestion-pipeline/22-RESEARCH.md
@src/storage/observations.ts
@src/shared/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Markdown section parser</name>
  <files>src/ingestion/markdown-parser.ts, src/ingestion/__tests__/markdown-parser.test.ts</files>
  <action>
Create `src/ingestion/markdown-parser.ts` with:

1. Export `ParsedSection` interface:
   ```typescript
   export interface ParsedSection {
     title: string;        // Full title: "Technology Stack > Languages" (docTitle > heading)
     heading: string;      // Just the heading text: "Languages"
     content: string;      // Everything under heading until next ## or EOF
     sourceFile: string;   // Filename: "STACK.md"
     sectionIndex: number; // 0-based index within file
   }
   ```

2. Export `parseMarkdownSections(fileContent: string, sourceFile: string): ParsedSection[]`:
   - Split on `## ` (level 2) headings -- these are primary sections in GSD output
   - The `# ` (level 1) heading is the doc title -- use as prefix: `"DocTitle > SectionHeading"`
   - `### ` subsections stay within their parent `## ` section (not split separately)
   - Skip sections with empty content after trimming
   - Normalize: trim whitespace from heading text and content

Key decisions per RESEARCH.md:
- Split on `## ` only (not `### ` or deeper) for useful granularity (~5-10 sections per file)
- No markdown AST parser needed -- simple line-by-line iteration with heading detection
- Handle edge cases: files with no `## ` headings (return empty array), files with only `# ` title (return empty), content before first `## ` heading (skip it)

Create `src/ingestion/__tests__/markdown-parser.test.ts` with tests:
- Basic: file with # title and 3 ## sections -> 3 ParsedSection objects with correct titles
- No title: file without # heading -> sections use heading text only (no ">" prefix)
- Empty sections: ## heading followed immediately by another ## -> skipped
- Subsections: ### within ## stay in parent content
- No sections: file with only prose -> empty array
- Edge: trailing newlines, mixed whitespace, code blocks containing ## (don't split on them)

Use vitest (the project's test framework). Import from relative path.
  </action>
  <verify>Run `npx vitest run src/ingestion/__tests__/markdown-parser.test.ts` -- all tests pass</verify>
  <done>parseMarkdownSections correctly splits GSD-format markdown into ParsedSection objects; all test cases pass</done>
</task>

<task type="auto">
  <name>Task 2: Knowledge ingester with idempotent upsert</name>
  <files>src/ingestion/knowledge-ingester.ts, src/ingestion/__tests__/knowledge-ingester.test.ts</files>
  <action>
Create `src/ingestion/knowledge-ingester.ts` with:

1. Export `IngestionStats` interface:
   ```typescript
   export interface IngestionStats {
     filesProcessed: number;
     sectionsCreated: number;
     sectionsRemoved: number;
   }
   ```

2. Export `KnowledgeIngester` class:
   ```typescript
   export class KnowledgeIngester {
     constructor(db: BetterSqlite3.Database, projectHash: string)
     async ingestDirectory(dirPath: string): Promise<IngestionStats>
     async ingestFile(filePath: string): Promise<IngestionStats>
   }
   ```

Implementation details for `ingestDirectory`:
- Read directory, filter to `.md` files only
- For each file, call `ingestFile`
- Aggregate stats across all files
- Return total stats

Implementation details for `ingestFile`:
- Read file content with `fs/promises.readFile`
- Parse with `parseMarkdownSections(content, basename(filePath))`
- Source tag convention: `"ingest:{filename}"` (e.g., `"ingest:STACK.md"`)
- **Idempotent strategy (per RESEARCH.md Pitfall 4):**
  1. Read files async first, then run all DB operations in a single `db.transaction()`
  2. Inside transaction: soft-delete ALL existing observations with matching `source` and `project_hash` using direct SQL: `UPDATE observations SET deleted_at = datetime('now'), updated_at = datetime('now') WHERE project_hash = ? AND source = ? AND deleted_at IS NULL`
  3. Count how many were soft-deleted for stats.sectionsRemoved
  4. Then create new observations for each parsed section using `ObservationRepository.createClassified()` with `classification: 'discovery'` (per RESEARCH.md -- bypasses noise filter, makes immediately searchable)
  5. Each observation: `kind: 'reference'`, `title: section.title`, `source: "ingest:{filename}"`, `sessionId: null` (not a user conversation)
- Read all files async first into a Map of filename to content, then run the sync transaction over the pre-read contents.

Detection logic for `ingestDirectory`:
- Accept explicit directory path
- Also implement static helper `KnowledgeIngester.detectKnowledgeDir(projectRoot: string): string | null` that checks in order:
  1. `{projectRoot}/.planning/codebase/` (GSD output)
  2. `{projectRoot}/.laminark/codebase/`
  Returns first existing directory, or null

Create `src/ingestion/__tests__/knowledge-ingester.test.ts`:
- Use an in-memory SQLite database (same pattern as other tests in the project)
- Run migrations on the test DB so observations table exists
- Test: ingest a directory with 2 markdown files -> correct IngestionStats, observations created with kind=reference and source=ingest:{filename}
- Test: re-ingest same directory -> old observations soft-deleted, new ones created (idempotent)
- Test: file removed between ingestions -> its observations get cleaned up
- Test: empty directory -> 0 stats, no errors
- Test: detectKnowledgeDir returns correct path or null

Import ObservationRepository from `../../storage/observations.js`, use `../../storage/index.js` for DB setup/migrations if available, otherwise set up manually.
  </action>
  <verify>Run `npx vitest run src/ingestion/__tests__/knowledge-ingester.test.ts` -- all tests pass</verify>
  <done>KnowledgeIngester.ingestDirectory reads markdown files, creates per-section reference observations, and handles re-ingestion idempotently; all tests pass</done>
</task>

</tasks>

<verification>
1. `npx vitest run src/ingestion/` -- all ingestion tests pass
2. `npx tsc --noEmit` -- no type errors
3. Verify observations created have: kind="reference", source="ingest:{filename}", classification="discovery", sessionId=null
</verification>

<success_criteria>
- Markdown files split into sections by ## headings
- Each section stored as a kind="reference" observation with "ingest:{filename}" source
- Re-ingestion replaces stale sections without duplication
- Removed sections cleaned up on re-ingestion
- All observations scoped to project_hash
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/22-knowledge-ingestion-pipeline/22-01-SUMMARY.md`
</output>
