---
phase: 17-replace-decisionmaking-regexes-and-broken-haiku-with-agent-sdk-haiku
plan: 02
type: execute
wave: 2
depends_on: ["17-01"]
files_modified:
  - src/intelligence/haiku-processor.ts
  - src/graph/entity-extractor.ts
  - src/graph/relationship-detector.ts
  - src/graph/signal-classifier.ts
  - src/hooks/admission-filter.ts
  - src/hooks/noise-patterns.ts
  - src/index.ts
  - src/graph/extraction-rules.ts
  - src/curation/observation-classifier.ts
autonomous: true

must_haves:
  truths:
    - "HaikuProcessor runs on a background timer and classifies unprocessed observations via Haiku"
    - "Noise observations are soft-deleted after Haiku classification (store-then-soft-delete pattern)"
    - "Entity extraction in the embedding loop uses Haiku instead of regex rules"
    - "Relationship detection in the embedding loop uses Haiku instead of regex patterns"
    - "Admission filter no longer calls isNoise() regex patterns (keeps other cheap filters)"
    - "ObservationClassifier (broken MCP sampling) is removed from index.ts"
    - "5-minute auto-promote fallback is removed"
    - "extraction-rules.ts regex file is deleted"
    - "observation-classifier.ts broken MCP classifier is deleted"
  artifacts:
    - path: "src/intelligence/haiku-processor.ts"
      provides: "Background Haiku processing orchestrator"
      exports: ["HaikuProcessor"]
    - path: "src/graph/entity-extractor.ts"
      provides: "Entity extraction delegating to Haiku agent"
      exports: ["extractEntities", "extractAndPersist", "extractEntitiesAsync"]
    - path: "src/graph/relationship-detector.ts"
      provides: "Relationship detection delegating to Haiku agent"
      exports: ["detectRelationships", "detectAndPersist", "detectRelationshipsAsync"]
  key_links:
    - from: "src/intelligence/haiku-processor.ts"
      to: "src/intelligence/haiku-classifier-agent.ts"
      via: "classifyWithHaiku import"
      pattern: "import.*haiku-classifier-agent"
    - from: "src/intelligence/haiku-processor.ts"
      to: "src/intelligence/haiku-entity-agent.ts"
      via: "extractEntitiesWithHaiku import"
      pattern: "import.*haiku-entity-agent"
    - from: "src/index.ts"
      to: "src/intelligence/haiku-processor.ts"
      via: "HaikuProcessor instantiation and start"
      pattern: "new HaikuProcessor"
---

<objective>
Wire Haiku agents into the processing pipeline: create the background processor, replace regex-based extraction with Haiku calls in the embedding loop, simplify the admission filter, and remove all obsolete regex/classifier code.

Purpose: This is the core integration plan. After this, all observation enrichment flows through Haiku instead of regexes, and the broken ObservationClassifier is gone.

Output: 1 new file (haiku-processor.ts), 5 modified files, 2 deleted files.
</objective>

<execution_context>
@/home/matthew/.claude/get-shit-done/workflows/execute-plan.md
@/home/matthew/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-replace-decisionmaking-regexes-and-broken-haiku-with-agent-sdk-haiku/17-RESEARCH.md
@.planning/phases/17-replace-decisionmaking-regexes-and-broken-haiku-with-agent-sdk-haiku/17-01-SUMMARY.md
@src/index.ts
@src/graph/entity-extractor.ts
@src/graph/relationship-detector.ts
@src/graph/signal-classifier.ts
@src/hooks/admission-filter.ts
@src/hooks/noise-patterns.ts
@src/curation/observation-classifier.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create HaikuProcessor and modify entity-extractor + relationship-detector</name>
  <files>
    src/intelligence/haiku-processor.ts
    src/graph/entity-extractor.ts
    src/graph/relationship-detector.ts
  </files>
  <action>
    1. Create `src/intelligence/haiku-processor.ts`:
       - Import `classifyWithHaiku` from `./haiku-classifier-agent.js`
       - Import `extractEntitiesWithHaiku` from `./haiku-entity-agent.js`
       - Import `inferRelationshipsWithHaiku` from `./haiku-relationship-agent.js`
       - Import `isHaikuEnabled` from `./haiku-client.js`
       - Import `ObservationRepository` from `../storage/observations.js`
       - Import `upsertNode` from `../graph/schema.js`
       - Import `applyQualityGate` from `../graph/write-quality-gate.js`
       - Import `insertEdge`, `getNodeByNameAndType` from `../graph/schema.js`
       - Import `enforceMaxDegree` from `../graph/constraints.js`
       - Import `broadcast` from `../web/routes/sse.js`
       - Import `debug` from `../shared/debug.js`
       - Import types from `better-sqlite3`

       Export `HaikuProcessor` class:
       ```
       constructor(db, projectHash, opts?: { intervalMs?: number, batchSize?: number, concurrency?: number })
       ```
       - `intervalMs` default 30_000 (30 seconds)
       - `batchSize` default 10
       - `concurrency` default 3 (parallel observations)
       - `start()`: starts a setInterval timer calling `processOnce()`
       - `stop()`: clears the timer
       - `processOnce()`:
         a. Check `isHaikuEnabled()` -- if false, return immediately (no API key)
         b. Query observations WHERE `classification IS NULL` (unprocessed), LIMIT batchSize
         c. Process up to `concurrency` observations in parallel via `Promise.all()`
         d. For each observation, call `processOne(obs)`
       - `processOne(obs)`:
         a. Call `classifyWithHaiku(obs.content, obs.source)` -- if noise, soft-delete and mark classification='noise', return
         b. Update observation classification in DB (discovery/problem/solution)
         c. Call `extractEntitiesWithHaiku(obs.content)` -- persist each entity via `upsertNode()`, apply `applyQualityGate()` before persistence
         d. If 2+ entities extracted, call `inferRelationshipsWithHaiku(obs.content, entities)` -- resolve to node IDs via `getNodeByNameAndType()`, persist via `insertEdge()`, enforce max degree
         e. Broadcast SSE events for new entities (same as current embedding loop does)
         f. Wrap entire processOne in try/catch -- individual observation failures logged, never crash the loop
       - Each Haiku call failure is caught individually -- if classifier fails, observation stays unclassified for retry next cycle

    2. Modify `src/graph/entity-extractor.ts`:
       - Keep the existing `extractEntities()` sync function BUT mark it as deprecated (it still uses regex rules internally for any code that calls it directly)
       - Add new export `extractEntitiesAsync(text: string, observationId: string): Promise<EntityExtractionResult>` that calls `extractEntitiesWithHaiku()` from the Haiku agent
       - The `extractAndPersist()` function stays sync for backward compatibility -- the HaikuProcessor handles the async path independently
       - Do NOT delete the regex path yet -- the HaikuProcessor calls agents directly, but extractAndPersist remains available as a non-Haiku fallback that index.ts will stop calling

    3. Modify `src/graph/relationship-detector.ts`:
       - Keep the existing `detectRelationships()` sync function, mark as deprecated
       - Add new export `detectRelationshipsAsync(text: string, entities: Array<{name: string, type: EntityType}>): Promise<RelationshipCandidate[]>` that calls `inferRelationshipsWithHaiku()`
       - `detectAndPersist()` stays sync for backward compatibility -- HaikuProcessor uses agents directly
  </action>
  <verify>
    - `npx tsc --noEmit src/intelligence/haiku-processor.ts src/graph/entity-extractor.ts src/graph/relationship-detector.ts` compiles clean
    - HaikuProcessor class has start/stop/processOnce methods
    - Existing sync functions in entity-extractor and relationship-detector still work unchanged
  </verify>
  <done>
    HaikuProcessor created with timer-based background processing. Entity-extractor and relationship-detector have new async exports delegating to Haiku agents while keeping sync regex paths for backward compatibility.
  </done>
</task>

<task type="auto">
  <name>Task 2: Simplify admission filter and rewire index.ts</name>
  <files>
    src/hooks/admission-filter.ts
    src/hooks/noise-patterns.ts
    src/graph/signal-classifier.ts
    src/index.ts
  </files>
  <action>
    1. Modify `src/hooks/admission-filter.ts`:
       - REMOVE the `import { isNoise } from './noise-patterns.js'` import
       - REMOVE the noise pattern check block (lines ~149-158 that call `isNoise(content)`)
       - KEEP all other filters: self-referential check, empty content check, HIGH_SIGNAL_TOOLS bypass, Bash command filtering (isMeaningfulBashCommand), and long content without indicators check
       - The noise filtering is now Haiku's job post-storage, not a pre-storage regex gate
       - Per locked decision: "store-then-soft-delete for noise -- observations are stored first, classified by Haiku, then noise is soft-deleted"

    2. Modify `src/hooks/noise-patterns.ts`:
       - Add a deprecation comment at the top: "DEPRECATED: Noise detection is now handled by Haiku classifier agent (haiku-classifier-agent.ts). This file is retained only for reference. No active code imports it."
       - Do NOT delete the file -- it may be useful as prompt reference for the Haiku classifier's noise examples

    3. Modify `src/graph/signal-classifier.ts`:
       - Add a deprecation comment: "DEPRECATED: Signal classification is now handled by the HaikuProcessor which classifies observations directly via Haiku. This file is retained for backward compatibility but is no longer called from the embedding loop."
       - Keep the existing code functional (don't break it) -- just mark deprecated

    4. Modify `src/index.ts` -- this is the critical wiring change:
       a. ADD import: `import { HaikuProcessor } from './intelligence/haiku-processor.js'`
       b. ADD import: `import { isHaikuEnabled } from './intelligence/haiku-client.js'`
       c. REMOVE import: `import { ObservationClassifier } from './curation/observation-classifier.js'`
       d. REMOVE import: `import { classifySignal } from './graph/signal-classifier.js'`
       e. REMOVE import: `import { extractAndPersist } from './graph/entity-extractor.js'`
       f. REMOVE import: `import { detectAndPersist } from './graph/relationship-detector.js'`
       g. REMOVE the entire graph extraction block inside `processUnembedded()` (lines ~196-326 starting from `const signal = graphConfig.enabled` through the `} catch (graphErr)` block). The embedding loop should ONLY handle: embeddings, SSE broadcast, topic shift detection. Graph extraction is now HaikuProcessor's job.
       h. REMOVE the ObservationClassifier instantiation (lines ~401-405):
          ```
          const classifier = new ObservationClassifier(db.db, projectHash, server, { ... });
          ```
       i. REMOVE `classifier.start()` from the `startServer` callback
       j. REMOVE `classifier.stop()` from the `shutdown()` function
       k. ADD HaikuProcessor instantiation after the embed timer setup:
          ```
          const haikuProcessor = new HaikuProcessor(db.db, projectHash, {
            intervalMs: 30_000,
            batchSize: 10,
            concurrency: 3,
          });
          ```
       l. ADD `haikuProcessor.start()` in the `startServer` callback (where classifier.start() was)
       m. ADD `haikuProcessor.stop()` in the `shutdown()` function (where classifier.stop() was)
       n. Keep the unused imports of `upsertNode`, `getNodeByNameAndType`, `insertEdge` ONLY IF they are used elsewhere in index.ts (check provenance edge logic). If provenance edges in processUnembedded also move to HaikuProcessor, remove those imports too.
       o. The `graphConfig` import and loading can stay -- HaikuProcessor doesn't need it (Haiku replaces the config-driven signal classification), but the CurationAgent still uses it.
  </action>
  <verify>
    - `npx tsc --noEmit` on the entire project compiles clean
    - `npm run build` succeeds
    - In index.ts: no reference to ObservationClassifier, classifySignal, extractAndPersist (from entity-extractor), detectAndPersist
    - In index.ts: HaikuProcessor is instantiated, started, and stopped
    - In admission-filter.ts: no import of noise-patterns, no isNoise() call
    - processUnembedded() in index.ts only handles embedding + topic shift + SSE broadcast (no graph extraction)
  </verify>
  <done>
    Admission filter simplified (noise regexes removed, cheap filters kept). Index.ts rewired: ObservationClassifier replaced by HaikuProcessor, graph extraction removed from embedding loop (now in HaikuProcessor). Signal-classifier and noise-patterns marked deprecated.
  </done>
</task>

<task type="auto">
  <name>Task 3: Delete obsolete files and clean up imports</name>
  <files>
    src/graph/extraction-rules.ts
    src/curation/observation-classifier.ts
    src/curation/index.ts
  </files>
  <action>
    1. Delete `src/graph/extraction-rules.ts` -- all entity extraction now goes through Haiku agent.
       - First check that NO file still imports from `./extraction-rules.js` or `../graph/extraction-rules.js` (entity-extractor.ts was the only consumer; after Task 1 modifications it should use the Haiku agent path instead). If entity-extractor.ts still imports it for the deprecated sync path, update entity-extractor.ts to inline the `ALL_RULES` reference or remove the sync `extractEntities()` function entirely (since HaikuProcessor calls agents directly, nothing in the live pipeline needs the regex path anymore). The write-quality-gate.ts does NOT depend on extraction-rules.ts.

    2. Delete `src/curation/observation-classifier.ts` -- broken MCP sampling classifier fully replaced by HaikuProcessor.
       - Check `src/curation/index.ts` -- if it re-exports from observation-classifier, update it to remove that export.
       - Verify no other file imports from `./observation-classifier.js` or `../curation/observation-classifier.js` (index.ts already had its import removed in Task 2).

    3. Run full build: `npm run build` to verify no broken imports.
    4. Run `npx tsc --noEmit` to verify type checking passes.
  </action>
  <verify>
    - `src/graph/extraction-rules.ts` does not exist
    - `src/curation/observation-classifier.ts` does not exist
    - `npm run build` succeeds with no errors
    - `npx tsc --noEmit` passes
    - `grep -r "extraction-rules" src/` returns no results (no broken imports)
    - `grep -r "observation-classifier" src/` returns no results
  </verify>
  <done>
    Regex extraction rules file deleted. Broken MCP sampling observation classifier deleted. Full build passes with no broken imports. All entity extraction and classification now routes through Haiku agents.
  </done>
</task>

</tasks>

<verification>
1. `npm run build` succeeds
2. No TypeScript errors: `npx tsc --noEmit`
3. In the live pipeline: hooks store observations -> HaikuProcessor classifies + extracts entities + infers relationships -> noise is soft-deleted
4. Old regex files (extraction-rules.ts, observation-classifier.ts) are gone
5. Admission filter keeps cheap filters (empty, bash navigation, long content) but no longer calls isNoise()
6. Embedding loop in index.ts only handles: embeddings, SSE broadcast, topic shift detection
7. HaikuProcessor is started/stopped with the MCP server lifecycle
</verification>

<success_criteria>
- HaikuProcessor is the sole path for observation classification and graph extraction
- ObservationClassifier (broken MCP sampling) is completely removed
- Admission filter simplified (no regex noise patterns)
- extraction-rules.ts deleted
- observation-classifier.ts deleted
- Full build passes
</success_criteria>

<output>
After completion, create `.planning/phases/17-replace-decisionmaking-regexes-and-broken-haiku-with-agent-sdk-haiku/17-02-SUMMARY.md`
</output>
